{"version":3,"sources":["webpack:///./src/pages/kafka/connect.mdx"],"names":["_frontmatter","layoutProps","MDXLayout","DefaultLayout","MDXContent","components","props","mdxType","parentName","isMDXComponent"],"mappings":"yeAMO,IAAMA,EAAe,GAOtBC,EAAc,CAClBD,gBAEIE,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGC,E,oIACF,mBACD,OAAO,YAACJ,EAAD,KAAeD,EAAiBK,EAAhC,CAAuCD,WAAYA,EAAYE,QAAQ,cAG5E,qBAAG,mBAAGC,WAAW,KAAQ,CACrB,KAAQ,oDADT,iBAAH,2QAGA,mBAAU,CACR,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPhB,WAUI,sBAAMA,WAAW,QAAW,CAC5B,UAAa,qCACb,MAAS,CACP,cAAiB,qBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,g5BACnB,eAAkB,QAClB,QAAW,YAnBjB,OAsBA,qBAAKA,WAAW,QAAW,CACvB,UAAa,0BACb,IAAO,kBACP,MAAS,kBACT,IAAO,kFACP,OAAU,CAAC,uFAAwF,uFAAwF,wFAAyF,yFACpR,MAAS,oCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,UAtCf,UAyCA,iEAAgD,mBAAGA,WAAW,KAAQ,CAClE,KAAQ,+DADoC,2CAAhD,8BAGA,sBACE,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,aAApB,gGAAmK,mBAAGA,WAAW,MAAS,CACtL,KAAQ,oDADuJ,uBAAnK,mDAGA,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,WAApB,+MACA,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,SAApB,8LAEF,mBAAU,CACR,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPhB,WAUI,sBAAMA,WAAW,QAAW,CAC5B,UAAa,qCACb,MAAS,CACP,cAAiB,qBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,ouBACnB,eAAkB,QAClB,QAAW,YAnBjB,OAsBA,qBAAKA,WAAW,QAAW,CACvB,UAAa,0BACb,IAAO,uBACP,MAAS,uBACT,IAAO,iFACP,OAAU,CAAC,sFAAuF,sFAAuF,uFAAwF,wFACjR,MAAS,oCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,UAtCf,UAyCA,qNACA,yCACA,sBACE,kBAAIA,WAAW,MAAf,4IACA,kBAAIA,WAAW,MAAf,gCACA,kBAAIA,WAAW,MAAf,2KACA,kBAAIA,WAAW,MAAf,+DACA,kBAAIA,WAAW,MAAf,qFACA,kBAAIA,WAAW,MAAf,oEAEF,sCACA,4IACA,iDAAgC,mBAAGA,WAAW,KAAQ,CAClD,KAAQ,0EADoB,oCAAhC,qFAEmI,mBAAGA,WAAW,KAAQ,CACrJ,KAAQ,sDADuH,kCAFnI,KAKA,sIACA,mBAAU,CACR,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPhB,WAUI,sBAAMA,WAAW,QAAW,CAC5B,UAAa,qCACb,MAAS,CACP,cAAiB,qBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,4jBACnB,eAAkB,QAClB,QAAW,YAnBjB,OAsBA,qBAAKA,WAAW,QAAW,CACvB,UAAa,0BACb,IAAO,0BACP,MAAS,0BACT,IAAO,+EACP,OAAU,CAAC,oFAAqF,oFAAqF,qFAAsF,sFAC3Q,MAAS,oCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,UAtCf,UAyCA,qBAAG,kBAAIA,WAAW,KAAf,yJACH,wlBACA,oCAAmB,mBAAGA,WAAW,KAAQ,CACrC,KAAQ,oDADO,gCAAnB,2BAGA,iGAAgF,mBAAGA,WAAW,KAAQ,CAClG,KAAQ,sBADoE,gCAAhF,KAGA,4EACA,mLACA,sBACE,kBAAIA,WAAW,MAAf,YAAiC,mBAAGA,WAAW,MAAS,CACpD,KAAQ,qGADqB,yBAAjC,0GAIF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,mDAIL,sBACE,kBAAIA,WAAW,MAAf,oKAAyL,0BAAYA,WAAW,MAAvB,cAAzL,kDAAmS,0BAAYA,WAAW,MAAvB,qBAAnS,mGAAqc,0BAAYA,WAAW,MAAvB,QAArc,kBAAygB,0BAAYA,WAAW,MAAvB,cAAzgB,MAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,mMAKL,+JAEA,sBACE,kBAAIA,WAAW,MAAf,0DAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,+IAKL,6BAAY,0BAAYA,WAAW,KAAvB,0BAAZ,2CAA0H,0BAAYA,WAAW,KAAvB,uBAA1H,gBAA0M,0BAAYA,WAAW,KAAvB,eAA1M,WACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,wIAQL,kCAAiB,0BAAYA,WAAW,KAAvB,+BAAjB,kFAA2K,0BAAYA,WAAW,KAAvB,eAA3K,4GACA,8HACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,gPASL,yHACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,iBADZ,8YAUL,iGAAgF,0BAAYA,WAAW,KAAvB,QAAhF,kHACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wNAIL,8EACA,oLACA,wIACA,wDAAuC,mBAAGA,WAAW,KAAQ,CACzD,KAAQ,sFAD2B,sBAAvC,gFAGA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,8SASL,uCAAsB,0BAAYA,WAAW,KAAvB,gCAAtB,4EACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,qZAOL,qIACA,0CACA,+GAA8F,0BAAYA,WAAW,KAAvB,iBAA9F,UAAkK,mBAAGA,WAAW,KAAQ,CACpL,KAAQ,0EADsJ,QAAlK,MAGA,sBACE,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,mBAApB,kEACA,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,mBAApB,4DACA,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,kBAApB,oEAEF,oWAEA,sBACE,kBAAIA,WAAW,MAAf,+EAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,wcAYL,sBACE,kBAAIA,WAAW,MAAf,uGAA4H,mBAAGA,WAAW,MAAS,CAC/I,KAAQ,0FADgH,cAA5H,MAIF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,mJAML,+GAA8F,mBAAGA,WAAW,KAAQ,CAChH,KAAQ,2DADkF,uBAA9F,MAGA,sBACE,kBAAIA,WAAW,MAAf,iDACA,kBAAIA,WAAW,MAAf,6BACA,kBAAIA,WAAW,MAAf,iIAEF,2EAA0D,0BAAYA,WAAW,KAAvB,QAA1D,mCAA8I,mBAAGA,WAAW,KAAQ,CAChK,KAAQ,yGADkI,yBAA9I,KAGA,iNACA,4BAAW,mBAAGA,WAAW,KAAQ,CAC7B,KAAQ,yHADD,wBAAX,mBAGA,oOACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,4FAIL,2IAA0H,mBAAGA,WAAW,KAAQ,CAC5I,KAAQ,4CAD8G,kBAA1H,KAGA,mBAAU,CACR,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPhB,WAUI,sBAAMA,WAAW,QAAW,CAC5B,UAAa,qCACb,MAAS,CACP,cAAiB,sBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,owBACnB,eAAkB,QAClB,QAAW,YAnBjB,OAsBA,qBAAKA,WAAW,QAAW,CACvB,UAAa,0BACb,IAAO,gBACP,MAAS,gBACT,IAAO,gFACP,OAAU,CAAC,qFAAsF,qFAAsF,uFACvL,MAAS,oCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,UAtCf,UAyCA,yVAAwU,0BAAYA,WAAW,KAAvB,mBAAxU,gIAAogB,mBAAGA,WAAW,KAAQ,CACthB,KAAQ,qEADwf,eAApgB,6CAGA,sKACA,wFACA,mEACA,kGAAiF,mBAAGA,WAAW,KAAQ,CACnG,KAAQ,sEADqE,aAAjF,KAGA,wCAAuB,mBAAGA,WAAW,KAAQ,CACzC,KAAQ,qCADW,oCAAvB,0CAGA,wFACA,oKAAmJ,0BAAYA,WAAW,KAAvB,aAAnJ,iGACA,mDAAkC,mBAAGA,WAAW,KAAQ,CACpD,KAAQ,wBADsB,WAAlC,cAGA,0DACA,sHAAqG,mBAAGA,WAAW,KAAQ,CACvH,KAAQ,iGADyF,QAArG,KAGA,sBACE,kBAAIA,WAAW,MAAf,gCAAqD,0BAAYA,WAAW,MAAvB,YAArD,8CAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,qCAIL,sBACE,kBAAIA,WAAW,MAAf,sGAA2H,0BAAYA,WAAW,MAAvB,qBAA3H,aAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,mDAIL,sBACE,kBAAIA,WAAW,MAAf,mMAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,+GAIL,4EACA,sBACE,kBAAIA,WAAW,MAAf,0DAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,qJAKL,8EAA6D,0BAAYA,WAAW,KAAvB,kBAA7D,gBAAwI,0BAAYA,WAAW,KAAvB,eAAxI,WACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,wIAQL,8HACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,uPASL,kEACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,iBADZ,iJAIL,sHACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,iNAKL,4IACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wNAIL,0CACA,sBACE,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,oDADQ,uCAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,yDADQ,sCAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,+DADQ,gCAApB,OAEiD,mBAAGA,WAAW,MAAS,CACpE,KAAQ,oDADqC,qCAGjD,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,2CADQ,yCAQ1BJ,EAAWK,gBAAiB","file":"component---src-pages-kafka-connect-mdx-bbc516bcdd1c1a7c57ef.js","sourcesContent":["import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/runner/work/refarch-eda/refarch-eda/docs-gatsby/node_modules/gatsby-theme-carbon/src/templates/Default.js\";\nexport const _frontmatter = {};\n\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n  return <div {...props} />;\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <p><a parentName=\"p\" {...{\n        \"href\": \"https://kafka.apache.org/documentation/#connect\"\n      }}>{`Kafka connect`}</a>{` is an open source component for easily integrate external systems with Kafka. It works with any Kafka products like IBM Event Streams and Red Hat AMQ Streams.   It uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.`}</p>\n    <span {...{\n      \"className\": \"gatsby-resp-image-wrapper\",\n      \"style\": {\n        \"position\": \"relative\",\n        \"display\": \"block\",\n        \"marginLeft\": \"auto\",\n        \"marginRight\": \"auto\",\n        \"maxWidth\": \"1152px\"\n      }\n    }}>{`\n      `}<span parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-background-image\",\n        \"style\": {\n          \"paddingBottom\": \"61.45833333333333%\",\n          \"position\": \"relative\",\n          \"bottom\": \"0\",\n          \"left\": \"0\",\n          \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABcSAAAXEgFnn9JSAAACRUlEQVQoz12SzWsUQRDF92/Wo3gRT6LkIijiRTzoRbxoREHiRyBGUFQIaCTJms/d7M5XT0/PdPVMP1/1JBpdKGp2pvrXr+rVpKhDnpvQ5HWwzNY0Ym0bbO1Cyq4bo/k/2vBPnW2lYa4muemlC4BnNB2Q1z2c7/nco7A9ZnlIsax6LE2P07JPzxmfi3qss62eGZhDr8DOuIjaxWiaIS4ricaFyJvjcSZx+9CnmOUS9+cSt6YSpzOJi1LSu9b72HQSCVOgEBg62ypwIGSAsYLYBwTREHgv4BhSEIKDU4/TQlDxf24E+wvgKIsEigLDpHbSFTZA1Shsey5Y/S54syN4+UOwvivgDEdAHbCoQsrG8RIKePz8Iz58nUZOiaIkTEjtVG5mKJuH1n4KLq8Kbr0XXHk15qyUNNusdFgWDfLKISO4sMClayt48uJNBH+V9aowdDrY0kp0lP35QLCyLnjwSXB7Q/DoC9uc+2RAUdVYZAalsQk4KyLuPHyKt5vf4kCgabwqlM40ASdsuabCjT3B9TXB3U3BjXeCe5vj/LTNwg68eEju6/xsC1y9eR/PXm8khQnIPey0iGsQq4YrwNkMNEXN6ENIBrXcKcdgFzRE1QoadkN3lQPpQUPPTOFsutqBxepU5I4Jas5UY0439048do49d1Hwi4Zt7Xm+k+S0fq+d1w7O1yakPdSFbn1Ma5NVenv6mA5NZ3+BR0vB7rHgcEFgOUK1luouANmygmrXx9ImcxLsrIW0MtrueEBX6BwSLtb9Af4GliqPLgHHgpoAAAAASUVORK5CYII=')\",\n          \"backgroundSize\": \"cover\",\n          \"display\": \"block\"\n        }\n      }}></span>{`\n  `}<img parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-image\",\n        \"alt\": \"Kafka component\",\n        \"title\": \"Kafka component\",\n        \"src\": \"/refarch-eda/static/2c439d91879f50027d15b2d6812e2de5/3cbba/kafka-components.png\",\n        \"srcSet\": [\"/refarch-eda/static/2c439d91879f50027d15b2d6812e2de5/7fc1e/kafka-components.png 288w\", \"/refarch-eda/static/2c439d91879f50027d15b2d6812e2de5/a5df1/kafka-components.png 576w\", \"/refarch-eda/static/2c439d91879f50027d15b2d6812e2de5/3cbba/kafka-components.png 1152w\", \"/refarch-eda/static/2c439d91879f50027d15b2d6812e2de5/b1c97/kafka-components.png 1189w\"],\n        \"sizes\": \"(max-width: 1152px) 100vw, 1152px\",\n        \"style\": {\n          \"width\": \"100%\",\n          \"height\": \"100%\",\n          \"margin\": \"0\",\n          \"verticalAlign\": \"middle\",\n          \"position\": \"absolute\",\n          \"top\": \"0\",\n          \"left\": \"0\"\n        },\n        \"loading\": \"lazy\"\n      }}></img>{`\n    `}</span>\n    <p>{`The general concepts are detailed in the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm.github.io/event-streams/connecting/connectors/\"\n      }}>{`IBM Event streams product documentation`}</a>{`. Here is a quick summary:`}</p>\n    <ul>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`connector`}</strong>{` represents a logical job to move data from / to kafka  to / from external systems. A lot of `}<a parentName=\"li\" {...{\n          \"href\": \"https://ibm.github.io/event-streams/connectors/\"\n        }}>{`existing connectors`}</a>{` can be reused, or you can implement your owns.`}</li>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`workers`}</strong>{` are JVM running the connector. For production deployment workers run in cluster or “distributed mode”. Workers belong to a group set with groupId. A kafka topic ensure the communication between workers.`}</li>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`tasks`}</strong>{`: each worker coordinates a set of tasks to copy data. Task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.`}</li>\n    </ul>\n    <span {...{\n      \"className\": \"gatsby-resp-image-wrapper\",\n      \"style\": {\n        \"position\": \"relative\",\n        \"display\": \"block\",\n        \"marginLeft\": \"auto\",\n        \"marginRight\": \"auto\",\n        \"maxWidth\": \"1152px\"\n      }\n    }}>{`\n      `}<span parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-background-image\",\n        \"style\": {\n          \"paddingBottom\": \"40.97222222222222%\",\n          \"position\": \"relative\",\n          \"bottom\": \"0\",\n          \"left\": \"0\",\n          \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABxElEQVQoz12S728SMRjH+Vf9KzTZmyUu0xldMl84N5csMSwxMyECzhlDEHYb4RC4Red0AzaYJwMC45fAHb3rx15hamzSPn2+bT7tt09DUoJUw20UQuCJKb4n8H2l+wJwdZe+pzQfdyp0F8LDcSTjMUwmCqBaaDCARiMAQb1usxU5YjmcY2M3ReNnjeThMas7FivbJnnrlHLlhH3zCXuFBT4YL2k1xxrYakmGQwXsduGy6mtgqXTFwnqKO48zLD5PcHVRJf7xLUuvH7EYXuEol6VgfeP+dpR7W1E2X+3RvO7hKgPVmiRghaZTZcadAc/OamzEnvIwepf1yCrlc5t0Nkb04AGR5DKf8lmK1nee7SRYCx+w+yZBr3ujnyuw7DgKGPh2XYnngW3XMTJxTCtGMh2nUvlBziySMY4x0nms4ok6tIyZSfG5aHJopOn1+8za/A2l/JsMR2NanRGqJmqjoN0R6uQpv0Yuo7HDYDibTxx3VhjlNSiSJswLG7pNZsAJdnvEu3ydF+8rFEo3Wvfn6/lSh839Ml+qPZ17CvYvUN/wDz2I8w2nF02+XnZo9ydaC75H4KJ23aV43qAzULr09Rf7H/gbbT5Jqprmh7EAAAAASUVORK5CYII=')\",\n          \"backgroundSize\": \"cover\",\n          \"display\": \"block\"\n        }\n      }}></span>{`\n  `}<img parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-image\",\n        \"alt\": \"Connectors and tasks\",\n        \"title\": \"Connectors and tasks\",\n        \"src\": \"/refarch-eda/static/bff8eeeb1e72e1c82b3caa98f7a72499/3cbba/connector-tasks.png\",\n        \"srcSet\": [\"/refarch-eda/static/bff8eeeb1e72e1c82b3caa98f7a72499/7fc1e/connector-tasks.png 288w\", \"/refarch-eda/static/bff8eeeb1e72e1c82b3caa98f7a72499/a5df1/connector-tasks.png 576w\", \"/refarch-eda/static/bff8eeeb1e72e1c82b3caa98f7a72499/3cbba/connector-tasks.png 1152w\", \"/refarch-eda/static/bff8eeeb1e72e1c82b3caa98f7a72499/0710a/connector-tasks.png 1298w\"],\n        \"sizes\": \"(max-width: 1152px) 100vw, 1152px\",\n        \"style\": {\n          \"width\": \"100%\",\n          \"height\": \"100%\",\n          \"margin\": \"0\",\n          \"verticalAlign\": \"middle\",\n          \"position\": \"absolute\",\n          \"top\": \"0\",\n          \"left\": \"0\"\n        },\n        \"loading\": \"lazy\"\n      }}></img>{`\n    `}</span>\n    <p>{`When a connector is submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work.`}</p>\n    <h2>{`Characteristics`}</h2>\n    <ul>\n      <li parentName=\"ul\">{`Copy vast quantities of data from source to kafka: work at the datasource level. So when it is a database, it uses JDBC API for example.`}</li>\n      <li parentName=\"ul\">{`Support streaming and batch.`}</li>\n      <li parentName=\"ul\">{`Scale at the organization level, even if it can support a standalone, mono connector approach to start small, it is possible to run in parallel on distributed cluster.`}</li>\n      <li parentName=\"ul\">{`Copy data, externalizing transformation in other framework.`}</li>\n      <li parentName=\"ul\">{`Kafka Connect defines three models: data model, worker model and connector model.`}</li>\n      <li parentName=\"ul\">{`Provide a REST interface to manage connectors and monitor jobs.`}</li>\n    </ul>\n    <h2>{`Installation`}</h2>\n    <p>{`The  Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment.`}</p>\n    <p>{`We recommend reading the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm.github.io/event-streams/connecting/setting-up-connectors/\"\n      }}>{`IBM  event streams documentation`}</a>{` for installing Kafka connect with IBM Event Streams or you can also leverage the `}<a parentName=\"p\" {...{\n        \"href\": \"https://strimzi.io/docs/0.17.0/#kafka-connect-str\"\n      }}>{`Strimzi Kafka connect operator`}</a>{`.`}</p>\n    <p>{`With IBM Event Streams on premise deployment, the connectors setup is part of the user admin console toolbox:`}</p>\n    <span {...{\n      \"className\": \"gatsby-resp-image-wrapper\",\n      \"style\": {\n        \"position\": \"relative\",\n        \"display\": \"block\",\n        \"marginLeft\": \"auto\",\n        \"marginRight\": \"auto\",\n        \"maxWidth\": \"1152px\"\n      }\n    }}>{`\n      `}<span parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-background-image\",\n        \"style\": {\n          \"paddingBottom\": \"40.97222222222222%\",\n          \"position\": \"relative\",\n          \"bottom\": \"0\",\n          \"left\": \"0\",\n          \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABRklEQVQoz31RTUvEMBDtH/fgQVG8CYIiuEdvCrIsqChIC4IHL4s397hLSzf9SlvbZJJnJrqlrmLgMWEy82beSxAnKZIkQV3XkFJ68H0bUtZ/5rcRlI3Bf4eMhSaDqiHYUd5aO0Su8XXaIMizDMvlEmmaDuAt27aFUmogyFxdHMfI8xxFUaDrOvR9DyJeaDPKEXIjE1RVhbIsfdysr7XG8ztwO7doO+UG9D7HhFEUIQxDROET7l8FZnPg4c0Rdr32/Lz6BsaYIZ4+EnavNfJaDzKFEJhMJrhwOD87wf7lAjtXwOG0R5BVbrKmX95xsyL7LcdAuLr2Q0GPbNjq+JLMHq5WKz+VwV4JB+3kjYubWmK9Xvt3tqlpGg/lLPjhIRtPRN4bvivlpBkNIQkHU4u9G4ujGUG2NGzOVjC0/xCDl4V1ki2O7wifdd1if7sNI5cAAAAASUVORK5CYII=')\",\n          \"backgroundSize\": \"cover\",\n          \"display\": \"block\"\n        }\n      }}></span>{`\n  `}<img parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-image\",\n        \"alt\": \"Event Streams connector\",\n        \"title\": \"Event Streams connector\",\n        \"src\": \"/refarch-eda/static/065602b9bb67479c9e4119b4a6195620/3cbba/es-connectors.png\",\n        \"srcSet\": [\"/refarch-eda/static/065602b9bb67479c9e4119b4a6195620/7fc1e/es-connectors.png 288w\", \"/refarch-eda/static/065602b9bb67479c9e4119b4a6195620/a5df1/es-connectors.png 576w\", \"/refarch-eda/static/065602b9bb67479c9e4119b4a6195620/3cbba/es-connectors.png 1152w\", \"/refarch-eda/static/065602b9bb67479c9e4119b4a6195620/f9628/es-connectors.png 1343w\"],\n        \"sizes\": \"(max-width: 1152px) 100vw, 1152px\",\n        \"style\": {\n          \"width\": \"100%\",\n          \"height\": \"100%\",\n          \"margin\": \"0\",\n          \"verticalAlign\": \"middle\",\n          \"position\": \"absolute\",\n          \"top\": \"0\",\n          \"left\": \"0\"\n        },\n        \"loading\": \"lazy\"\n      }}></img>{`\n    `}</span>\n    <p><em parentName=\"p\">{`Deploying connectors against an IBM Event Streams cluster, you need to have API key with permissions to produce and consume messages for all topics.`}</em></p>\n    <p>{`As an extendable framework, kafka connect, can have new connector plugins. To deploy new connector, the kafka docker image defining the connector needs to be updated with the connector jar and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL.`}</p>\n    <p>{`Here is the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm.github.io/event-streams/connectors/\"\n      }}>{`list of supported connectors`}</a>{` for IBM Event Streams.`}</p>\n    <p>{`We will use this image to run the kafka connect in standalone mode or in `}<a parentName=\"p\" {...{\n        \"href\": \"#distributed-mode\"\n      }}>{`the distributed mode section`}</a>{`.`}</p>\n    <h2>{`Getting started with kafka connect standalone mode`}</h2>\n    <p>{`For development and test purposes, we can use Kafka connect in standalone mode, but still connected to IBM Event Streams running on-premise or on-cloud.  `}</p>\n    <ul>\n      <li parentName=\"ul\">{`From the `}<a parentName=\"li\" {...{\n          \"href\": \"https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/Dockerfile\"\n        }}>{`downloaded dockerfile`}</a>{` (in the refarch-kc repository) we can build a new kafka connect environment image using the command:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`docker build -t ibmcase/kafkaconnect:0.0.1 .\n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">{`Start a container with kafka connector, to run a standalone connector: you need to use a worker configuration and one of the connector properties file under the `}<inlineCode parentName=\"li\">{`connectors`}</inlineCode>{` folder. Those files will be mounted under the `}<inlineCode parentName=\"li\">{`/opt/kafka/config`}</inlineCode>{` inside the container. Also, as we want to test sending the content of a file, we mount a local `}<inlineCode parentName=\"li\">{`data`}</inlineCode>{` folder to the `}<inlineCode parentName=\"li\">{`/home/data`}</inlineCode>{`:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`# in the refarch-kc/docker/kafka-connect folder\ndocker run -ti  --rm -v $(pwd)/config:/opt/kafka/config -v $(pwd)/data:/home/data --entrypoint bash -p 8083:8083 ibmcase/kafkaconnect:0.0.1\n`}</code></pre>\n    <p>{`!!! Note\nYou need to modify those property files to set the API key for your own event streams cluster, and set any other properties.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Inside the container starts the standalone connector:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`cd /opt/kafka\n./bin/connect-standalone.sh config/worker-standalone.properties config/file-source.properties config/file-sink.properties\n`}</code></pre>\n    <p>{`The  `}<inlineCode parentName=\"p\">{`file-source.properties`}</inlineCode>{` configures a file reader to source the `}<inlineCode parentName=\"p\">{`data/access_log.txt`}</inlineCode>{` file to the `}<inlineCode parentName=\"p\">{`clickstream`}</inlineCode>{` topic:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`name=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/home/kafka-connect/access_log.txt\ntopic=clickstream\n`}</code></pre>\n    <p>{`While the `}<inlineCode parentName=\"p\">{`config/file-sink.properties`}</inlineCode>{` defines a file sink stream to create a json file by getting messages from the `}<inlineCode parentName=\"p\">{`clickstream`}</inlineCode>{` topic. The file sink connector can read from multiple topics to aggregate the content in the same file.`}</p>\n    <p>{`The standalone connector worker configuration specifies where to connect, and what converters to use:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`bootstrap.servers=....\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n\n# Local storage file for offset data\noffset.storage.file.filename=/tmp/connect.offsets\n`}</code></pre>\n    <p>{`The execution trace shows the producer id, and the consumer id, and the task for each connector:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-log\"\n      }}>{`INFO Creating task local-file-source-0 (org.apache.kafka.connect.runtime.Worker:414)\n...\nINFO TaskConfig values: \n    task.class = class org.apache.kafka.connect.file.FileStreamSourceTask\n...\n INFO Creating connector local-file-sink of type FileStreamSink (org.apache.kafka.connect.runtime.Worker:246)\n INFO Creating task local-file-sink-0 (org.apache.kafka.connect.runtime.Worker:414)\n`}</code></pre>\n    <p>{`To validate the data are well published see the generated file under the `}<inlineCode parentName=\"p\">{`data`}</inlineCode>{` folder. As the Json converter was used, the message was wrapped into a json document with schema and payload.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\\\"POST /xmlrpc.php HTTP/1.0\\\\\" 200 370 \\\\\"-\\\\\" \\\\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\\\"\"}\n`}</code></pre>\n    <h2>{`Connecting to IBM Cloud Event Streams remote cluster`}</h2>\n    <p>{`To connect to Event Streams on IBM Cloud the properties needs to define the broker adviser URLs and the API key that you get from the service crendentials.`}</p>\n    <p>{`This API key must provide permission to produce and consume messages for all topics, and also to create topics.`}</p>\n    <p>{`With Event streams on Cloud the `}<a parentName=\"p\" {...{\n        \"href\": \"https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect\"\n      }}>{`following document`}</a>{` explains what properties to add to the worker and connectors configuration.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`bootstrap.servers=broker-3-qnsdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprt...\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"98....\";\n\n`}</code></pre>\n    <p>{`Using the same `}<inlineCode parentName=\"p\">{`file source stream connector`}</inlineCode>{` to send records and a simple consumer console to trace the output like:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`docker run -ti  -v $(pwd)/config:/opt/kafka/config --entrypoint bash  ibmcase/kafkaconnect:0.0.1\n\nesuser@3245874dcdd3: cd /opt/kafka/bin/\nesuser@3245874dcdd3: ./kafka-console-consumer.sh --bootstrap-server eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443 --consumer.config /opt/kafka/config/console-consumer.properties --topic clickstream --from-beginning\"\n`}</code></pre>\n    <p>{`The console-consumer.properties specifies the SASL properties to connect to the remote broker using API key.`}</p>\n    <h2>{`Distributed mode`}</h2>\n    <p>{`When running in distributed mode, the connectors need three topics as presented in the `}<inlineCode parentName=\"p\">{`create topics`}</inlineCode>{` table `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm.github.io/event-streams/connecting/setting-up-connectors/\"\n      }}>{`here`}</a>{`. `}</p>\n    <ul>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`connect-configs`}</strong>{`: This topic will store the connector and task configurations.`}</li>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`connect-offsets`}</strong>{`: This topic is used to store offsets for Kafka Connect.`}</li>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`connect-status`}</strong>{`: This topic will store status updates of connectors and tasks.`}</li>\n    </ul>\n    <p>{`In distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves via Kafka topic.  If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers.\nIf a new worker starts work, a rebalance ensures it takes over some work from the existing workers.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Using IBM Event Streams CLI, the topics are created via the commands like:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`# log to the kubernetes cluster:\ncloudctl login -a https://icp-console.apps.green.ocp.csplab.local\n# initialize the event streams CLI plugin\ncloudctl es init\n# Create the Kafka topic\ncloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compact\ncloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compact\ncloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact\ncloudctl es topics\n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">{`When using a kafka cluster managed with Strimzi topic operator you can use the topic definitions in `}<a parentName=\"li\" {...{\n          \"href\": \"https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect\"\n        }}>{`the folder`}</a>{`:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`oc apply -f strimzi-connect-config-topic.yaml\noc apply -f strimzi-connect-offsets-topic.yaml\noc apply -f strimzi-connect-status-topic.yaml\n`}</code></pre>\n    <p>{`The connector configuration needs to specify some other properties as explained in the `}<a parentName=\"p\" {...{\n        \"href\": \"https://kafka.apache.org/documentation/#connectconfigs\"\n      }}>{`kafka documentation`}</a>{`):`}</p>\n    <ul>\n      <li parentName=\"ul\">{`group.id to specify the connect cluster name.`}</li>\n      <li parentName=\"ul\">{`key and value converters.`}</li>\n      <li parentName=\"ul\">{`replication factors and topic name for the three needed topics, if Kafka connect is enabled to create topics on the cluster.`}</li>\n    </ul>\n    <p>{`When using Event Streams as kafka cluster, add the `}<inlineCode parentName=\"p\">{`sasl`}</inlineCode>{` properties as described in the `}<a parentName=\"p\" {...{\n        \"href\": \"https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect#distributed_worker\"\n      }}>{`product documentation`}</a>{`.`}</p>\n    <p>{`With Event Streams as part of the Cloud Pak for integration, the administration console explains the steps to setup connectors, get distributed configuration and how to add connectors.`}</p>\n    <p>{`See `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/distributed-workers.properties\"\n      }}>{`this properties file`}</a>{` as an example.`}</p>\n    <p>{`To start a Kafka connect in distributed mode locally, connected to Event Streams deployed on-premise use the following command (the entry point in the dockerfile use the connect-distributed mode script):`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`docker run -v $(pwd)/config:/opt/kafka/config -p 8083:8083 ibmcase/kafkaconnect:0.0.1\n`}</code></pre>\n    <p>{`To illustrate the Kakfa Connect distributed mode, we will add a source connector from a Mongo DB data source using `}<a parentName=\"p\" {...{\n        \"href\": \"https://www.mongodb.com/kafka-connector\"\n      }}>{`this connector`}</a>{`.`}</p>\n    <span {...{\n      \"className\": \"gatsby-resp-image-wrapper\",\n      \"style\": {\n        \"position\": \"relative\",\n        \"display\": \"block\",\n        \"marginLeft\": \"auto\",\n        \"marginRight\": \"auto\",\n        \"maxWidth\": \"1036px\"\n      }\n    }}>{`\n      `}<span parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-background-image\",\n        \"style\": {\n          \"paddingBottom\": \"48.611111111111114%\",\n          \"position\": \"relative\",\n          \"bottom\": \"0\",\n          \"left\": \"0\",\n          \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB20lEQVQoz2WSS2/TQBSF/Uf5BazY0RU/gA0SG4TEggWL7soKIRYRBKUPl1aQkKjNo5CEpE1iO4+J7fHY83HHaaBpr3TlGd87555zZjxT5IClKAqstdyNNDOoOGG1jrlXIi9s2Z/LcZfb8Kbxglm4JI41V8OQg1qf/eqAk+YIK51qnZAkCf3hmI/+b94fDamc9ghmIcuVRWZJTyEYSUnKmwUB7c6Q3EDV77H3usaTV8e8OfDJtRWWm8lfaj6PX3zm0fMTnr78QPNnR9hvlaQc1+vEaYrnpiu1Kgvdqx61agX/6Cvfz09lImi90XpzPaJ92aUrw/u/2oRhwNaFzBhG0ylGFHn/xIsnvUaLH/UG9UZTWHeIE0u0EJaplUPIHuYL5CDl3pic++E5Y90k55deKRKdkaRajDZEUUYQafEpZalSVpJurdaZ+KflUooSpMS4TW/7oySJk6jFk5z5OuXPxAFtXsGmRy5BTB2FMddBLEzNQ8DtYutFezDj8OKGZ/u+5Ddaw6ismXzDptoas/fuhLeVi9un9h+wlLzDUBpUYphEirPLAe1RRKxz7vb0xxM+HZ5z1uzIWyx2ag8A3ddJzoWpdXJsTpZlpOKrdilrpRRBMGOxmIvPux66+AsdPPnol0i/gAAAAABJRU5ErkJggg==')\",\n          \"backgroundSize\": \"cover\",\n          \"display\": \"block\"\n        }\n      }}></span>{`\n  `}<img parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-image\",\n        \"alt\": \"Mongo source \",\n        \"title\": \"Mongo source \",\n        \"src\": \"/refarch-eda/static/254a8415cdb514248a00723294bc8030/edd00/kconnect-mongo.png\",\n        \"srcSet\": [\"/refarch-eda/static/254a8415cdb514248a00723294bc8030/7fc1e/kconnect-mongo.png 288w\", \"/refarch-eda/static/254a8415cdb514248a00723294bc8030/a5df1/kconnect-mongo.png 576w\", \"/refarch-eda/static/254a8415cdb514248a00723294bc8030/edd00/kconnect-mongo.png 1036w\"],\n        \"sizes\": \"(max-width: 1036px) 100vw, 1036px\",\n        \"style\": {\n          \"width\": \"100%\",\n          \"height\": \"100%\",\n          \"margin\": \"0\",\n          \"verticalAlign\": \"middle\",\n          \"position\": \"absolute\",\n          \"top\": \"0\",\n          \"left\": \"0\"\n        },\n        \"loading\": \"lazy\"\n      }}></img>{`\n    `}</span>\n    <p>{`When using as a source, the connector publishes data changes from MongoDB into Kafka topics for streaming to consuming apps. Data is captured via Change Streams within the MongoDB cluster and published into Kafka topics. The installation of a connector is done by adding the jars from the connector into the plugin path (`}<inlineCode parentName=\"p\">{`/opt/connectors`}</inlineCode>{`) as defined in the connector properties. In the case of mongodb kafka connector the manual installation instructions are in `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/mongodb/mongo-kafka/blob/master/docs/install/\"\n      }}>{`this github`}</a>{`. The download page includes an uber jar.`}</p>\n    <p>{`As we run the kakfa connect as docker container, the approach is to build a new docker image based one of the Kafka image publicly available.`}</p>\n    <p>{`To define and start a connector, you do a POST to the REST API.`}</p>\n    <h2>{`Verifying the connectors via the REST api`}</h2>\n    <p>{`The documentation about the REST APIs for the distributed connector is in `}<a parentName=\"p\" {...{\n        \"href\": \"https://docs.confluent.io/current/connect/references/restapi.html\"\n      }}>{`this site`}</a>{`.`}</p>\n    <p>{`For example the `}<a parentName=\"p\" {...{\n        \"href\": \"http://localhost:8083/connectors\"\n      }}>{`http://localhost:8083/connectors`}</a>{` is the base URL when running locally.`}</p>\n    <h2>{`Deploy the Kafka connect as a service within Openshift cluster`}</h2>\n    <p>{`When you use IBM Event Streams on Openshift, you can deploy the IBM kafka connector environment as Docker containers, and define the needed `}<inlineCode parentName=\"p\">{`connect-*`}</inlineCode>{` topics as explained in previous section. The product documentation describes how to do that.`}</p>\n    <p>{`Another approach is to use `}<a parentName=\"p\" {...{\n        \"href\": \"https://strimzi.io/\"\n      }}>{`Strimzi`}</a>{` operator.`}</p>\n    <h2>{`Running with local kafka cluster`}</h2>\n    <p>{`We are using a local kafka cluster started with docker-compose as defined in the compose file `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/backbone-compose.yml\"\n      }}>{`here`}</a>{`.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`The docker network should be `}<inlineCode parentName=\"li\">{`kafkanet`}</inlineCode>{`, if not already created do the following`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`docker network create kafkanet\n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">{`Start the kafka broker (bitnami distribution) and zookeeper node using the command below under the `}<inlineCode parentName=\"li\">{`refarch-kc/docker`}</inlineCode>{` folder:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`docker-compose -f backbone-compose.yml up -d\n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">{`Start a container with kafka code, to run a standalone connector: you need to use a worker configuration and a connector properties files. Those files will be mounted under the /home folder:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`docker run -ti  -rm --name kconnect -v $(pwd):/home --network kafkanet -p 8083:8083 bitnami/kafka:2 bash\n`}</code></pre>\n    <p>{`Need to map the port 8083, to access the REST APIs.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Inside the container starts the standalone connector:`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`cd /opt/bitnami/kafka\n./bin/connect-standalone.sh /home/kafka-connect/worker-standalone.properties /home/kafka-connect/file-source.properties\n`}</code></pre>\n    <p>{`The above file configures a file reader to source the `}<inlineCode parentName=\"p\">{`access_log.txt`}</inlineCode>{` file to the `}<inlineCode parentName=\"p\">{`clickstream`}</inlineCode>{` topic:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`name=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/home/kafka-connect/access_log.txt\ntopic=clickstream\n`}</code></pre>\n    <p>{`The standalone connector worker configuration specifies where to connect, and what converters to use:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`bootstrap.servers=kafka1:9092\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n\n# Local storage file for offset data\noffset.storage.file.filename=/tmp/connect.offsets\n`}</code></pre>\n    <p>{`The execution trace shows the producer id`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-log\"\n      }}>{`INFO [Producer clientId=connector-producer-local-file-source-0] Cluster ID: tj8y0hiZSYWHB9vLHGP1Ew (org.apache.kafka.clients.Metadata:261)\n`}</code></pre>\n    <p>{`To validate the data are well published run another container with the consumer console tool:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`docker run -ti  --name sinktrace --rm  --network kafkanet bitnami/kafka:2 bash -c \"\n/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic clickstream --from-beginning\"\n`}</code></pre>\n    <p>{`As the Json converter was used the trace show the message was wrapped into a json document with schema and payload.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\\\"POST /xmlrpc.php HTTP/1.0\\\\\" 200 370 \\\\\"-\\\\\" \\\\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\\\"\"}\n`}</code></pre>\n    <h2>{`Further Readings`}</h2>\n    <ul>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://kafka.apache.org/documentation/#connect\"\n        }}>{`Apache Kafka connect documentation`}</a></li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://docs.confluent.io/current/connect/index.html\"\n        }}>{`Confluent Connector Documentation`}</a></li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://ibm.github.io/event-streams/connecting/connectors/\"\n        }}>{`IBM Event Streams Connectors`}</a>{` or `}<a parentName=\"li\" {...{\n          \"href\": \"https://ibm.github.io/event-streams/connectors/\"\n        }}>{`the list of supported connectors`}</a></li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://github.com/mongodb/mongo-kafka\"\n        }}>{`MongoDB Connector for Apache Kafka`}</a></li>\n    </ul>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}