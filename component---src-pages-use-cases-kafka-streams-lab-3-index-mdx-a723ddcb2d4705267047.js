(window.webpackJsonp=window.webpackJsonp||[]).push([[64],{"M+yy":function(e,t,a){"use strict";a.r(t),a.d(t,"_frontmatter",(function(){return i})),a.d(t,"default",(function(){return d}));var n=a("wx14"),r=a("zLVn"),s=(a("q1tI"),a("7ljp")),o=a("013z"),i=(a("qKvR"),{}),l=function(e){return function(t){return console.warn("Component '"+e+"' was not imported, exported, or provided by MDXProvider as global scope"),Object(s.b)("div",t)}},c=l("InlineNotification"),p=l("AnchorLinks"),u=l("AnchorLink"),b={_frontmatter:i},m=o.a;function d(e){var t=e.components,a=Object(r.a)(e,["components"]);return Object(s.b)(m,Object(n.a)({},b,a,{components:t,mdxType:"MDXLayout"}),Object(s.b)(c,{kind:"warning",mdxType:"InlineNotification"},Object(s.b)("strong",null,"TODO")," - Work in progress"),Object(s.b)(p,{mdxType:"AnchorLinks"},Object(s.b)(u,{mdxType:"AnchorLink"},"Overview"),Object(s.b)(u,{mdxType:"AnchorLink"},"Scenario Prerequisites"),Object(s.b)(u,{mdxType:"AnchorLink"},"Develop the application"),Object(s.b)(u,{mdxType:"AnchorLink"},"Integration Tests"),Object(s.b)(u,{mdxType:"AnchorLink"},"Deploy to OpenShift")),Object(s.b)("h2",null,"Overview"),Object(s.b)("p",null,"In this lab, weâ€™re going to use ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://quarkus.io"}),"Quarkus")," to develop the logic with Kafka streams api and microprofile reactive messaging."),Object(s.b)("p",null,"The requirements to address are:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"consume item sold from items topic, item has unique key. Item event has store information"),Object(s.b)("li",{parentName:"ul"},"compute for each item its current stock cross store"),Object(s.b)("li",{parentName:"ul"},"compute the store stock for each item"),Object(s.b)("li",{parentName:"ul"},"generate inventory event for store - item - stock"),Object(s.b)("li",{parentName:"ul"},"expose APIs to get stock for a store or for an item")),Object(s.b)("p",null,"Here is a simple diagram to illustrate the components used:"),Object(s.b)("p",null," ",Object(s.b)("span",Object(n.a)({parentName:"p"},{className:"gatsby-resp-image-wrapper",style:{position:"relative",display:"block",marginLeft:"auto",marginRight:"auto",maxWidth:"736px"}}),"\n      ",Object(s.b)("span",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-background-image",style:{paddingBottom:"42.36111111111111%",position:"relative",bottom:"0",left:"0",backgroundImage:"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABnElEQVQoz12SyW4TQRCG/bQcuHHjwlsgJRcuHEGASKQgc8AHgmIRxRbB9ogo3iCWJx57Fs/SM+Pu/qgekxAo6S91Vdf2V3frtNvja6fNxacOC8+DLINtKoghibFxIIgEMTqKGhg5b32fYDIjms8JZzPUcglFQeuHd0rvssPnL12uplOc9PwOzy8ecdB/zIvBU87895QaKmNRugnhqP+Tg5cfOTk54vXbd3ij0b4gWc6daG2oKmh/O+Ow+4w3V084vn7F4HZIVe6w1mCNQRRK7DTJybOUVFjVSgmrLS0ThUz8mCRX9GYBUaruG/hxxehmQ5TVDH6FGCm207qBMZp/RO5sktByu9kkW5I0p30+JFUVZalkfRF975rv4xtm/poP3UuUUNrnGuqdm9j+hTTBFXTKiXOKxulaqJd1TVVXDyax4tdoidu5afhPXL4raNZrtPDX0v0OMiLNMgXODhYLkiBobCuxVu7Ng/gGabqnXIQh5WaDEpRyLqTBSr7Brbx4IF9iOR4zHQ6Zyyv6k8m9P1utmvjyT56SvFoV/AaWvlsqWlP79gAAAABJRU5ErkJggg==')",backgroundSize:"cover",display:"block"}})),"\n  ",Object(s.b)("img",Object(n.a)({parentName:"span"},{className:"gatsby-resp-image-image",alt:"1",title:"1",src:"/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/d2d67/item-aggregator-ctx.png",srcSet:["/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/7fc1e/item-aggregator-ctx.png 288w","/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/a5df1/item-aggregator-ctx.png 576w","/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/d2d67/item-aggregator-ctx.png 736w"],sizes:"(max-width: 736px) 100vw, 736px",style:{width:"100%",height:"100%",margin:"0",verticalAlign:"middle",position:"absolute",top:"0",left:"0"},loading:"lazy"})),"\n    ")),Object(s.b)("p",null,"The goal of this lab, is to develop the green component which exposes an API to support Kafka stream interactive queries on top of the aggregates save in state store (light blue storage/per service deployed and persisted in kafka as topic)."),Object(s.b)("p",null,"We will be testing using ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://kafka.apache.org/documentation/streams/"}),"Apache Kafka Streams")," TopologyTestDriver to mimic a Topology, a Stream and Table. "),Object(s.b)("p",null,"This application is deployed to OpenShift cluster with Event Streams running. We use the quarkus kubernetes plugin with all the needed definitions are done in the ",Object(s.b)("inlineCode",{parentName:"p"},"application.properties"),"."),Object(s.b)("h2",null,"Scenario Pre-requisites"),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"Java")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"For the purposes of this lab we suggest Java 11+"),Object(s.b)("li",{parentName:"ul"},"Quarkus 1.7.2+")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"Git client")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"Maven")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Maven will be needed for bootstrapping our application from the command-line and running\nour application.")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"An IDE of your choice")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Ideally an IDE that supports Quarkus (such as Visual Studio Code)")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"OpenShift Container Platform")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"v4.4.x")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"IBM Cloud Pak for Integration")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"CP4I2020.2")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"IBM Event Streams")),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"The section on use with Event Streams on CP4I assumes Event Streams v10. If using a previous version such as ESv2019.4.2, there are some differences to how you would configure ",Object(s.b)("inlineCode",{parentName:"li"},"application.properties")," to establish a connection.")),Object(s.b)("p",null,Object(s.b)("strong",{parentName:"p"},"Code Source"),": clone the following git repository: ",Object(s.b)("inlineCode",{parentName:"p"},"git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory"),"."),Object(s.b)("p",null,"The final source code is in this Git repository: ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory"}),"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory"),"."),Object(s.b)("h2",null,"Use application as-is"),Object(s.b)("p",null,"If you do not want to develop the application, you can deploy it on OpenShift using our ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://hub.docker.com/r/ibmcase/item-aggregator"}),"docker image"),". See the ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory"}),"repository readme")," to do so."),Object(s.b)("h2",null,"Develop the application"),Object(s.b)("h3",null,"Setting up the Quarkus Application"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"We will bootstrap the Quarkus application with the following Maven command (See ",Object(s.b)("a",Object(n.a)({parentName:"li"},{href:"https://quarkus.io/guides/maven-tooling#project-creation"}),"Quarkus maven tooling guide")," for more information):")),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),'mvn io.quarkus:quarkus-maven-plugin:1.7.2.Final:create \\\n    -DprojectGroupId=ibm.garage \\\n    -DprojectArtifactId=quarkus-kstreams-lab3 \\\n    -Dextensions="kafka, resteasy-jsonb, quarkus-resteasy-mutiny,smallrye-health,quarkus-smallrye-openapi,openshift"\n')),Object(s.b)("p",null,"You can replace the ",Object(s.b)("inlineCode",{parentName:"p"},"projectGroupId, projectArtifactId")," fields as you like."),Object(s.b)("p",null,Object(s.b)("em",{parentName:"p"},"Recall that is if you want to add a quarkus extension do something like: ",Object(s.b)("inlineCode",{parentName:"em"},'./mvnw quarkus:add-extension -Dextensions="kafka"'))),Object(s.b)("h3",null,"Start the dev mode"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"./mvnw quarkus:dev\n")),Object(s.b)("p",null,"Going to the URL ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"http://localhost:8080/"}),"http://localhost:8080/")," will generate an exception, as we need to add configuration for Kafka-Streams."),Object(s.b)("p",null,"Let add the minimum into the ",Object(s.b)("inlineCode",{parentName:"p"},"application.properties")),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-properties"}),"quarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n\nquarkus.log.console.level=INFO\nquarkus.log.console.enable=true\nquarkus.http.port=8080\nquarkus.swagger-ui.always-include=true\nquarkus.openshift.expose=true\n")),Object(s.b)("p",null,"Now the application should display a basic web page. As we defined to use OpenAPI the following address should give us the API defined: ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"http://localhost:8080/swagger-ui/#/default"}),"http://localhost:8080/swagger-ui/#/default"),"."),Object(s.b)("p",null,"and health works too: ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"http://localhost:8080/health"}),"http://localhost:8080/health")),Object(s.b)("p",null,"Let add a simple resource under the following package ",Object(s.b)("inlineCode",{parentName:"p"},"ibm.garage.lab3.api")),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-java"}),'import javax.enterprise.context.ApplicationScoped;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\n\nimport io.smallrye.mutiny.Uni;\nimport io.vertx.core.json.JsonObject;\n\n@ApplicationScoped\n@Path("/inventory")\npublic class InventoryResource {\n    \n    @GET\n    @Path("/store/{storeID}")\n    @Produces(MediaType.APPLICATION_JSON)\n    public  Uni<JsonObject> getStock(@PathParam("storeID") String storeID) {\n            JsonObject stock = new JsonObject("{\\"name\\": \\"hello you\\", \\"id\\": \\"" + storeID + "\\"}");\n            return Uni.createFrom().item( stock);\n    }\n}\n')),Object(s.b)("p",null,"Outside of the traditional JAXRS annotation, we are using Uni class from ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://smallrye.io/smallrye-mutiny/"}),"Mutiny")," to get our API being asynchronous non-blocking: Quarkus uses ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://vertx.io/"}),"Vert.x")," to support non-blocking IO programming model and Mutiny is another abstraction to manage mono or multi elements in a reactive way."),Object(s.b)("p",null,"A page refresh on ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"http://localhost:8080/swagger-ui/"}),"http://localhost:8080/swagger-ui/")," will get the new API which should work."),Object(s.b)("h3",null,"Deploy to OpenShift using s2i"),Object(s.b)("p",null,"Before going too far in the development, let deploy this simple app to OpenShift. We assume you are logged to the cluster via ",Object(s.b)("inlineCode",{parentName:"p"},"oc login...")),Object(s.b)("p",null,"The following command should package the application and create OpenShift manifests, build a docker images and push it to OpenShift Private registry."),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"./mvnw package -Dquarkus.kubernetes.deploy=true\n")),Object(s.b)("p",null,"It can take some seconds to build and deploy: ",Object(s.b)("inlineCode",{parentName:"p"},"oc get pods -w")," lets you see the build pods and the running app once the build is done. As we expose the application an OpenShift route was created. The url is visible at the end of the build output, something like:"),Object(s.b)("p",null,Object(s.b)("inlineCode",{parentName:"p"},"...The deployed application can be accessed at: http://quarkus-kstreams-lab3...")),Object(s.b)("p",null,"For example this was the URL to access the swagger:"),Object(s.b)("p",null,Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/"}),"http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/")),Object(s.b)("h3",null,"Define the domain entities"),Object(s.b)("p",null,"Under the ",Object(s.b)("inlineCode",{parentName:"p"},"src/main/java/../domain")," folder add the two classes representing the business entities we are using:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"}),'public class Item {\n    public static String RESTOCK = "RESTOCK";\n    public static String SALE = "SALE";\n    public String storeName;\n    public String sku;\n    public int quantity;\n    public String type;\n    public Double price;\n    public String timestamp;\n\n    public Item(){}\n}\n')),Object(s.b)("p",null,"This item will also being used for event structure on ",Object(s.b)("inlineCode",{parentName:"p"},"items")," topic. The type attribute is to specify if this is a sale event or a restock event."),Object(s.b)("p",null,"The inventory per store includes a map of item.sku and quantity."),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"}),"public class Inventory {\n    public String storeName;\n    public HashMap<String,Long> stock = new HashMap<String,Long>();\n    public Inventory(){}\n}\n")),Object(s.b)("p",null,"As part of the logic we want to add methods in the Inventory class to update the quantity given an item. So the two following methods are added"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"}),'public Inventory updateStockQuantity(String k, Item newValue) {\n        this.storeName = k;\n        if (newValue.type.equals("SALE")) \n            newValue.quantity=-newValue.quantity;\n        return this.updateStock(newValue.sku,newValue.quantity);\n    }\n\n    public Inventory updateStock(String sku, long newV) {\n        if (stock.get(sku) == null) {\n            stock.put(sku, Long.valueOf(newV));\n        } else {\n            Long currentValue = stock.get(sku);\n            stock.put(sku, Long.valueOf(newV) + currentValue );\n        }\n        return this;\n    }\n')),Object(s.b)("p",null,"Modify the InventoryResource to return the inventory instead of JsonObject (we will connect interactive query later in this lab)."),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-java"}),'public  Uni<Inventory> getStock(@PathParam("storeID") String storeID) {\n        Inventory stock = new Inventory();\n        stock.storeName = storeID;\n        Item newItem = new Item();\n        newItem.quantity = 10;\n        newItem.sku="item-01";\n        newItem.type = Item.RESTOCK;\n        stock.updateStockQuantity(storeID, newItem);\n            return Uni.createFrom().item( stock);\n    }\n')),Object(s.b)("p",null,"You should get a json document like the following:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-json"}),'{"stock": {\n    "item-01": 10\n  },\n  "storeName": "Store-A"\n}\n')),Object(s.b)("p",null,"Now we are good with the REST end point. Lets add Kafka-streams to connect to Event Streams."),Object(s.b)("h3",null,"Add Kafka"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),'./mvnw Quarkus:add-extension -Dextensions="kafka,kafka-streams,smallrye-reactive-messaging-kafka"\n')),Object(s.b)("p",null,"Since we will be using the Kafka Streams testing functionality we will need to edit the ",Object(s.b)("inlineCode",{parentName:"p"},"pom.xml")," to add\nthe dependency to our project. Open ",Object(s.b)("inlineCode",{parentName:"p"},"pom.xml")," and add the following."),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-xml"}),"<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n")),Object(s.b)("p",null,"Modify the properties to add kafka, kafka-streams and reactive messaging parameters like"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-properties"}),"quarkus.kafka-streams.auto.offset.reset=latest\nquarkus.kafka-streams.health.enabled=true\nquarkus.kafka-streams.consumer.session.timeout.ms=7000\nquarkus.kafka-streams.consumer.heartbeat.interval.ms=200\nquarkus.kafka-streams.application-id=item-aggregator\nquarkus.kafka-streams.topics=items,inventory\n\nmp.messaging.incoming.item-channel.connector=smallrye-kafka\nmp.messaging.incoming.item-channel.topic=items\nmp.messaging.incoming.item-channel.group.id=item-aggregator\n")),Object(s.b)("h3",null,"Define an item deserializer"),Object(s.b)("p",null,"The item needs to be deserialized to a Item bean, so we add a new class:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-java"}),"import io.quarkus.kafka.client.serialization.JsonbDeserializer;\n\npublic class ItemDeserializer extends JsonbDeserializer<Item> {\n    public ItemDeserializer(){\n        // pass the class to the parent.\n        super(Item.class);\n    }\n}\n")),Object(s.b)("p",null,"and a declaration in the properties file (change the class name if needed):"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-properties"}),"mp.messaging.incoming.item-channel.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nmp.messaging.incoming.item-channel.value.deserializer=ibm.gse.eda.inventory.infrastructure.ItemDeserializer\n")),Object(s.b)("h3",null,"Define the topology"),Object(s.b)("p",null,"While in dev mode, we can add the ",Object(s.b)("inlineCode",{parentName:"p"},"StoreInventoryAgent")," class under the infrastructure folder. This class will define the topology to consume messages from the ",Object(s.b)("inlineCode",{parentName:"p"},"items")," topic. The Serdes are class to support the serialization and deserialization of the beans we define as part of the event model."),Object(s.b)("p",null,"We will start just by having a print out topology to get the plumbing done. So it will consume items topic:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"}),'@ApplicationScoped\npublic class StoreInventoryAgent {\n    \n    public String itemSoldTopicName = "items";\n\n    private JsonbSerde<Item> itemSerde = new JsonbSerde<>(Item.class);\n   \n\n    public Topology buildTopology(){\n        StreamsBuilder builder = new StreamsBuilder();\n        \n        builder.stream(itemSoldTopicName, \n            Consumed.with(Serdes.String(), itemSerde))\n            .peek( (k,v) -> System.out.println(k));\n\n        return builder.build();\n    }\n}\n')),Object(s.b)("h3",null,"Connect to Event Streams"),Object(s.b)("p",null,"We need to complete the configuration to connect to the remote Event Streams running on OpenShift."),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Create the items and inventory topics, following the instructions as described ","[in this note]","(../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:"),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"cloudctl es topic-create --name items --partitions 3 --replication-factor 3\ncloudctl es topic-create --name inventory --partitions 1 --replication-factor 3\ncloudctl es topics\n"))),Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"To connect from your computer to Event Streams running on OpenShift, we need to define a user with ",Object(s.b)("inlineCode",{parentName:"p"},"scram-sha-512")," password. ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://ibm.github.io/event-streams/getting-started/connecting/"}),"See product documentation")," on how to do it, or use our ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"http://localhost:8000/use-cases/overview/pre-requisites#get-shram-user"}),"quick summary here"),".")),Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Get Server TLS certificate. See our ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"http://localhost:8000/use-cases/overview/pre-requisites#get-tls-server-public-certificate"}),"quick summary here"))),Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Modify the ",Object(s.b)("inlineCode",{parentName:"p"},"application.properties")," file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka stream can create topics to backup state stores, and one for kafka streams consumer and producer tasks:"))),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-properties"}),'kafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\n%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\"${KAFKA_USER}\\" password\\=\\"${KAFKA_PASSWORD}\\";\n%dev.kafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=${USER_CERT_PATH}\n%prod.kafka.ssl.keystore.password=${USER_CERT_PWD}\n%prod.kafka.ssl.keystore.type=PKCS12\n')),Object(s.b)("p",null,"The above settings take into account that when running locally (",Object(s.b)("inlineCode",{parentName:"p"},"%dev")," profile) we use the ",Object(s.b)("inlineCode",{parentName:"p"},"scram-sha")," mechanism to authenticate, and when we deploy on openshift, the ",Object(s.b)("inlineCode",{parentName:"p"},"%prod")," profile is used with TLS mutual authentication  (client certificate in keystore)."),Object(s.b)("p",null,"The same approach applies for Kafka Stream:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),'quarkus.kafka-streams.bootstrap-servers=${KAFKA_BROKERS}\nquarkus.kafka-streams.security.protocol=${SECURE_PROTOCOL}\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\n%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\n%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\"${KAFKA_USER}\\" password\\=\\"${KAFKA_PASSWORD}\\";\nquarkus.kafka-streams.ssl.truststore.location=${KAFKA_CERT_PATH}\nquarkus.kafka-streams.ssl.truststore.password=${KAFKA_CERT_PWD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n# Only if TLS is used for authentication instead of scram\n%prod.quarkus.kafka-streams.ssl.keystore.location=${USER_CERT_PATH}\n%prod.quarkus.kafka-streams.ssl.keystore.password=${USER_CERT_PWD}\n%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12\n')),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Define a file, like ",Object(s.b)("inlineCode",{parentName:"p"},".env"),", to set environment variables, and modify the settings accordingly"),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),"KAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\nKAFKA_USER=\nKAFKA_PASSWORD=\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=\nSECURE_PROTOCOL=SASL_SSL\n"))),Object(s.b)("li",{parentName:"ul"},Object(s.b)("p",{parentName:"li"},"Restart the quarkus in dev mode"),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"source .env\n./mvnw quarkus:dev\n")),Object(s.b)("p",{parentName:"li"},"normally you should not get any exception and should get a trace like"),Object(s.b)("pre",{parentName:"li"},Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),"   AdminClientConfig values: \n   bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]\n   client.dns.lookup = default\n   client.id = \n   connections.max.idle.ms = 300000\n   default.api.timeout.ms = 60000\n   metadata.max.age.ms = 300000\n   metric.reporters = []\n   metrics.num.samples = 2\n   metrics.recording.level = INFO\n   metrics.sample.window.ms = 30000\n")))),Object(s.b)("h3",null,"Finish the topology"),Object(s.b)("p",null,"Now that we are connected to a kafka backbone, we need to finalize the stream topology. The requirements can be bullet listed as:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"out-topic: inventory: contains the inventory stock events."),Object(s.b)("li",{parentName:"ul"},"Ktable <storeID, <itemID, count> with store. To keep store inventory"),Object(s.b)("li",{parentName:"ul"},"Interactive query to get data from store and expose the result as reactive REST resource. We will cover this in next section.")),Object(s.b)("p",null,"To update the buildTopology function by getting the store and build a Ktable:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"})," KTable<String,Inventory> inventory = builder.stream(itemSoldTopicName, \n                        Consumed.with(Serdes.String(), itemSerde))\n            // use store name as key\n            .map((k,v) ->  new KeyValue<>(v.storeName, v))\n            .groupByKey(Grouped.with(Serdes.String(),itemSerde))\n       \n")),Object(s.b)("p",null,"Then the operation to take this <storeName, item> record and transform it to Inventory instance, and update existing inventory entry is the ",Object(s.b)("inlineCode",{parentName:"p"},"aggregate")," function:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"}),".aggregate(\n      () ->  new Inventory(), // initializer\n      (k , newItem, currentInventory) \n            -> currentInventory.updateStockQuantity(k,newItem), \n      Materialized.<String,Inventory,KeyValueStore<Bytes,byte[]>>as(StoreInventoryAgent.STOCKS_STORE_NAME)\n            .withKeySerde(Serdes.String())\n            .withValueSerde(inventorySerde));\n")),Object(s.b)("p",null,"First row is to initialize new key, record with an empty Inventory object.\nThe second row is executed when a key is found (first key too), and update the currentInventory with the new quantity from the item. The outcome of this is a Ktable<storeName, Inventory>\nThe content is materialized in a state store."),Object(s.b)("p",null,"Finally the KTable is streamed out to the inventory topic:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"}),"inventory.toStream()\n            .to(inventoryStockTopicName,\n                Produced.with(Serdes.String(),inventorySerde));\n      \n")),Object(s.b)("p",null,"The KTable is also materialized as a store that can be accessed via an API like ",Object(s.b)("inlineCode",{parentName:"p"},"/inventory/store/{storeid}/{itemid}")," using interactive query."),Object(s.b)("p",null,"As items topic can be partitioned, a REST call may not reach the good end points, as the local store may not have the expected queried key. So the code is using interactive query to get access to the local state stores or return a URL of a remote store where the records for the given key are."),Object(s.b)("h2",null,"Topology test"),Object(s.b)("p",null,"We already presented how to use the TopologyTestDriver in previous labs. The class for testing is ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/test/java/ut/TestInventory.java"}),"ut.TestInventory.java"),"."),Object(s.b)("h2",null,"Interaction query"),Object(s.b)("p",null,"Now as presented in ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"../../technology/kafka-streams"}),"this note"),", as soon as we use KTable materialized with state store we can use query to get the last state of the records saved.\nThe API returns a query result on the inventory. We can define such bean as:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"}),"public class InventoryQueryResult {\n    private static InventoryQueryResult NOT_FOUND = new InventoryQueryResult(null, null, null);\n    private final Inventory result;\n    private final String host;\n    private final Integer port;\n\n    public static InventoryQueryResult notFound() {\n        return NOT_FOUND;\n    }\n\n    public Optional<Inventory> getResult() {\n        return Optional.ofNullable(result);\n    }\n}\n")),Object(s.b)("p",null,"So the Resource class is not"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-java"}),'@GET\n@Path("/store/{storeID}")\n@Produces(MediaType.APPLICATION_JSON)\npublic Uni<Inventory> getStock(@PathParam("storeID") String storeID) {\n    InventoryQueryResult result = queries.getStoreStock(storeID);\n    if (result.getResult().isPresent()) {\n        return Uni.createFrom().item(result.getInventory());\n    } else {\n        return  Uni.createFrom().item(InventoryQueryResult.notFound());\n    }\n}\n')),Object(s.b)("p",null,"The queries is the new class to support interactive query. The principle is simple, we need to access the store that has the storeID key we search for. But there is a small problem, due to the fact that the input topic may be partitioned so the local store may not have the data for the given key. Therefore Kafka streams offers an API to get metadata of the store allocation between nodes for the Kafka Streams."),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-Java"})," @Inject\nKafkaStreams streams;\n\nmetadata = streams.queryMetadataForKey(\n    StoreInventoryAgent.STOCKS_STORE_NAME,\n        storeID,\n        Serdes.String().serializer());\n    ...\n    if (metadata.getActiveHost().host().equals(host)) {\n        Inventory result = getStockStore().get(storeID);\n        return InventoryQueryResult.found(result);\n    } else {\n        // call remote or propagate to ask the client to call the other host\n        return InventoryQueryResult.foundRemotely(metadata.getActiveHost());\n    }\n")),Object(s.b)("h2",null,"API"),Object(s.b)("p",null,"Now we want to complete our APIs by adding information on the store metadata from URL ",Object(s.b)("inlineCode",{parentName:"p"},"/meta-data"),". The method to add to the Resource class is:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-java"}),'@GET\n@Path("/meta-data")\n@Produces(MediaType.APPLICATION_JSON)\npublic Multi<PipelineMetadata> getMetaData() {\n    return Multi.createFrom().items(queries.getStockStoreMetaData().stream());\n}\n')),Object(s.b)("p",null,Object(s.b)("em",{parentName:"p"},"It is possible, while testing the API, to get a 404 response. The execption may be linked to the state of the kafka stream processing: for example something like: ",Object(s.b)("inlineCode",{parentName:"em"},"java.lang.IllegalStateException: KafkaStreams is not running. State is CREATED."),". This may be due to the test data we have, as once kafka stream for a specific group-id has consumed the records then the offsets are committed, and a new start will not process the old records. Changing the application-id properties can re-read all the records from offset 0.")," "),Object(s.b)("h2",null,"Integration tests"),Object(s.b)("p",null," For running the integration test, we propose to copy the e2e folder from the solution repository and follow the ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing"}),"readme instructions section end-to-end-testing "),"."),Object(s.b)("h2",null,"Deploy to OpenShift"),Object(s.b)("p",null,"Be sure to have done ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift"}),"the steps described here")," to get user credentials and Server side certificate. "),Object(s.b)("p",null,"The deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.",Object(s.b)("br",{parentName:"p"}),"\n","Here are the interesting properties to set environment variables from secrets "),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-properties"}),"%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL\nquarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\nquarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12\nquarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password\nquarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12\nquarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.USER_CERT_PWD.value=user.password\n")),Object(s.b)("p",null,"And an extract of the expected generated openshift manifests from those configurations:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-yaml"}),"    spec:\n      containers:\n      - env:\n        - name: KAFKA_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: ca.password\n              name: sandbox-rp-cluster-ca-cert\n        - name: USER_CERT_PATH\n          value: /deployments/certs/user/user.p12\n        - name: USER_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: KAFKA_BROKERS\n          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n        - name: KAFKA_CERT_PATH\n          value: /deployments/certs/server/ca.p12\n        - name: KAFKA_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: SECURE_PROTOCOL\n          value: SASL_SSL\n")),Object(s.b)("p",null,"Finally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),"quarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred\n")),Object(s.b)("p",null,"which generates:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{}),'        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: es-cert\n          readOnly: false\n          subPath: ""\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: ""\n')),Object(s.b)("p",null,"Now any deployment using the following command should work:"),Object(s.b)("pre",null,Object(s.b)("code",Object(n.a)({parentName:"pre"},{className:"language-shell"}),"./mvnw clean package -DQuarkus.kubernetes.deploy=true\n")),Object(s.b)("p",null,"The last piece is to go to EventStreams console and look at the inventory topic for messages generated. As an alternate we could use ",Object(s.b)("a",Object(n.a)({parentName:"p"},{href:"../../overview/pre-requisites#using-kafdrop"}),"Kafdrop"),"."))}d.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-use-cases-kafka-streams-lab-3-index-mdx-a723ddcb2d4705267047.js.map