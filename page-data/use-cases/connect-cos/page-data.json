{"componentChunkName":"component---src-pages-use-cases-connect-cos-index-mdx","path":"/use-cases/connect-cos/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to IBM COS","description":"Apache Kafka to IBM Cloud Object Storage Source Connector usecase"},"relativePagePath":"/use-cases/connect-cos/index.mdx","titleType":"append","MdxNode":{"id":"59a7d688-86ad-5326-a261-a86bcbd504a8","children":[],"parent":"ad3d5018-6410-5bb0-ac3c-ce3730f16020","internal":{"content":"---\ntitle: Kafka Connect to IBM COS\ndescription: Apache Kafka to IBM Cloud Object Storage Source Connector usecase\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Scenario Prerequisites</AnchorLink>\n  <AnchorLink>Create Event Streams Topic</AnchorLink>\n  <AnchorLink>Create Producer Application</AnchorLink>\n  <AnchorLink>Create an IBM COS Service and COS Bucket</AnchorLink>\n  <AnchorLink>Create IBM COS Service Credentials</AnchorLink>\n  <AnchorLink>Set up the Kafka Connect Cluster</AnchorLink>\n  <AnchorLink>Build and Inject IBM COS Sink Connector</AnchorLink>\n  <AnchorLink>Apply IBM COS Sink Connector</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nNow that you have an Event Streams instance installed on Cloud Pak for Integration on top of OpenShift Container Platform the goal of this story is to show a possible use case that we can use with this technology. With IBM Event Streams we have access to the powerful capabilities of Kafka in addition to all the monitoring and logging capabilities that IBM provides on top of that with Event Streams.\n\nWe will create a simple Quarkus (a super sonic and sub-atomic Kubernetes native framework for Java) application that utilizes MicroProfile Reactive Messaging in order for us to send a stream of data to our Event Streams/Kafka topic. We will then create a Kafka Connect cluster using the Strimzi Operator. Lastly we'll send messages to an Event Streams topic from our Quarkus application which then triggers the IBM COS Connector to grab messages and place into an IBM COS Bucket.\n\n![Architecture Diagram](./images/quarkus-to-event-streams-to-cos.png)\n\n## Scenario Prerequisites\n\n**OpenShift Container Platform Cluster**\n  - This scenario will assume you have a 4.x Cluster as we will make use of Operators, though this one is 4.3 specifically.\n\n**Cloud Pak for Integration**\n  - This will assume you have probably at least a 2019.4.1 or 2020.x.x release of the Cloud Pak for Integration installed on OpenShift. This story will also assume you have followed the installation instructions for Event Streams outlined in [the 2020-2 product documentation](https://ibm.github.io/event-streams/installing/installing/) or from the [Cloud Pak Playbook](https://cloudpak8s.io/integration/cp4i-deploy-eventstreams/) and have a working Event Streams instance.\n\n**Java**\n  - Java Development Kit (JDK) v1.8+ (Java 8+)\n\n**Maven**\n  - The scenario uses Maven v3.6.3\n\n**Gradle**\n  - Ideally v4.0+ (Note - the gradle shadowJar command might not work on Java versions newer to Java 8)\n\n**An IDE of your choice**\n  - Visual Studio Code is used in this scenario.\n\n**Git**\n  - We will need to clone repositories.\n\n**An IBM Cloud Account (free)**\n  - A free (Lite) IBM Cloud Object Storage trial Service account [IBM Cloud Object Storage](https://cloud.ibm.com/catalog/services/cloud-object-storage)\n\n## Create Event Streams Topic\n\nIn this section, we will need to create a single `INBOUND` topic. The `INBOUND` topic is where our Quarkus application will produce to and where our IBM Cloud Object Storage Sink Connector will pull data from to send to the COS Bucket. Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) section to understand how to create a topic in IBM Event Streams.\n\n## Create Producer Application\n\nIn this section, we are going to create a producer application to send messages into the IBM Event Streams topic we created in the previou section. This application will be a Java application based on Quarkus, a supersonic subatomic Java framework, which will also make use of the MicroProfile Reactive Messaging specification.\n\n1. Create the Quarkus project.\n\n  ```bash\n  mvn io.quarkus:quarkus-maven-plugin:1.6.0.Final:create \\\n      -DprojectGroupId=org.acme \\\n      -DprojectArtifactId=quarkus-kafka \\\n      -Dextensions=\"kafka,resteasy-jsonb\"\n  ```\n\n1. Create the following Java file in the following path.\n\n  ```bash\n  src/main/java/org/acme/kafka/producer/Producer.java\n  ```\n\n  ![Quarkus Project Folder Structure](./images/quarkus-folder-structure.png)\n\n1. Within your `Producer.java` file, add the following code:\n\n  ```java\n  package org.acme.kafka.producer;\n\n  import io.reactivex.Flowable;\n  import io.smallrye.reactive.messaging.kafka.KafkaRecord;\n\n  import org.eclipse.microprofile.reactive.messaging.Outgoing;\n\n  import javax.enterprise.context.ApplicationScoped;\n  import java.util.Random;\n  import java.util.concurrent.TimeUnit;\n\n  /**\n   * This class produces a message every 5 seconds.\n   * The Kafka configuration is specified in the application.properties file.\n  */\n  @ApplicationScoped\n  public class Producer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"INBOUND\")\n    public Flowable<KafkaRecord<Integer, String>> generate() {\n      return Flowable.interval(5, TimeUnit.SECONDS)\n        .onBackpressureDrop()\n        .map(tick -> {\n          return KafkaRecord.of(random.nextInt(100), String.valueOf(random.nextInt(100)));\n        });\n    }\n  }\n  ```\n\n  The `@Outgoing` annotation is for specifying the name of the [channel](https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/2/concepts.html#channels), but it will default to that channel's name if a topic name is not provided in the `application.properties` file. We will address that a little bit later.\n\n  What does this `Producer.java` code do?\n    * The `@Outgoing` annotation indicates that we're sending to a channel (or topic) and we're not expecting any data.\n    * The `generate()` function returns an [RX Java 2 Flowable Object](https://www.baeldung.com/rxjava-2-flowable) emmitted every 5 seconds.\n    * The `Flowable` object returns a `KafkaRecord` of type key type Integer and value type String.\n\n1. Update our `applications.properties` file that was automatically generated when the Quarkus project was created at `src/main/resources` with the following code.\n\n  ```properties\n\n  quarkus.http.port=8080\n  quarkus.log.console.enable=true\n  quarkus.log.console.level=INFO\n\n  # Event Streams Connection details\n  mp.messaging.connector.smallrye-kafka.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  mp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\n  mp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\n  mp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\n  mp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                  username=REPLACE_WITH_YOUR_SCRAM_USERNAME \\\n                  password=REPLACE_WITH_YOUR_SCRAM_PASSWORD;\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.location=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n\n  # Initial mock JSON message producer configuration\n  mp.messaging.outgoing.INBOUND.connector=smallrye-kafka\n  mp.messaging.outgoing.INBOUND.topic=REPLACE_WITH_YOUR_TOPIC\n  mp.messaging.outgoing.INBOUND.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  mp.messaging.outgoing.INBOUND.key.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  ```\n\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION`: The location where you downloaded your PCKS12 TLS certificate to.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above.\n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n1. Run the producer application.\n\n  ```bash\n  ./mvnw quarkus:dev\n  ```\n\n1. Since the code sends a message every 5 seconds, you can leave it on for a bit. Check out messages are making it into the yopic using your IBM Event Streams user interface. You can click the message under \"Indexed Timestamp\" to see the contents and details of the message.\n\n  ![ES Topic Messages](./images/event-streams-topic-messages.png)\n\n\n## Create an IBM COS Service and COS Bucket\n\nIn this section, we are going to see how to create an IBM Cloud Obeject Storage (IBM COS) Service in your IBM Cloud account and a bucket within your IBM COS Service. We assume you already have an IBM Cloud account already and, if not, you can sign up for one here at [IBM Cloud](https://cloud.ibm.com).\n\n1. Once you are inside your IBM Cloud account, traverse to the `Catalog` section. In the search type in `IBM Cloud Object Storage`\n\n  ![IBM COS Catalog Search](./images/ibm-cloud-create-cos-service.png)\n\n1. Name your IBM COS Service with something unique. Since this is a free account, we can stick with the `Lite Plan`.\n\n  ![IBM COS Create COS Service](./images/ibm-cloud-create-cos-service-2.png)\n\n1.  Now that the IBM Cloud Object Storage Service is created, traverse to it and let's create a new bucket. On the `Create Bucket` screen pick `Custom Bucket`.\n\n  ![IBM COS Custom Bucket](./images/ibm-cos-create-bucket.png)\n\n1. When selecting options for the bucket, name your bucket something unique. For `Resiliency` let's select `Regional`. For location select an area from the drop-down that you want. **IMPORTANT:** for `Storage Class` select `Standard`. The IBM COS Sink connector seems to not play well with buckets that are created with the `Smart Tier` Storage Class. Leave everything else as-is and hit `Create Bucket`.\n\n  ![IBM COS Custom Bucket Settings](./images/ibm-cos-bucket-settings.png)\n\n## Create IBM COS Service Credentials\n\nNow that we have created our IBM Cloud Object Storage Service and bucket created, we now need to create the Service Credential so that we can connect to it.\n\n1. Inside your IBM COS Service, select `Service Credentials` and then click the `New Credential` button.\n\n  ![IBM COS Service Credential](./images/ibm-cos-create-service-cred.png)\n\n1. Name your credential and select `Manager` from the `Role:` drop-down menu and click `Add`.\n\n  ![IBM COS SC Settings](./images/ibm-cos-service-credentials.png)\n\n1. Expand your newly created Service Credential and write down the values for `\"apikey\"` and `\"resource_instance_id\"`. You will need this later in the [Build and Apply IBM COS Sink Connector](#build-and-apply-ibm-cos-sink-connector) section.\n\n  ![Expanded Service Cred](./images/ibm-service-credential-keys.png)\n\n## Set up the Kafka Connect Cluster\n\nIn this section, we are going to see how to deploy a [Kafka Connect](https://kafka.apache.org/documentation/#connect) cluster on OpenShift which will be the engine running the source and sink connector we decide to use for our use case. **IMPORTANT:** We assume you have deployed your IBM Event Streams instance with an **internal TLS secured listener** which your Kafka Connect cluster will use to connect. For more detail about listeners, check the IBM Event Streams documentation [here](https://ibm.github.io/event-streams/installing/configuring/#kafka-access).\n\nIf you inspect your IBM Event Streams instance by executing the following command:\n\n```shell\noc get EventStreams YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME -o yaml\n```\n\nYou should see a `Tls` listener:\n\n![connect5](./images/connect-5.png)\n\nNow, follow the next steps in order to get your Kafka Connect cluster deployed:\n\n1. Go to you IBM Event Streams dashboard, click on the `Find more on the toolbox` option.\n\n  ![Toolbox1](./images/connect-1.png)\n\n1. Click on the `Set up` button for the `Set up a Kafka Connect environment` option.\n\n  ![Toolbox2](./images/connect-2.png)\n\n1. Click on `Download Kafka Connect ZIP` button.\n\n  ![Toolbox3](./images/connect-3.png)\n\n1. The above downloads a zip file which contains a `kafka-connect-s2i.yaml` file. Open that yaml file and take note of the `productID` and `cloudpakId` values as you will need these in the following step.\n\n  ![connect4](./images/connect-4.png)\n\n1. Instead of using the previous yaml file, create a new `kafka-connect-s2i.yaml` file with the following contents:\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1beta1\n  kind: KafkaConnectS2I\n  metadata:\n    name: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n    annotations:\n      eventstreams.ibm.com/use-connector-resources: \"true\"\n  spec:\n    logging:\n      type: external\n      name: custom-connect-log4j\n    version: 2.6.0\n    replicas: 1\n    bootstrapServers: YOUR_INTERNAL_BOOTSTRAP_ADDRESS\n    template:\n      pod:\n        imagePullSecrets: []\n        metadata:\n          annotations:\n            eventstreams.production.type: CloudPakForIntegrationNonProduction\n            productID: YOUR_PRODUCT_ID\n            productName: IBM Event Streams for Non Production\n            productVersion: 10.2.0\n            productMetric: VIRTUAL_PROCESSOR_CORE\n            productChargedContainers: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n            cloudpakId: YOUR_CLOUDPAK_ID\n            cloudpakName: IBM Cloud Pak for Integration\n            cloudpakVersion: 2020.4.1\n            productCloudpakRatio: \"2:1\"\n    tls:\n        trustedCertificates:\n          - secretName: YOUR_CLUSTER_TLS_CERTIFICATE_SECRET\n            certificate: ca.crt\n    authentication:\n      type: tls\n      certificateAndKey:\n        certificate: user.crt\n        key: user.key\n        secretName: YOUR_TLS_CREDENTIALS_SECRET\n    config:\n      group.id: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n      key.converter: org.apache.kafka.connect.json.JsonConverter\n      value.converter: org.apache.kafka.connect.json.JsonConverter\n      key.converter.schemas.enable: false\n      value.converter.schemas.enable: false\n      offset.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-offsets\n      config.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-configs\n      status.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-status\n      config.storage.replication.factor: 1\n      offset.storage.replication.factor: 1\n      status.storage.replication.factor: 1\n  ```\n\n  where you will need to replace the following placeholders with the appropriate values for you IBM Event Streams cluster and service credentials:\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: A name you want to provide your Kafka Connect cluster and resources with.\n   * `YOUR_INTERNAL_BOOTSTRAP_ADDRESS`: This is the internal bootstrap address of your IBM Event Streams instance. You can review how to find this url [here](/use-cases/overview/pre-requisites#get-kafka-bootstrap-url). Use the **internal** bootstrap address which should be in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-kafka-bootstrap.eventstreams.svc:9093`:\n   ![connect6](./images/connect-6.png)\n   * `YOUR_TLS_CREDENTIALS_SECRET`: This is the name you give to your TLS credentials for your internal IBM Event Streams listener when you click on `Generate TLS credentials`:\n   ![connect7](./images/connect-7.png)\n   * `YOUR_CLUSTER_TLS_CERTIFICATE_SECRET`: This is the secret name where IBM Event Streams stores the TLS certificate for establishing secure communications. This secret name is in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-cluster-ca-cert`. You can always use `oc get secrets` to list all the secrets.\n   * `YOUR_PRODUCT_ID`: This is the `productID` value you noted down earlier.\n   * `YOUR_CLOUDPAK_ID`: This is the `cloudpakID` value you noted earlier.  \n\n1. Deploy your Kafka Connect cluster by executing\n\n  ```shell\n  oc apply -f kafkaconnect-s2i.yaml \n  ```\n\n1. If you list the pods, you should see three new pods: one for the Kafka Connect build task, another for the Kafka Connect deploy task and the actual Kafka Connect cluster pod.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-xxxxx      1/1     Running     0          17m\n  ```\n \n## Build and Inject IBM COS Sink Connector\n\nThe IBM COS Sink Connector source code is availabe at this repository [here](https://github.com/ibm-messaging/kafka-connect-ibmcos-sink).\n\n**IMPORTANT:** Make sure you have **Java 8** installed on your workstation and that is the default Java version of your system since the IBM COS Sink Connector can only be built with that version of Java.\n\n1. Clone the Kafka Connect IBM COS Source Connector repository and then change your folder.\n\n  ```shell\n  git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink.git\n  cd kafka-connect-ibmcos-sink/\n  ```\n\n1. Build the connect using `Gradle`.\n\n  ```shell\n  gradle shadowJar\n  ```\n\n1. The newly built connector binaries are in the `build/libs/` folder. Move it into a `connectors` folder for ease of use.\n\n  ```shell\n  mkdir connectors\n  cp build/libs/kafka-connect-ibmcos-sink-*-all.jar connectors/\n  ```\n\n1. Now that we have the connector in the `connectors/` folder, we somehow need embed it into our Kakfa Connect cluster. For that, we need to trigger another build for our Kafka Connect cluster but this time specifying the files we want to get embedded. What the followin command does is it builds a new image with your provided connectors/plugins and triggers a new deployment for your Kafka Connect cluster.\n\n  ```shell\n  oc start-build connect-cluster-101-connect --from-dir ./connectors/ --follow\n  ```\n\n1. Since the last commands triggers a new build, we should now see three new pods for the build task, the deploy task and the resulting Kafka Connect cluster. Also, we should see the previous Kafka Connect cluster pod if gone.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx      1/1     Running     0          17m\n  ```\n\n1. Once the new Kafka Connect cluster pod is up and running we can actually exec into it\n\n  ```shell\n  oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash\n  ```\n\n1. to see if there is any connector already set up, which should not be the case\n\n  ```shell\n  curl localhost:8083/connectors\n  []\n  ```\n\n## Apply IBM COS Sink Connector\n\nIn this section, we are going to see how to set up, apply or register the IBM COS Sink Connector we embedded into our Kafka Connect cluster so that it starts sending messages from our `INBOUND` topic into our IBM Cloud Object Storage bucket.\nproceed.\n\n1. Create a new file named `kafka-cos-sink-connector.yaml` and past the following code in it.\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1alpha1\n  kind: KafkaConnector\n  metadata:\n    name: cos-sink-connector\n    labels:\n      eventstreams.ibm.com/cluster: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n  spec:\n    class: com.ibm.eventstreams.connect.cossink.COSSinkConnector\n    tasksMax: 1\n    config:\n      key.converter: org.apache.kafka.connect.storage.StringConverter\n      value.converter: org.apache.kafka.connect.storage.StringConverter\n      topics: TOPIC_NAME\n      cos.api.key: IBM_COS_API_KEY\n      cos.bucket.location: IBM_COS_BUCKET_LOCATION\n      cos.bucket.name: IBM_COS_BUCKET_NAME\n      cos.bucket.resiliency: IBM_COS_RESILIENCY\n      cos.service.crn: \"IBM_COS_CRM\"\n      cos.object.records: 5\n      cos.object.deadline.seconds: 5\n      cos.object.interval.seconds: 5\n  ```\n\n  where\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: is the name you gave previously to your Kakfa Connect cluster.\n   * `TOPIC_NAME`: is the name of the topic you created in IBM Event Streams at the beginning of this lab.\n   * `IBM_COS_API_KEY`: is your IBM Cloud Object Storage service credentials `apikey` value. Review first sections of this lab if you don't remember where and how to find this value.\n   * `IBM_COS_BUCKET_LOCATION`: is your IBM Cloud Object Storage bucket location. Review first sections of this lab if you don't remember where and how to find this value (it usually is in the form of something like `us-east` or `eu-gb` for example).\n   * `IBM_COS_RESILIENCY`: is your IBM Cloud Object Storage resiliency option. Review first sections of this lab if you don't remember where and how to find this value (it should be `regional`).\n   * `IBM_COS_CRM`: is your IBM Cloud Object Storage CRN. Review first sections of this lab if you don't remember where and how to find this value. It usually ends with a double `::` at the end of it. **IMPORTANT:** you might need to retain the double quotation marks here as the CRN has colons in it and may collide with yaml syntax.\n\n1. Apply the yaml which will create a `KafkaConnnector` custom resource behind the scenes and register/set up the IBM COS Sink Connector in your Kafka Connect cluster.\n\n  ```shell\n  oc apply -f kafka-cos-sink-connector.yaml\n  ```\n\n1. The initialization of the connector can take a minute or two. You can check the status of the connector to see if everything connected succesfully.\n\n  ```shell\n  oc describe kafkaconnector cos-sink-connector\n  ```\n\n1. When the IBM COS Sink connector is successfully up and running you should see something similar to the below.\n\n  ![IBM COS Sink Connector success](./images/ibm-cos-sink-connector-success.png)\n\n1. You should also see a new connector being registered if you exec into the Kafka Connect cluster pod and query for the existing connectors again:\n\n  ```shell\n  oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash\n  bash-4.4$ curl localhost:8083/connectors\n  [\"cos-sink-connector\"]\n  ```\n\n1. Finally, we can check whether the messages from our IBM Event Streams topic are getting propagated to our IBM Cloud Object Storage bucket. If you go to your IBM COS bucket, you should find some files in it. The name of the file inside the bucket has starting offset and ending offset. You can download one of these object files to make sure that the value inside matches the value inside your `INBOUND` topic.\n\n  ![End to End Success](./images/ibm-cos-bucket-success.png)\n","type":"Mdx","contentDigest":"5bcd7ad2e71f5797a012ee62b362e9a0","owner":"gatsby-plugin-mdx","counter":723},"frontmatter":{"title":"Kafka Connect to IBM COS","description":"Apache Kafka to IBM Cloud Object Storage Source Connector usecase"},"exports":{},"rawBody":"---\ntitle: Kafka Connect to IBM COS\ndescription: Apache Kafka to IBM Cloud Object Storage Source Connector usecase\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Scenario Prerequisites</AnchorLink>\n  <AnchorLink>Create Event Streams Topic</AnchorLink>\n  <AnchorLink>Create Producer Application</AnchorLink>\n  <AnchorLink>Create an IBM COS Service and COS Bucket</AnchorLink>\n  <AnchorLink>Create IBM COS Service Credentials</AnchorLink>\n  <AnchorLink>Set up the Kafka Connect Cluster</AnchorLink>\n  <AnchorLink>Build and Inject IBM COS Sink Connector</AnchorLink>\n  <AnchorLink>Apply IBM COS Sink Connector</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nNow that you have an Event Streams instance installed on Cloud Pak for Integration on top of OpenShift Container Platform the goal of this story is to show a possible use case that we can use with this technology. With IBM Event Streams we have access to the powerful capabilities of Kafka in addition to all the monitoring and logging capabilities that IBM provides on top of that with Event Streams.\n\nWe will create a simple Quarkus (a super sonic and sub-atomic Kubernetes native framework for Java) application that utilizes MicroProfile Reactive Messaging in order for us to send a stream of data to our Event Streams/Kafka topic. We will then create a Kafka Connect cluster using the Strimzi Operator. Lastly we'll send messages to an Event Streams topic from our Quarkus application which then triggers the IBM COS Connector to grab messages and place into an IBM COS Bucket.\n\n![Architecture Diagram](./images/quarkus-to-event-streams-to-cos.png)\n\n## Scenario Prerequisites\n\n**OpenShift Container Platform Cluster**\n  - This scenario will assume you have a 4.x Cluster as we will make use of Operators, though this one is 4.3 specifically.\n\n**Cloud Pak for Integration**\n  - This will assume you have probably at least a 2019.4.1 or 2020.x.x release of the Cloud Pak for Integration installed on OpenShift. This story will also assume you have followed the installation instructions for Event Streams outlined in [the 2020-2 product documentation](https://ibm.github.io/event-streams/installing/installing/) or from the [Cloud Pak Playbook](https://cloudpak8s.io/integration/cp4i-deploy-eventstreams/) and have a working Event Streams instance.\n\n**Java**\n  - Java Development Kit (JDK) v1.8+ (Java 8+)\n\n**Maven**\n  - The scenario uses Maven v3.6.3\n\n**Gradle**\n  - Ideally v4.0+ (Note - the gradle shadowJar command might not work on Java versions newer to Java 8)\n\n**An IDE of your choice**\n  - Visual Studio Code is used in this scenario.\n\n**Git**\n  - We will need to clone repositories.\n\n**An IBM Cloud Account (free)**\n  - A free (Lite) IBM Cloud Object Storage trial Service account [IBM Cloud Object Storage](https://cloud.ibm.com/catalog/services/cloud-object-storage)\n\n## Create Event Streams Topic\n\nIn this section, we will need to create a single `INBOUND` topic. The `INBOUND` topic is where our Quarkus application will produce to and where our IBM Cloud Object Storage Sink Connector will pull data from to send to the COS Bucket. Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) section to understand how to create a topic in IBM Event Streams.\n\n## Create Producer Application\n\nIn this section, we are going to create a producer application to send messages into the IBM Event Streams topic we created in the previou section. This application will be a Java application based on Quarkus, a supersonic subatomic Java framework, which will also make use of the MicroProfile Reactive Messaging specification.\n\n1. Create the Quarkus project.\n\n  ```bash\n  mvn io.quarkus:quarkus-maven-plugin:1.6.0.Final:create \\\n      -DprojectGroupId=org.acme \\\n      -DprojectArtifactId=quarkus-kafka \\\n      -Dextensions=\"kafka,resteasy-jsonb\"\n  ```\n\n1. Create the following Java file in the following path.\n\n  ```bash\n  src/main/java/org/acme/kafka/producer/Producer.java\n  ```\n\n  ![Quarkus Project Folder Structure](./images/quarkus-folder-structure.png)\n\n1. Within your `Producer.java` file, add the following code:\n\n  ```java\n  package org.acme.kafka.producer;\n\n  import io.reactivex.Flowable;\n  import io.smallrye.reactive.messaging.kafka.KafkaRecord;\n\n  import org.eclipse.microprofile.reactive.messaging.Outgoing;\n\n  import javax.enterprise.context.ApplicationScoped;\n  import java.util.Random;\n  import java.util.concurrent.TimeUnit;\n\n  /**\n   * This class produces a message every 5 seconds.\n   * The Kafka configuration is specified in the application.properties file.\n  */\n  @ApplicationScoped\n  public class Producer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"INBOUND\")\n    public Flowable<KafkaRecord<Integer, String>> generate() {\n      return Flowable.interval(5, TimeUnit.SECONDS)\n        .onBackpressureDrop()\n        .map(tick -> {\n          return KafkaRecord.of(random.nextInt(100), String.valueOf(random.nextInt(100)));\n        });\n    }\n  }\n  ```\n\n  The `@Outgoing` annotation is for specifying the name of the [channel](https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/2/concepts.html#channels), but it will default to that channel's name if a topic name is not provided in the `application.properties` file. We will address that a little bit later.\n\n  What does this `Producer.java` code do?\n    * The `@Outgoing` annotation indicates that we're sending to a channel (or topic) and we're not expecting any data.\n    * The `generate()` function returns an [RX Java 2 Flowable Object](https://www.baeldung.com/rxjava-2-flowable) emmitted every 5 seconds.\n    * The `Flowable` object returns a `KafkaRecord` of type key type Integer and value type String.\n\n1. Update our `applications.properties` file that was automatically generated when the Quarkus project was created at `src/main/resources` with the following code.\n\n  ```properties\n\n  quarkus.http.port=8080\n  quarkus.log.console.enable=true\n  quarkus.log.console.level=INFO\n\n  # Event Streams Connection details\n  mp.messaging.connector.smallrye-kafka.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\n  mp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\n  mp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\n  mp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\n  mp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                  username=REPLACE_WITH_YOUR_SCRAM_USERNAME \\\n                  password=REPLACE_WITH_YOUR_SCRAM_PASSWORD;\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.location=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION\n  mp.messaging.connector.smallrye-kafka.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n\n  # Initial mock JSON message producer configuration\n  mp.messaging.outgoing.INBOUND.connector=smallrye-kafka\n  mp.messaging.outgoing.INBOUND.topic=REPLACE_WITH_YOUR_TOPIC\n  mp.messaging.outgoing.INBOUND.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  mp.messaging.outgoing.INBOUND.key.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n  ```\n\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION`: The location where you downloaded your PCKS12 TLS certificate to.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above.\n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n1. Run the producer application.\n\n  ```bash\n  ./mvnw quarkus:dev\n  ```\n\n1. Since the code sends a message every 5 seconds, you can leave it on for a bit. Check out messages are making it into the yopic using your IBM Event Streams user interface. You can click the message under \"Indexed Timestamp\" to see the contents and details of the message.\n\n  ![ES Topic Messages](./images/event-streams-topic-messages.png)\n\n\n## Create an IBM COS Service and COS Bucket\n\nIn this section, we are going to see how to create an IBM Cloud Obeject Storage (IBM COS) Service in your IBM Cloud account and a bucket within your IBM COS Service. We assume you already have an IBM Cloud account already and, if not, you can sign up for one here at [IBM Cloud](https://cloud.ibm.com).\n\n1. Once you are inside your IBM Cloud account, traverse to the `Catalog` section. In the search type in `IBM Cloud Object Storage`\n\n  ![IBM COS Catalog Search](./images/ibm-cloud-create-cos-service.png)\n\n1. Name your IBM COS Service with something unique. Since this is a free account, we can stick with the `Lite Plan`.\n\n  ![IBM COS Create COS Service](./images/ibm-cloud-create-cos-service-2.png)\n\n1.  Now that the IBM Cloud Object Storage Service is created, traverse to it and let's create a new bucket. On the `Create Bucket` screen pick `Custom Bucket`.\n\n  ![IBM COS Custom Bucket](./images/ibm-cos-create-bucket.png)\n\n1. When selecting options for the bucket, name your bucket something unique. For `Resiliency` let's select `Regional`. For location select an area from the drop-down that you want. **IMPORTANT:** for `Storage Class` select `Standard`. The IBM COS Sink connector seems to not play well with buckets that are created with the `Smart Tier` Storage Class. Leave everything else as-is and hit `Create Bucket`.\n\n  ![IBM COS Custom Bucket Settings](./images/ibm-cos-bucket-settings.png)\n\n## Create IBM COS Service Credentials\n\nNow that we have created our IBM Cloud Object Storage Service and bucket created, we now need to create the Service Credential so that we can connect to it.\n\n1. Inside your IBM COS Service, select `Service Credentials` and then click the `New Credential` button.\n\n  ![IBM COS Service Credential](./images/ibm-cos-create-service-cred.png)\n\n1. Name your credential and select `Manager` from the `Role:` drop-down menu and click `Add`.\n\n  ![IBM COS SC Settings](./images/ibm-cos-service-credentials.png)\n\n1. Expand your newly created Service Credential and write down the values for `\"apikey\"` and `\"resource_instance_id\"`. You will need this later in the [Build and Apply IBM COS Sink Connector](#build-and-apply-ibm-cos-sink-connector) section.\n\n  ![Expanded Service Cred](./images/ibm-service-credential-keys.png)\n\n## Set up the Kafka Connect Cluster\n\nIn this section, we are going to see how to deploy a [Kafka Connect](https://kafka.apache.org/documentation/#connect) cluster on OpenShift which will be the engine running the source and sink connector we decide to use for our use case. **IMPORTANT:** We assume you have deployed your IBM Event Streams instance with an **internal TLS secured listener** which your Kafka Connect cluster will use to connect. For more detail about listeners, check the IBM Event Streams documentation [here](https://ibm.github.io/event-streams/installing/configuring/#kafka-access).\n\nIf you inspect your IBM Event Streams instance by executing the following command:\n\n```shell\noc get EventStreams YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME -o yaml\n```\n\nYou should see a `Tls` listener:\n\n![connect5](./images/connect-5.png)\n\nNow, follow the next steps in order to get your Kafka Connect cluster deployed:\n\n1. Go to you IBM Event Streams dashboard, click on the `Find more on the toolbox` option.\n\n  ![Toolbox1](./images/connect-1.png)\n\n1. Click on the `Set up` button for the `Set up a Kafka Connect environment` option.\n\n  ![Toolbox2](./images/connect-2.png)\n\n1. Click on `Download Kafka Connect ZIP` button.\n\n  ![Toolbox3](./images/connect-3.png)\n\n1. The above downloads a zip file which contains a `kafka-connect-s2i.yaml` file. Open that yaml file and take note of the `productID` and `cloudpakId` values as you will need these in the following step.\n\n  ![connect4](./images/connect-4.png)\n\n1. Instead of using the previous yaml file, create a new `kafka-connect-s2i.yaml` file with the following contents:\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1beta1\n  kind: KafkaConnectS2I\n  metadata:\n    name: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n    annotations:\n      eventstreams.ibm.com/use-connector-resources: \"true\"\n  spec:\n    logging:\n      type: external\n      name: custom-connect-log4j\n    version: 2.6.0\n    replicas: 1\n    bootstrapServers: YOUR_INTERNAL_BOOTSTRAP_ADDRESS\n    template:\n      pod:\n        imagePullSecrets: []\n        metadata:\n          annotations:\n            eventstreams.production.type: CloudPakForIntegrationNonProduction\n            productID: YOUR_PRODUCT_ID\n            productName: IBM Event Streams for Non Production\n            productVersion: 10.2.0\n            productMetric: VIRTUAL_PROCESSOR_CORE\n            productChargedContainers: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n            cloudpakId: YOUR_CLOUDPAK_ID\n            cloudpakName: IBM Cloud Pak for Integration\n            cloudpakVersion: 2020.4.1\n            productCloudpakRatio: \"2:1\"\n    tls:\n        trustedCertificates:\n          - secretName: YOUR_CLUSTER_TLS_CERTIFICATE_SECRET\n            certificate: ca.crt\n    authentication:\n      type: tls\n      certificateAndKey:\n        certificate: user.crt\n        key: user.key\n        secretName: YOUR_TLS_CREDENTIALS_SECRET\n    config:\n      group.id: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n      key.converter: org.apache.kafka.connect.json.JsonConverter\n      value.converter: org.apache.kafka.connect.json.JsonConverter\n      key.converter.schemas.enable: false\n      value.converter.schemas.enable: false\n      offset.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-offsets\n      config.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-configs\n      status.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-status\n      config.storage.replication.factor: 1\n      offset.storage.replication.factor: 1\n      status.storage.replication.factor: 1\n  ```\n\n  where you will need to replace the following placeholders with the appropriate values for you IBM Event Streams cluster and service credentials:\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: A name you want to provide your Kafka Connect cluster and resources with.\n   * `YOUR_INTERNAL_BOOTSTRAP_ADDRESS`: This is the internal bootstrap address of your IBM Event Streams instance. You can review how to find this url [here](/use-cases/overview/pre-requisites#get-kafka-bootstrap-url). Use the **internal** bootstrap address which should be in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-kafka-bootstrap.eventstreams.svc:9093`:\n   ![connect6](./images/connect-6.png)\n   * `YOUR_TLS_CREDENTIALS_SECRET`: This is the name you give to your TLS credentials for your internal IBM Event Streams listener when you click on `Generate TLS credentials`:\n   ![connect7](./images/connect-7.png)\n   * `YOUR_CLUSTER_TLS_CERTIFICATE_SECRET`: This is the secret name where IBM Event Streams stores the TLS certificate for establishing secure communications. This secret name is in the form of `YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-cluster-ca-cert`. You can always use `oc get secrets` to list all the secrets.\n   * `YOUR_PRODUCT_ID`: This is the `productID` value you noted down earlier.\n   * `YOUR_CLOUDPAK_ID`: This is the `cloudpakID` value you noted earlier.  \n\n1. Deploy your Kafka Connect cluster by executing\n\n  ```shell\n  oc apply -f kafkaconnect-s2i.yaml \n  ```\n\n1. If you list the pods, you should see three new pods: one for the Kafka Connect build task, another for the Kafka Connect deploy task and the actual Kafka Connect cluster pod.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-xxxxx      1/1     Running     0          17m\n  ```\n \n## Build and Inject IBM COS Sink Connector\n\nThe IBM COS Sink Connector source code is availabe at this repository [here](https://github.com/ibm-messaging/kafka-connect-ibmcos-sink).\n\n**IMPORTANT:** Make sure you have **Java 8** installed on your workstation and that is the default Java version of your system since the IBM COS Sink Connector can only be built with that version of Java.\n\n1. Clone the Kafka Connect IBM COS Source Connector repository and then change your folder.\n\n  ```shell\n  git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink.git\n  cd kafka-connect-ibmcos-sink/\n  ```\n\n1. Build the connect using `Gradle`.\n\n  ```shell\n  gradle shadowJar\n  ```\n\n1. The newly built connector binaries are in the `build/libs/` folder. Move it into a `connectors` folder for ease of use.\n\n  ```shell\n  mkdir connectors\n  cp build/libs/kafka-connect-ibmcos-sink-*-all.jar connectors/\n  ```\n\n1. Now that we have the connector in the `connectors/` folder, we somehow need embed it into our Kakfa Connect cluster. For that, we need to trigger another build for our Kafka Connect cluster but this time specifying the files we want to get embedded. What the followin command does is it builds a new image with your provided connectors/plugins and triggers a new deployment for your Kafka Connect cluster.\n\n  ```shell\n  oc start-build connect-cluster-101-connect --from-dir ./connectors/ --follow\n  ```\n\n1. Since the last commands triggers a new build, we should now see three new pods for the build task, the deploy task and the resulting Kafka Connect cluster. Also, we should see the previous Kafka Connect cluster pod if gone.\n\n  ```shell\n  oc get pods\n\n  NAME                                                 READY   STATUS      RESTARTS   AGE\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          31m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-build      0/1     Completed   0          18m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-deploy     0/1     Completed   0          17m\n  YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx      1/1     Running     0          17m\n  ```\n\n1. Once the new Kafka Connect cluster pod is up and running we can actually exec into it\n\n  ```shell\n  oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash\n  ```\n\n1. to see if there is any connector already set up, which should not be the case\n\n  ```shell\n  curl localhost:8083/connectors\n  []\n  ```\n\n## Apply IBM COS Sink Connector\n\nIn this section, we are going to see how to set up, apply or register the IBM COS Sink Connector we embedded into our Kafka Connect cluster so that it starts sending messages from our `INBOUND` topic into our IBM Cloud Object Storage bucket.\nproceed.\n\n1. Create a new file named `kafka-cos-sink-connector.yaml` and past the following code in it.\n\n  ```yaml\n  apiVersion: eventstreams.ibm.com/v1alpha1\n  kind: KafkaConnector\n  metadata:\n    name: cos-sink-connector\n    labels:\n      eventstreams.ibm.com/cluster: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n  spec:\n    class: com.ibm.eventstreams.connect.cossink.COSSinkConnector\n    tasksMax: 1\n    config:\n      key.converter: org.apache.kafka.connect.storage.StringConverter\n      value.converter: org.apache.kafka.connect.storage.StringConverter\n      topics: TOPIC_NAME\n      cos.api.key: IBM_COS_API_KEY\n      cos.bucket.location: IBM_COS_BUCKET_LOCATION\n      cos.bucket.name: IBM_COS_BUCKET_NAME\n      cos.bucket.resiliency: IBM_COS_RESILIENCY\n      cos.service.crn: \"IBM_COS_CRM\"\n      cos.object.records: 5\n      cos.object.deadline.seconds: 5\n      cos.object.interval.seconds: 5\n  ```\n\n  where\n   * `YOUR_KAFKA_CONNECT_CLUSTER_NAME`: is the name you gave previously to your Kakfa Connect cluster.\n   * `TOPIC_NAME`: is the name of the topic you created in IBM Event Streams at the beginning of this lab.\n   * `IBM_COS_API_KEY`: is your IBM Cloud Object Storage service credentials `apikey` value. Review first sections of this lab if you don't remember where and how to find this value.\n   * `IBM_COS_BUCKET_LOCATION`: is your IBM Cloud Object Storage bucket location. Review first sections of this lab if you don't remember where and how to find this value (it usually is in the form of something like `us-east` or `eu-gb` for example).\n   * `IBM_COS_RESILIENCY`: is your IBM Cloud Object Storage resiliency option. Review first sections of this lab if you don't remember where and how to find this value (it should be `regional`).\n   * `IBM_COS_CRM`: is your IBM Cloud Object Storage CRN. Review first sections of this lab if you don't remember where and how to find this value. It usually ends with a double `::` at the end of it. **IMPORTANT:** you might need to retain the double quotation marks here as the CRN has colons in it and may collide with yaml syntax.\n\n1. Apply the yaml which will create a `KafkaConnnector` custom resource behind the scenes and register/set up the IBM COS Sink Connector in your Kafka Connect cluster.\n\n  ```shell\n  oc apply -f kafka-cos-sink-connector.yaml\n  ```\n\n1. The initialization of the connector can take a minute or two. You can check the status of the connector to see if everything connected succesfully.\n\n  ```shell\n  oc describe kafkaconnector cos-sink-connector\n  ```\n\n1. When the IBM COS Sink connector is successfully up and running you should see something similar to the below.\n\n  ![IBM COS Sink Connector success](./images/ibm-cos-sink-connector-success.png)\n\n1. You should also see a new connector being registered if you exec into the Kafka Connect cluster pod and query for the existing connectors again:\n\n  ```shell\n  oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash\n  bash-4.4$ curl localhost:8083/connectors\n  [\"cos-sink-connector\"]\n  ```\n\n1. Finally, we can check whether the messages from our IBM Event Streams topic are getting propagated to our IBM Cloud Object Storage bucket. If you go to your IBM COS bucket, you should find some files in it. The name of the file inside the bucket has starting offset and ending offset. You can download one of these object files to make sure that the value inside matches the value inside your `INBOUND` topic.\n\n  ![End to End Success](./images/ibm-cos-bucket-success.png)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-cos/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}