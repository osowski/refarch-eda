{"componentChunkName":"component---src-pages-use-cases-kafka-streams-lab-3-index-mdx","path":"/use-cases/kafka-streams/lab-3/","result":{"pageContext":{"frontmatter":{"title":"Kafka Streams Test Lab 3","description":"Using Kafka Streams to compute real time inventory stock"},"relativePagePath":"/use-cases/kafka-streams/lab-3/index.mdx","titleType":"append","MdxNode":{"id":"77708b60-f77b-5b8e-8070-251c4cbab58a","children":[],"parent":"2c7111bc-107e-5563-a59a-ee1cee267695","internal":{"content":"---\ntitle: Kafka Streams Test Lab 3\ndescription: Using Kafka Streams to compute real time inventory stock\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Work in progress</strong> Updated 04/13/2020 - End to end testing could be better\n</InlineNotification>\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Pre-requisites</AnchorLink>\n    <AnchorLink>Deploy to OpenShift</AnchorLink>\n    <AnchorLink>Interactive queries</AnchorLink>\n    <AnchorLink>Integration Tests</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n\nIn this lab, we're going to use [Quarkus](https://quarkus.io) to develop the real time inventory logic using Kafka Streams APIs and microprofile reactive messaging.\n\nThe requirements to address are:\n\n- consume item sold events from the `items` topic. Item has SKU as unique key. Item event has store ID reference\n- compute for each item its current stock cross stores\n- compute the store's stock for each item\n- generate inventory event for store - item - stock\n- expose APIs to get stock for a store or for an item\n\nHere is a simple diagram to illustrate the components used:\n\n ![1](./images/item-aggregator-ctx.png)\n\nThe goal of this lab, is to develop the green component which exposes an API to support Kafka Streams interactive query on top of the aggregate to compute inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic).\n\nWe will be unit testing the stream logic using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TopologyTestDriver class. \n\nThis application is deployed to OpenShift cluster with Event Streams or Strimzi running in the same cluster. We use the Quarkus Kubernetes plugin to build the yaml manifests for deployment. \n\nThis application needs the [Item Store sell simulator](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) to perform the end to end testing and to demonstrate the end to end scenario.\n\n## Pre-requisites\n\n**OpenShift Container Platform**\n- v4.6.x\n\n**IBM Cloud Pak for Integration**\n- CP4I2021.4\n\n**Kafka**\n- The lab uses Event Streams v10.x on Cloud Pack for Integration or Kafka Strimzi.\n\n**Code Source**: clone the following git repository: `git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory`.\n\n\n## Use application as-is\n\nIf you do not want to develop the application, you can deploy the existing final app to your OpenShift clustr using our [docker image](https://hub.docker.com/r/ibmcase/item-aggregator) and the following steps:\n\n1. Get the Kafka streams credentials and Bootstrap URL\n\n As the application is deployed in the same cluster as the Kafka cluster we will use internal route.\n\n\n\n### Connect to Event Streams\n\nWe need to complete the configuration to connect to the remote Event Streams running on OpenShift.\n\n* Create the items and inventory topics, following the instructions as described [in this note](../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:\n\n ```shell\n cloudctl es topic-create --name items --partitions 3 --replication-factor 3\n cloudctl es topic-create --name inventory --partitions 1 --replication-factor 3\n cloudctl es topics\n ```\n\n* To connect from your computer to Event Streams running on OpenShift, we need to define a user with `scram-sha-512` password, as this is the mechanism for external to the cluster connection. [See product documentation](https://ibm.github.io/event-streams/getting-started/connecting/) on how to do it, or use our [quick summary here](/use-cases/overview/pre-requisites#get-shram-user).\n\n* Get Server TLS certificate into the `certs` folder. See our [quick summary here](/use-cases/overview/pre-requisites#get-tls-server-public-certificate)\n\n ```shell\n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f - \n\n ```\n\n* Modify the `application.properties` file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka streams can create topics to backup state stores, and one for kafka streams consumer and producer tasks:\n\n```properties\nkafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\n%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\n%dev.kafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=${USER_CERT_PATH}\n%prod.kafka.ssl.keystore.password=${USER_CERT_PWD}\n%prod.kafka.ssl.keystore.type=PKCS12\n```\n\nThe above settings take into account that when running locally (`%dev` profile) we use the `scram-sha` mechanism to authenticate, and when we deploy on openshift, the `%prod` profile is used with TLS mutual authentication  (client certificate in keystore).\n\nThe same approach applies for Kafka Stream:\n\n```\nquarkus.kafka-streams.bootstrap-servers=${KAFKA_BROKERS}\nquarkus.kafka-streams.security.protocol=${SECURE_PROTOCOL}\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\n%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\n%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\nquarkus.kafka-streams.ssl.truststore.location=${KAFKA_CERT_PATH}\nquarkus.kafka-streams.ssl.truststore.password=${KAFKA_CERT_PWD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n# Only if TLS is used for authentication instead of scram\n%prod.quarkus.kafka-streams.ssl.keystore.location=${USER_CERT_PATH}\n%prod.quarkus.kafka-streams.ssl.keystore.password=${USER_CERT_PWD}\n%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12\n```\n\n* Define a file, like `.env`, to set environment variables, and modify the settings from your Event Streams configuration.\n\n```\nKAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\nKAFKA_USER=\nKAFKA_PASSWORD=\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=\nSECURE_PROTOCOL=SASL_SSL\n```\n\n* Restart the quarkus in dev mode\n\n ```shell\n source .env\n ./mvnw quarkus:dev\n ```\n\n normally you should not get any exception and should get a trace like\n\n ```\n    AdminClientConfig values: \n    bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]\n    client.dns.lookup = default\n    client.id = \n    connections.max.idle.ms = 300000\n    default.api.timeout.ms = 60000\n    metadata.max.age.ms = 300000\n    metric.reporters = []\n    metrics.num.samples = 2\n    metrics.recording.level = INFO\n    metrics.sample.window.ms = 30000\n\n    ....\n    INFO  [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kafka-streams, kubernetes, kubernetes-client, mutiny, resteasy, resteasy-jsonb, resteasy-mutiny, smallrye-context-propagation, smallrye-health, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, swagger-ui, vertx]\n ```\n\n\n\n## Integration tests\n\n For running the integration test, we propose to copy the e2e folder from the solution repository and follow the [readme instructions section end-to-end-testing ](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing).\n\n## Deploy to OpenShift\n\nBe sure to have done [the steps described here](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to get user credentials and server side certificate. \n\nThe deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.  \nHere are the interesting properties to set environment variables from secrets \n\n```properties\n%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL\nquarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\nquarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12\nquarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password\nquarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12\nquarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.USER_CERT_PWD.value=user.password\n```\n\nAnd an extract of the expected generated openshift manifests from those configurations:\n\n```yaml\n    spec:\n      containers:\n      - env:\n        - name: KAFKA_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: ca.password\n              name: sandbox-rp-cluster-ca-cert\n        - name: USER_CERT_PATH\n          value: /deployments/certs/user/user.p12\n        - name: USER_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: KAFKA_BROKERS\n          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n        - name: KAFKA_CERT_PATH\n          value: /deployments/certs/server/ca.p12\n        - name: KAFKA_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: SECURE_PROTOCOL\n          value: SASL_SSL\n```\n\nFinally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:\n\n```\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred\n```\n\nwhich generates:\n\n```\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: es-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n```\n\nNow any deployment using the following command should work:\n\n```shell\n./mvnw clean package -DQuarkus.kubernetes.deploy=true\n```\n\nThe last piece is to go to EventStreams console and look at the inventory topic for messages generated. As an alternate we could use [Kafdrop](../../overview/pre-requisites#using-kafdrop).\n\n## Another item producer\n\nWe have done a simple app to produce item sale or restock events. The app is not exposed with API defined in Swagger or with JAXRS annotations, but expose one method to be exposed as a service. See the [ ItemSimulatorFunction code](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/05fcdcb7d09d674d9eb2cda2e28601171ba51166/item-kafka-producer/src/main/java/ibm/gse/eda/api/ItemSimulatorFunction.java#L18-L25). \n\nThe simulator is using reactive messaging, but as we mix imperative with reactive programming, the code is using Emitter, and then Munity Multi to create and send the Kafka records:\n\n```java\n@Inject\n@Channel(\"items\")\nEmitter<Item> emitter;\n\n\npublic void sendItems(Integer numberOfRecords) {\n    Multi.createFrom().items(buildItems(numberOfRecords).stream()).subscribe().with(item -> {\n            logger.warning(\"send \" + item.toString());\n            Message<Item> record = KafkaRecord.of(item.storeName,item);\n            emitter.send(record );\n        }, failure -> System.out.println(\"Failed with \" + failure.getMessage()));\n   \n```\n\n### Running the application in dev mode\n\nYou can run your application in dev mode that enables live coding using:\n\n```\n./mvnw quarkus:dev\n```\n\nBut as we connect to a remote Kafka Cluster you need to define environment variables as:\n\n```shell\nKAFKA_BROKERS=....containers.appdomain.cloud:443\nKAFKA_USER=<a>-scram-user\nKAFKA_PASSWORD=<a-password>\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=<server-ca-certificate-password>\nSECURE_PROTOCOL=SASL_SSL\n```\n\n### Packaging and running the application\n\nThe application can be packaged using `./mvnw package`.\nIt produces the `item-kafka-producer-1.0.0-SNAPSHOT-runner.jar` file in the `/target` directory.\nBe aware that it’s not an _über-jar_ as the dependencies are copied into the `target/lib` directory.\n\nThe application is now runnable using `java -jar target/item-kafka-producer-1.0.0-SNAPSHOT-runner.jar`.\n\n### Deployment to OpenShift\n\nThe code includes declaration to build the necessary environment variables, secrets, volumes to get connected to a Kafka cluster using TLS authentication, and deploy in one command:\n\n```shell\nmvn package -DskipTests -Dquarkus.kubernetes.deploy=true\n```\n\nSee the application.properties for `quarkus.openshift.env-vars.*` settings.\n\nIt is also supposed to be deployed as knative app, but there is an issue on the generation of volume declarations in the knative.yaml file, so we could not make it in one command.\n\nThe code is also available as docker image: [ibmcase/item-kafka-producer](https://hub.docker.com/r/ibmcase/item-kafka-producer).","type":"Mdx","contentDigest":"e31784e42fedae02426ba3da8a0c8dc3","owner":"gatsby-plugin-mdx","counter":782},"frontmatter":{"title":"Kafka Streams Test Lab 3","description":"Using Kafka Streams to compute real time inventory stock"},"exports":{},"rawBody":"---\ntitle: Kafka Streams Test Lab 3\ndescription: Using Kafka Streams to compute real time inventory stock\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Work in progress</strong> Updated 04/13/2020 - End to end testing could be better\n</InlineNotification>\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Pre-requisites</AnchorLink>\n    <AnchorLink>Deploy to OpenShift</AnchorLink>\n    <AnchorLink>Interactive queries</AnchorLink>\n    <AnchorLink>Integration Tests</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n\nIn this lab, we're going to use [Quarkus](https://quarkus.io) to develop the real time inventory logic using Kafka Streams APIs and microprofile reactive messaging.\n\nThe requirements to address are:\n\n- consume item sold events from the `items` topic. Item has SKU as unique key. Item event has store ID reference\n- compute for each item its current stock cross stores\n- compute the store's stock for each item\n- generate inventory event for store - item - stock\n- expose APIs to get stock for a store or for an item\n\nHere is a simple diagram to illustrate the components used:\n\n ![1](./images/item-aggregator-ctx.png)\n\nThe goal of this lab, is to develop the green component which exposes an API to support Kafka Streams interactive query on top of the aggregate to compute inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic).\n\nWe will be unit testing the stream logic using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TopologyTestDriver class. \n\nThis application is deployed to OpenShift cluster with Event Streams or Strimzi running in the same cluster. We use the Quarkus Kubernetes plugin to build the yaml manifests for deployment. \n\nThis application needs the [Item Store sell simulator](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) to perform the end to end testing and to demonstrate the end to end scenario.\n\n## Pre-requisites\n\n**OpenShift Container Platform**\n- v4.6.x\n\n**IBM Cloud Pak for Integration**\n- CP4I2021.4\n\n**Kafka**\n- The lab uses Event Streams v10.x on Cloud Pack for Integration or Kafka Strimzi.\n\n**Code Source**: clone the following git repository: `git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory`.\n\n\n## Use application as-is\n\nIf you do not want to develop the application, you can deploy the existing final app to your OpenShift clustr using our [docker image](https://hub.docker.com/r/ibmcase/item-aggregator) and the following steps:\n\n1. Get the Kafka streams credentials and Bootstrap URL\n\n As the application is deployed in the same cluster as the Kafka cluster we will use internal route.\n\n\n\n### Connect to Event Streams\n\nWe need to complete the configuration to connect to the remote Event Streams running on OpenShift.\n\n* Create the items and inventory topics, following the instructions as described [in this note](../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:\n\n ```shell\n cloudctl es topic-create --name items --partitions 3 --replication-factor 3\n cloudctl es topic-create --name inventory --partitions 1 --replication-factor 3\n cloudctl es topics\n ```\n\n* To connect from your computer to Event Streams running on OpenShift, we need to define a user with `scram-sha-512` password, as this is the mechanism for external to the cluster connection. [See product documentation](https://ibm.github.io/event-streams/getting-started/connecting/) on how to do it, or use our [quick summary here](/use-cases/overview/pre-requisites#get-shram-user).\n\n* Get Server TLS certificate into the `certs` folder. See our [quick summary here](/use-cases/overview/pre-requisites#get-tls-server-public-certificate)\n\n ```shell\n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f - \n\n ```\n\n* Modify the `application.properties` file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka streams can create topics to backup state stores, and one for kafka streams consumer and producer tasks:\n\n```properties\nkafka.bootstrap.servers=${KAFKA_BROKERS}\nkafka.security.protocol=${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\n%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\n%dev.kafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=${USER_CERT_PATH}\n%prod.kafka.ssl.keystore.password=${USER_CERT_PWD}\n%prod.kafka.ssl.keystore.type=PKCS12\n```\n\nThe above settings take into account that when running locally (`%dev` profile) we use the `scram-sha` mechanism to authenticate, and when we deploy on openshift, the `%prod` profile is used with TLS mutual authentication  (client certificate in keystore).\n\nThe same approach applies for Kafka Stream:\n\n```\nquarkus.kafka-streams.bootstrap-servers=${KAFKA_BROKERS}\nquarkus.kafka-streams.security.protocol=${SECURE_PROTOCOL}\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\n%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\n%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PASSWORD}\\\";\nquarkus.kafka-streams.ssl.truststore.location=${KAFKA_CERT_PATH}\nquarkus.kafka-streams.ssl.truststore.password=${KAFKA_CERT_PWD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n# Only if TLS is used for authentication instead of scram\n%prod.quarkus.kafka-streams.ssl.keystore.location=${USER_CERT_PATH}\n%prod.quarkus.kafka-streams.ssl.keystore.password=${USER_CERT_PWD}\n%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12\n```\n\n* Define a file, like `.env`, to set environment variables, and modify the settings from your Event Streams configuration.\n\n```\nKAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\nKAFKA_USER=\nKAFKA_PASSWORD=\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=\nSECURE_PROTOCOL=SASL_SSL\n```\n\n* Restart the quarkus in dev mode\n\n ```shell\n source .env\n ./mvnw quarkus:dev\n ```\n\n normally you should not get any exception and should get a trace like\n\n ```\n    AdminClientConfig values: \n    bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]\n    client.dns.lookup = default\n    client.id = \n    connections.max.idle.ms = 300000\n    default.api.timeout.ms = 60000\n    metadata.max.age.ms = 300000\n    metric.reporters = []\n    metrics.num.samples = 2\n    metrics.recording.level = INFO\n    metrics.sample.window.ms = 30000\n\n    ....\n    INFO  [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kafka-streams, kubernetes, kubernetes-client, mutiny, resteasy, resteasy-jsonb, resteasy-mutiny, smallrye-context-propagation, smallrye-health, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, swagger-ui, vertx]\n ```\n\n\n\n## Integration tests\n\n For running the integration test, we propose to copy the e2e folder from the solution repository and follow the [readme instructions section end-to-end-testing ](https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing).\n\n## Deploy to OpenShift\n\nBe sure to have done [the steps described here](../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift) to get user credentials and server side certificate. \n\nThe deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.  \nHere are the interesting properties to set environment variables from secrets \n\n```properties\n%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL\nquarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\nquarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12\nquarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password\nquarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12\nquarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.USER_CERT_PWD.value=user.password\n```\n\nAnd an extract of the expected generated openshift manifests from those configurations:\n\n```yaml\n    spec:\n      containers:\n      - env:\n        - name: KAFKA_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: ca.password\n              name: sandbox-rp-cluster-ca-cert\n        - name: USER_CERT_PATH\n          value: /deployments/certs/user/user.p12\n        - name: USER_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: KAFKA_BROKERS\n          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n        - name: KAFKA_CERT_PATH\n          value: /deployments/certs/server/ca.p12\n        - name: KAFKA_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: SECURE_PROTOCOL\n          value: SASL_SSL\n```\n\nFinally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:\n\n```\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred\n```\n\nwhich generates:\n\n```\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: es-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n```\n\nNow any deployment using the following command should work:\n\n```shell\n./mvnw clean package -DQuarkus.kubernetes.deploy=true\n```\n\nThe last piece is to go to EventStreams console and look at the inventory topic for messages generated. As an alternate we could use [Kafdrop](../../overview/pre-requisites#using-kafdrop).\n\n## Another item producer\n\nWe have done a simple app to produce item sale or restock events. The app is not exposed with API defined in Swagger or with JAXRS annotations, but expose one method to be exposed as a service. See the [ ItemSimulatorFunction code](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/05fcdcb7d09d674d9eb2cda2e28601171ba51166/item-kafka-producer/src/main/java/ibm/gse/eda/api/ItemSimulatorFunction.java#L18-L25). \n\nThe simulator is using reactive messaging, but as we mix imperative with reactive programming, the code is using Emitter, and then Munity Multi to create and send the Kafka records:\n\n```java\n@Inject\n@Channel(\"items\")\nEmitter<Item> emitter;\n\n\npublic void sendItems(Integer numberOfRecords) {\n    Multi.createFrom().items(buildItems(numberOfRecords).stream()).subscribe().with(item -> {\n            logger.warning(\"send \" + item.toString());\n            Message<Item> record = KafkaRecord.of(item.storeName,item);\n            emitter.send(record );\n        }, failure -> System.out.println(\"Failed with \" + failure.getMessage()));\n   \n```\n\n### Running the application in dev mode\n\nYou can run your application in dev mode that enables live coding using:\n\n```\n./mvnw quarkus:dev\n```\n\nBut as we connect to a remote Kafka Cluster you need to define environment variables as:\n\n```shell\nKAFKA_BROKERS=....containers.appdomain.cloud:443\nKAFKA_USER=<a>-scram-user\nKAFKA_PASSWORD=<a-password>\nKAFKA_CERT_PATH=${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=<server-ca-certificate-password>\nSECURE_PROTOCOL=SASL_SSL\n```\n\n### Packaging and running the application\n\nThe application can be packaged using `./mvnw package`.\nIt produces the `item-kafka-producer-1.0.0-SNAPSHOT-runner.jar` file in the `/target` directory.\nBe aware that it’s not an _über-jar_ as the dependencies are copied into the `target/lib` directory.\n\nThe application is now runnable using `java -jar target/item-kafka-producer-1.0.0-SNAPSHOT-runner.jar`.\n\n### Deployment to OpenShift\n\nThe code includes declaration to build the necessary environment variables, secrets, volumes to get connected to a Kafka cluster using TLS authentication, and deploy in one command:\n\n```shell\nmvn package -DskipTests -Dquarkus.kubernetes.deploy=true\n```\n\nSee the application.properties for `quarkus.openshift.env-vars.*` settings.\n\nIt is also supposed to be deployed as knative app, but there is an issue on the generation of volume declarations in the knative.yaml file, so we could not make it in one command.\n\nThe code is also available as docker image: [ibmcase/item-kafka-producer](https://hub.docker.com/r/ibmcase/item-kafka-producer).","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-streams/lab-3/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}