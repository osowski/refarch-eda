{"componentChunkName":"component---src-pages-use-cases-kafka-streams-lab-2-index-mdx","path":"/use-cases/kafka-streams/lab-2/","result":{"pageContext":{"frontmatter":{"title":"Kafka Streams Test Lab 2","description":"Using Kafka Streams Test for more Operators and Optionally Send to Event Streams"},"relativePagePath":"/use-cases/kafka-streams/lab-2/index.mdx","titleType":"append","MdxNode":{"id":"b62e2b37-71ed-58f5-b801-07a836353ae8","children":[],"parent":"3b5b5e04-2efc-556f-94d0-79436e1b921b","internal":{"content":"---\ntitle: Kafka Streams Test Lab 2\ndescription: Using Kafka Streams Test for more Operators and Optionally Send to Event Streams\n---\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Adding in more Kafka Streams operators</AnchorLink>\n    <AnchorLink>Producing to and Consuming from a Kafka Topic on Event Streams</AnchorLink>\n</AnchorLinks>\n\n## Overview\n- This is a continuation of the previous [Lab 1](/use-cases/kafka-streams/lab-1/). You should complete Lab 1 first before you get started here.\n- There's a few more pre-reqs (if you so choose to use them) outlined below.\n\n## Scenario Prerequisites\n**Java**\n- For the purposes of this lab we suggest Java 8+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n**OpenShift Container Platform, IBM Cloud Pak for Integration and IBM Event Streams**\n- This is an optional portion of the lab for those who have access to an OCP Cluster where IBM Cloud Pak for Integration has been installed on top and an IBM Event Streams instance deployed.\n\n- **The following are optional**\n- **OpenShift Container Platform**\n    - v4.4.x\n- **IBM Cloud Pak for Integration**\n    - CP4I2020.2\n- **IBM Event Streams**\n    - IBM Event Streams v10 or latter preferrably. If you are using a previous version of IBM Event Streams, there are some differences as to how you would configure `application.properties` to establish the connection to IBM Event Streams.\n\n## Adding in more Kafka Streams operators\n\nIn this section we are going to to add more functionality to our previous test class in order to see, work with and understand more Kafka Streams operators.\n\n- Add the following definitions to the `TestFinancialMessage.java` Java class:\n\n```java\nprivate static String tradingTable = \"tradingTable\";\nprivate static String tradingStoreName = \"tradingStore\";\nprivate static TestInputTopic<String, String> tradingTableTopic;\n```\n\n- Add the following Store and KTable definitions inside the `buildTopology()` function to support the trading fuctionality we are adding to our application:\n\n```java\nKeyValueBytesStoreSupplier tradingStoreSupplier = Stores.persistentKeyValueStore(tradingStoreName);\n\nKTable<String, String> stockTradingStore = builder.table(tradingTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(tradingStoreSupplier));\n```\n\n- Add the following import to your Java class so that you can use objects of type KTable:\n\n```java\nimport org.apache.kafka.streams.kstream.KTable;\n```\n\n- Edit the `branch[1]` logic again to create new `KeyValue` pairs of `userId` and `stockSymbol`\n\n```java\nbranches[1].filter(\n            (key, value) -> (value.totalCost > 5000)\n        )\n        .map(\n            (key, value) -> KeyValue.pair(value.userId, value.stockSymbol)\n        )\n        .to(\n            tradingTable,\n            Produced.with(Serdes.String(), Serdes.String())\n        );\n```\n\nNotice that, previously, we wrote straight to `outTopic`. However, we are now writing to\na KTable which we can query in our tests by using the State Store it is materialised as.\n\n- Before we create a test for the new functionality, remove or comment out the previous existing tests cases as these do no longer apply.\n\n- Create a new test with the code below:\n\n```java\n    @Test\n    public void filterAndMapNewPair() {\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n\n        KeyValueStore<String,ValueAndTimestamp<String>> tableStore = testDriver.getTimestampedKeyValueStore(tradingStoreName);\n        Assertions.assertEquals(1, tableStore.approximateNumEntries());\n        Assertions.assertEquals(\"MET\", tableStore.get(\"1\").value());\n    }\n```\n\nThe first assertion checks whether the store has a record. The second assertion checks that the mock record that we\ninserted has the correct value as our map function created new KeyValue pairs of type `<userId, stockSymbol>`.\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the tests pass with the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 20:47:06,478 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.086s. Listening on: http://localhost:8081\n2021-01-16 20:47:06,479 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 20:47:06,479 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.611 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\n2021-01-16 20:47:08,248 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.276 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 20:47:08,290 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 20:47:08,292 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 20:47:08,325 INFO  [io.quarkus] (main) Quarkus stopped in 0.026s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n```\n\nNow we are going to do something a little bit more advanced. We are going to join a KStream with a KTable. The Streams API\nhas an inner join, left join, and an outer join. [KStream-KTable joins](https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#kstream-ktable-join) are [non-windowed](https://docs.confluent.io/platform/current/streams/concepts.html#windowing) and asymmetric.\nBy asymmetric we mean that a join only gets triggered if the left input KStream gets a new record while the right KTable holds the latest input records materialized.\n\n- Add the following new attributes:\n\n```java\n    private static String joinedTopicName = \"joinedTopic\";\n    private static TestOutputTopic<String, String> joinedTopic;\n    private static String joinedStoreName = \"joinedStore\";\n```\n\n- Replace the `buildTopology()` function for the following new one:\n\n```java\npublic static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n        KeyValueBytesStoreSupplier tradingStoreSupplier = Stores.persistentKeyValueStore(tradingStoreName);\n        KeyValueBytesStoreSupplier joinedStoreSupplier = Stores.persistentKeyValueStore(joinedStoreName);\n\n        KStream<String, FinancialMessage> transactionStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        KTable<String, String> stockTradingStore = builder.table(tradingTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(tradingStoreSupplier));\n\n        KTable<String, String> joinedMessageStore = builder.table(joinedTopicName,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(joinedStoreSupplier));\n\n        KStream<String, String> joinedStream = transactionStream.join(\n            stockTradingStore,\n            (financialMessage, companyName) -> \"userId = \" + financialMessage.userId + \" companyName = \" + companyName);\n\n        joinedStream.to(\n            joinedTopicName,\n            Produced.with(Serdes.String(), Serdes.String()));\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer<FinancialMessage>());\n        tradingTableTopic = testDriver.createInputTopic(tradingTable, new StringSerializer(), new StringSerializer());\n        joinedTopic = testDriver.createOutputTopic(joinedTopicName, new StringDeserializer(), new StringDeserializer());\n    }\n```\n\nWe can see that our `buildTopology()` function still contains the `transactionsStream` KStream that will contain the stream of `FinancialMessages` being received through the `inTopicName`. Then, We can see a KTable called `stockTradingStore`, which will get materialized as a State Store, that will contain the messages comming in through the input topic called `tradingTable`. This KTable will hold data about the tradding companies that will serve to enhance the incoming `FinancialMessages` with. The join between the KStream and the KTable is being done below and is called `joinedStream`. The result of this join is being outputed into a topic called `joinedTopicName`. Finally, this output topic is being store in a KTable called `joinedMessageStore`, and materialized in its respective State Store, in order to be able to query it later on in our tests. The inner join is performed on matching keys between the KStream and the KTable and the matched records\nproduce a new `<String, String>` pair with the value of `userId` and `companyName`.\n\nIn order to test the above new functionality of the `buildTopology()` function, remove or comment out the existing test and create the following new one:\n\n```java\n    @Test\n    public void checkStreamAndTableJoinHasOneRecord() {\n\n        tradingTableTopic.pipeInput(\"1\", \"Metropolitan Museum of Art\");\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n\n        KeyValueStore<String,ValueAndTimestamp<String>> joinedTableStore = testDriver.getTimestampedKeyValueStore(joinedStoreName);\n        Assertions.assertEquals(1, joinedTableStore.approximateNumEntries());\n        System.out.println(joinedTableStore.get(\"1\").value());\n    }\n```\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 21:53:03,682 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.134s. Listening on: http://localhost:8081\n2021-01-16 21:53:03,684 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 21:53:03,685 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.574 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-16 21:53:05,432 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.294 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 21:53:05,482 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 21:53:05,484 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.044 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 21:53:05,518 INFO  [io.quarkus] (main) Quarkus stopped in 0.026s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n```\n\nWe can see that our test passes successfully as the `FinancialMessage` with `userId=1` and the KTable record with key `1` successfully joined to produce the output message: `userId = 1 companyName = Metropolitan Museum of Art`\n\nWe are now going to create yet another new test class. But first, we are going to create two new POJO classes to work with. These are `EnrichedMessage.java` and `AggregatedMessage.java`. Both of them should be placed where the previos POJO class is: `src/main/java/com/ibm/garage/cpat/domain`.\n\n- Create the `EnrichedMessage.java` Java file with the following code:\n\n```java\npackage com.ibm.garage.cpat.domain;\n\n\npublic class EnrichedMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public double adjustedCost;\n    public boolean technicalValidation;\n    public String companyName;\n\n    public EnrichedMessage (FinancialMessage message, String companyName) {\n        this.userId = message.userId;\n        this.stockSymbol = message.stockSymbol;\n        this.quantity = message.quantity;\n        this.stockPrice = message.stockPrice;\n        this.totalCost = message.totalCost;\n        this.companyName = companyName;\n\n        if (message.technicalValidation)\n        {\n            this.technicalValidation = message.technicalValidation;\n            this.adjustedCost = message.totalCost * 1.15;\n        }\n\n        else {\n            this.technicalValidation = message.technicalValidation;\n            this.adjustedCost = message.totalCost;\n        }\n    }\n}\n```\n\n- Create the `AggregatedMessage.java` Java file with the following code:\n\n```java\npackage com.ibm.garage.cpat.domain;\n\nimport java.math.BigDecimal;\nimport java.math.RoundingMode;\n\n\npublic class AggregatedMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public double adjustedCost;\n    public boolean technicalValidation;\n    public String companyName;\n    public int count;\n    public double sum;\n    public double average;\n\n    public AggregatedMessage updateFrom(EnrichedMessage message) {\n        this.userId = message.userId;\n        this.stockSymbol = message.stockSymbol;\n        this.quantity = message.quantity;\n        this.stockPrice = message.stockPrice;\n        this.totalCost = message.totalCost;\n        this.companyName = message.companyName;\n        this.adjustedCost = message.adjustedCost;\n        this.technicalValidation = message.technicalValidation;\n\n        this.count ++;\n        this.sum += message.adjustedCost;\n        this.average = BigDecimal.valueOf(sum / count)\n                    .setScale(1, RoundingMode.HALF_UP).doubleValue();\n\n        return this;\n    }\n}\n```\n\n- Now create the new test class named `TestAggregate.java` in the same path we have the other test classes (`src/test/java/com/ibm/garage/cpat`) and paste the following code:\n\n```java\npackage com.ibm.garage.cpat.lab;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.LongDeserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KGroupedStream;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.KTable;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Windowed;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\nimport org.apache.kafka.streams.processor.StateStore;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueIterator;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.kafka.client.serialization.JsonbDeserializer;\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\nimport io.quarkus.kafka.client.serialization.JsonbSerializer;\nimport io.quarkus.test.junit.QuarkusTest;\n\nimport com.ibm.garage.cpat.domain.*;\n\n\n@QuarkusTest\npublic class TestAggregate {\n\n    private static TopologyTestDriver testDriver;\n    private static String inTopicName = \"financialMessages\";\n    private static String outTopicName = \"enrichedMessages\";\n    private static String storeName = \"financialStore\";\n    private static String aggregatedTopicName = \"aggregatedMessages\";\n\n    private static String companyTable = \"companyTable\";\n    private static String companyStoreName = \"companyStore\";\n\n    private static TestInputTopic<String, FinancialMessage> inTopic;\n    private static TestOutputTopic<String, EnrichedMessage> outTopic;\n    private static TestOutputTopic<String, AggregatedMessage> aggregatedTopic;\n    private static TestInputTopic<String, String> companyTableTopic;\n\n    private static final JsonbSerde<FinancialMessage> financialMessageSerde = new JsonbSerde<>(FinancialMessage.class);\n    private static final JsonbSerde<EnrichedMessage> enrichedMessageSerde = new JsonbSerde<>(EnrichedMessage.class);\n    private static final JsonbSerde<AggregatedMessage> aggregatedMessageSerde = new JsonbSerde<>(AggregatedMessage.class);\n\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab3\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:3456\");\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde);\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n        KeyValueBytesStoreSupplier companyStoreSupplier = Stores.persistentKeyValueStore(companyStoreName);\n\n        // create a KStream for financial messages.\n        KStream<String, FinancialMessage> financialStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        // create a KTable from a topic for companies.\n        KTable<String, String> companyStore = builder.table(companyTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(companyStoreSupplier));\n\n        // join KStream with KTable and use aggregate.\n        KStream<String, EnrichedMessage> enrichedStream = financialStream.join(\n                companyStore,\n                //(financialMessage, companyName) -> financialMessage.userId,\n                (financialMessage, companyName) -> {\n                    return new EnrichedMessage(financialMessage, companyName);\n                }\n            );\n\n        enrichedStream.groupByKey()\n            .aggregate(\n                AggregatedMessage::new,\n                (userId, value, aggregatedMessage) -> aggregatedMessage.updateFrom(value),\n                Materialized.<String, AggregatedMessage> as(storeSupplier)\n                                .withKeySerde(Serdes.String())\n                                .withValueSerde(aggregatedMessageSerde)\n            )\n            .toStream()\n            .to(\n                aggregatedTopicName,\n                Produced.with(Serdes.String(), aggregatedMessageSerde)\n            );\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer<FinancialMessage>());\n        //outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new JsonbDeserializer<>(EnrichedMessage.class));\n        companyTableTopic = testDriver.createInputTopic(companyTable, new StringSerializer(), new StringSerializer());\n        aggregatedTopic = testDriver.createOutputTopic(aggregatedTopicName, new StringDeserializer(), new JsonbDeserializer<>(AggregatedMessage.class));\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n}\n\n```\n\nEven though the code above might seem a completely new one at first glance, it is just an extra step of the previous code we have been working with. We can see that we still have both our `financialStream` KStream receiving `FinancialMessage` from the `inTopicName` topic and our `companyStore` KTable holding the latest being received from the `companyTable` input topic. Then, we also still have the join of the previous two KStream and KTable in our `enrichedStream` KStream that will be a KStream of `EnrichedMessage`. What is new in this code is the following aggregation we are doing on the resulting `enrichedStream` KStream. We are grouping `EnrichedMessage` objects by key and doing an aggregation of that grouping. The aggregation result will be an `AggregatedMessage` which will get materialized as `storeSupplier` so that we can query it later on. We are also converting that KTable to a KStream that will get outputed to `aggregatedTopicName`. The aggregate logic will simply work out some average and count of `EnrichedMessage` you can check out in the `EnrichedMessage.java` Java file.\n\n\n- Now add a test that will make sure the functionality we have implemented in the `buildTopology()` function works as desired:\n\n```java\n    @Test\n    public void aggregatedMessageExists() {\n\n        companyTableTopic.pipeInput(\"1\", \"Metropolitan Museum of Art\");\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 6634.56, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n        inTopic.pipeInput(\"1\", mock2);\n\n        KeyValueStore<String,ValueAndTimestamp<AggregatedMessage>> aggregatedTableStore = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(2, aggregatedTableStore.approximateNumEntries());\n        System.out.println(\"Average = \" + aggregatedTableStore.get(\"1\").value().average);\n        Assertions.assertEquals(16389.3, aggregatedTableStore.get(\"2\").value().average);\n    }\n```\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-17 13:13:02,533 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.178s. Listening on: http://localhost:8081\n2021-01-17 13:13:02,535 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-17 13:13:02,535 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.441 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestAggregate\nAverage = 16389.3\n2021-01-17 13:13:04,216 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.339 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestAggregate\n[ERROR] aggregatedMessageExists  Time elapsed: 0.122 s  <<< ERROR!\njava.lang.NullPointerException\n\tat com.ibm.garage.cpat.lab.TestAggregate.aggregatedMessageExists(TestAggregate.java:145)\n\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-17 13:13:04,283 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.055 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-17 13:13:04,333 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-17 13:13:04,335 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.043 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-17 13:13:04,372 INFO  [io.quarkus] (main) Quarkus stopped in 0.029s\n[INFO] \n[INFO] Results:\n[INFO] \n[ERROR] Errors: \n[ERROR]   TestAggregate.aggregatedMessageExists:145 NullPointer\n[INFO] \n[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0\n```\n\nWe see the test fails, but that is expected. In this test we have two different `financialMessage` inserted with a key of `\"1\"` and there is only one entry\nin the KTable `(\"1\", \"Metropolitan Museum of Art\")`. Those two `financialMessage` will get enriched with the only record in the KTable. Later on, those two `EnrichedMessage` should get grouped by as they have the same key and result in an `AggregatedMessage`. There are two assertions in this test. The first one passes as the `aggregatedTableStore` contains two entries: The first `EnrichedMessage` that eas aggregated with a new empty `AggregatedMessage` as the initializer and then the second `EnrichedMessage` that it was aggregated with the resulting `AggregatedMessage` of the previous `EnrichedMessage`. However, the second second assertion fails. The reason for this is that even though there are two records in the `aggregatedTableStore`, this store is key based and will return, as a result, the latest `AggregatedMessage` it holds for a particular key. Then, if we want to retrieve the latest `AggregatedMessage` for out key `1` we need to change the assetion:\n\n```java\nAssertions.assertEquals(16389.3, aggregatedTableStore.get(\"2\").value().average);\n```\n\nto use the appropriate key:\n\n```java\nAssertions.assertEquals(16389.3, aggregatedTableStore.get(\"1\").value().average);\n```\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-17 13:14:40,370 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.025s. Listening on: http://localhost:8081\n2021-01-17 13:14:40,371 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-17 13:14:40,372 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.495 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestAggregate\nAverage = 16389.3\n2021-01-17 13:14:42,114 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.33 s - in com.ibm.garage.cpat.lab.TestAggregate\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-17 13:14:42,177 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.051 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-17 13:14:42,227 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-17 13:14:42,229 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-17 13:14:42,272 INFO  [io.quarkus] (main) Quarkus stopped in 0.035s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0\n```\n\n## Producing to and Consuming from a Kafka Topic on Event Streams\n\nWhen using the Kafka Streams API, we usually do it against a Kafka instance containing data already in, at least, one of its topics. In order to build up that scenario, we are going to use the MicroProfile Reactive Messaging library to send messages to a topic.\n\n- For using the MicroProfile Reactive Messaging library, we first need to add such dependency to our `pom.xml` file:\n\n```xml\n<dependency>\n    <groupId>io.quarkus</groupId>\n    <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>\n</dependency>\n```\n\n- Create the `MockProducer.java` Java file in `src/main/java/com/ibm/garage/cpat/infrastructure` with the following code:\n\n```java\npackage com.ibm.garage.cpat.infrastructure;\n\nimport javax.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.reactivex.Flowable;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\n\nimport java.util.concurrent.TimeUnit;\nimport java.util.Random;\n\nimport com.ibm.garage.cpat.domain.*;\n\n\n@ApplicationScoped\npublic class MockProducer {\n\n    private Random random = new Random();\n\n    FinancialMessage mock = new FinancialMessage(\n        \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n\n    @Outgoing(\"mock-messages\")\n    public Flowable<KafkaRecord<String,FinancialMessage>> produceMock() {\n        return Flowable.interval(5, TimeUnit.SECONDS)\n                       .map(tick -> {\n                            return setRandomUserId(mock);\n                        });\n    }\n\n    public KafkaRecord<String, FinancialMessage> setRandomUserId(FinancialMessage mock) {\n        mock.userId = String.valueOf(random.nextInt(100));\n\n        return KafkaRecord.of(mock.userId, mock);\n    }\n}\n```\n\nThe producer code above produces a `FinancialMessage` every 5 seconds to the `mock-messages` channel\nwith a random userId (out of 100). We will see later on how the `mock-messages` channel relates to a Kafka topic through configuration.\n\n- Next, create the topology that we are going to build for processing the messages sent by our previous producer. Create a `FinancialMessageTopology.java` Java file in `src/main/java/com/ibm/garage/cpat/domain` with the following code:\n\n```java\npackage com.ibm.garage.cpat.domain;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Produces;\n\nimport org.eclipse.microprofile.config.inject.ConfigProperty;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.Produced;\n\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\n\n\n@ApplicationScoped\npublic class FinancialMessageTopology {\n\n    @ConfigProperty(name = \"START_TOPIC_NAME\")\n    private String INCOMING_TOPIC;\n\n    @ConfigProperty(name = \"TARGET_TOPIC_NAME\")\n    private String OUTGOING_TOPIC;\n\n\n    @Produces\n    public Topology buildTopology() {\n\n        StreamsBuilder builder = new StreamsBuilder();\n\n        JsonbSerde<FinancialMessage> financialMessageSerde = new JsonbSerde<>(FinancialMessage.class);\n\n        // Stream reads from input topic, filters it by checking the boolean field on the message.\n        // If the boolean is true, it gets passed to the mapValues function which will then send that record\n        // to an outgoing topic.\n\n        builder.stream(\n            INCOMING_TOPIC,\n            Consumed.with(Serdes.String(), financialMessageSerde)\n        )\n        .filter (\n            (key, message) -> checkValidation(message)\n        )\n        .mapValues (\n            checkedMessage -> adjustPostValidation(checkedMessage)\n        )\n        .to (\n            OUTGOING_TOPIC,\n            Produced.with(Serdes.String(), financialMessageSerde)\n        );\n\n        return builder.build();\n    }\n\n    public boolean checkValidation(FinancialMessage message) {\n        return (message.technicalValidation);\n    }\n\n    public FinancialMessage adjustPostValidation(FinancialMessage message) {\n        message.totalCost = message.totalCost * 1.15;\n\n        return message;\n    }\n\n}\n```\n\nThe code above builds a KStream from the messages in `INCOMING_TOPIC`. It will filter the `FinanacialMessage` based on a boolean property of them. Then, it will do some adjustment of the cost attribute within that `FinancialMessage` post message validation and send that out to the `OUTGOING_TOPIC`.\n\nNow, we have seen the code for both the mock producer and the topology we want to build are dependant on certain configuration variables such as the input and output topics they are meant to produce to and consume from. In\na Quarkus application, all the configuration settings is done through a properties file, which makes the application portable across different environments. This property file, which we already had to configure in previous labs is called `application.properties` and is located in `src/main/resources`. \n\n- In order to configure the application to work with an IBM Event Streams v10 or later, paste the following configuration in the `application.properties` file:\n\n```properties\nquarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=${CERT_PASSWORD}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.type=PKCS12\n\n\n# Initial mock JSON message producer configuration\nmp.messaging.outgoing.mock-messages.connector=smallrye-kafka\nmp.messaging.outgoing.mock-messages.topic=${START_TOPIC_NAME}\nmp.messaging.outgoing.mock-messages.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n\n\n\n# Quarkus Kafka Streams configuration settings\nquarkus.kafka-streams.bootstrap-servers=${BOOTSTRAP_SERVERS}\nquarkus.kafka-streams.application-id=financial-stream\nquarkus.kafka-streams.application-server=localhost:8080\nquarkus.kafka-streams.topics=${START_TOPIC_NAME},${TARGET_TOPIC_NAME}\nquarkus.kafka-streams.health.enabled=true\n\nquarkus.kafka-streams.security.protocol=SASL_SSL\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\nquarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\nquarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nquarkus.kafka-streams.ssl.truststore.location=${CERT_LOCATION}\nquarkus.kafka-streams.ssl.truststore.password=${CERT_PASSWORD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n\n# pass-through options\nkafka-streams.cache.max.bytes.buffering=10240\nkafka-streams.commit.interval.ms=1000\nkafka-streams.metadata.max.age.ms=500\nkafka-streams.auto.offset.reset=latest\nkafka-streams.metrics.recording.level=DEBUG\n```\n\n- If using a previous IBM Event Streams version (such as v2019.4.2) or on IBM Cloud, use the following configuration in the `application.properties` file:\n\n```properties\nquarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=PLAIN\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.PlainLoginModule required \\\n                username=\"token\" \\\n                password=${API_KEY};\n# If connecting to Event Streams on IBM Cloud the following truststore options are not needed.\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=password\n\n\n# Initial mock JSON message producer configuration\nmp.messaging.outgoing.mock-messages.connector=smallrye-kafka\nmp.messaging.outgoing.mock-messages.topic=${START_TOPIC_NAME}\nmp.messaging.outgoing.mock-messages.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n\n\n\n# Quarkus Kafka Streams configuration settings\nquarkus.kafka-streams.bootstrap-servers=${BOOTSTRAP_SERVERS}\nquarkus.kafka-streams.application-id=financial-stream\nquarkus.kafka-streams.application-server=localhost:8080\nquarkus.kafka-streams.topics=${START_TOPIC_NAME},${TARGET_TOPIC_NAME}\nquarkus.kafka-streams.health.enabled=true\n\nquarkus.kafka-streams.security.protocol=SASL_SSL\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\nquarkus.kafka-streams.sasl.mechanism=PLAIN\nquarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.PlainLoginModule required \\\n                username=\"token\" \\\n                password=${API_KEY};\n# If connecting to Event Streams on IBM Cloud the following truststore options are not needed.\nquarkus.kafka-streams.ssl.truststore.location=${CERT_LOCATION}\nquarkus.kafka-streams.ssl.truststore.password=password\n\n# pass-through options\nkafka-streams.cache.max.bytes.buffering=10240\nkafka-streams.commit.interval.ms=1000\nkafka-streams.metadata.max.age.ms=500\nkafka-streams.auto.offset.reset=latest\nkafka-streams.metrics.recording.level=DEBUG\n```\n\nThere are some environment variables our `application.properties` file depends on:\n\n- `START_TOPIC_NAME`: The Kafka topic the mock producer will produce `FinancialMessage` to and the topology consume messages from for processing. **IMPORTANT:** Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you **must** create this topic in IBM Event Streams. See [here](/use-cases/overview/pre-requisites) for more details as to how to create a topic.\n- `TARGET_TOPIC_NAME`: The Kafka topic the topology will produce the processed `FinancialMessage` to. **IMPORTANT:** Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you **must** create this topic in IBM Event Streams. See [here]((/use-cases/overview/pre-requisites) for more details as to how to create a topic\n- `BOOTSTRAP_SERVERS`: Your IBM Event Streams bootstrap server. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n- `CERT_LOCATION`: The location where the PKCS12 certificate for the SSL connection to the IBM Event Streams instance is. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n- `CERT_PASSWORD`: The password of the PKCS12 certificate. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n- `API_KEY` if you are using an IBM Event Streams instance in IBM Cloud or\n- `SCRAM_USERNAME` and `SCRAM_PASSWORD`: The SCRAM credentials for your application to get authenticated and authorized to work with IBM Event Streams. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n\nExport the variables and values on the terminal you will run the application from:\n\n- IBM Event Strams v10 or later:\n\n```shell\nexport BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \\\nexport CERT_LOCATION=/path-to-pkcs12-cert/es-cert.p12 \\\nexport CERT_PASSWORD=certificate-password \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-password\n```\n\n- Previous IBM Event Streams versions:\n\n```shell\nexport BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \\\nexport CERT_LOCATION=/path-to-jks-cert/es-cert.jks \\\nexport API_KEY=your-api-key\n```\n\n- Local Kafka Cluster:\n\n```shell\nexport BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \n```\n\n- You can now test the Quarkus application\n\n```shell\n./mvnw quarkus:dev\n```\n\n- You should now be able to see `FinancialMessage` events in the input topic and those processed messages in the output topic you have specified above in the IBM Event Streams user interface.","type":"Mdx","contentDigest":"08ae97603113e3cf097ba6814cccc7e2","owner":"gatsby-plugin-mdx","counter":767},"frontmatter":{"title":"Kafka Streams Test Lab 2","description":"Using Kafka Streams Test for more Operators and Optionally Send to Event Streams"},"exports":{},"rawBody":"---\ntitle: Kafka Streams Test Lab 2\ndescription: Using Kafka Streams Test for more Operators and Optionally Send to Event Streams\n---\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Adding in more Kafka Streams operators</AnchorLink>\n    <AnchorLink>Producing to and Consuming from a Kafka Topic on Event Streams</AnchorLink>\n</AnchorLinks>\n\n## Overview\n- This is a continuation of the previous [Lab 1](/use-cases/kafka-streams/lab-1/). You should complete Lab 1 first before you get started here.\n- There's a few more pre-reqs (if you so choose to use them) outlined below.\n\n## Scenario Prerequisites\n**Java**\n- For the purposes of this lab we suggest Java 8+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n**OpenShift Container Platform, IBM Cloud Pak for Integration and IBM Event Streams**\n- This is an optional portion of the lab for those who have access to an OCP Cluster where IBM Cloud Pak for Integration has been installed on top and an IBM Event Streams instance deployed.\n\n- **The following are optional**\n- **OpenShift Container Platform**\n    - v4.4.x\n- **IBM Cloud Pak for Integration**\n    - CP4I2020.2\n- **IBM Event Streams**\n    - IBM Event Streams v10 or latter preferrably. If you are using a previous version of IBM Event Streams, there are some differences as to how you would configure `application.properties` to establish the connection to IBM Event Streams.\n\n## Adding in more Kafka Streams operators\n\nIn this section we are going to to add more functionality to our previous test class in order to see, work with and understand more Kafka Streams operators.\n\n- Add the following definitions to the `TestFinancialMessage.java` Java class:\n\n```java\nprivate static String tradingTable = \"tradingTable\";\nprivate static String tradingStoreName = \"tradingStore\";\nprivate static TestInputTopic<String, String> tradingTableTopic;\n```\n\n- Add the following Store and KTable definitions inside the `buildTopology()` function to support the trading fuctionality we are adding to our application:\n\n```java\nKeyValueBytesStoreSupplier tradingStoreSupplier = Stores.persistentKeyValueStore(tradingStoreName);\n\nKTable<String, String> stockTradingStore = builder.table(tradingTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(tradingStoreSupplier));\n```\n\n- Add the following import to your Java class so that you can use objects of type KTable:\n\n```java\nimport org.apache.kafka.streams.kstream.KTable;\n```\n\n- Edit the `branch[1]` logic again to create new `KeyValue` pairs of `userId` and `stockSymbol`\n\n```java\nbranches[1].filter(\n            (key, value) -> (value.totalCost > 5000)\n        )\n        .map(\n            (key, value) -> KeyValue.pair(value.userId, value.stockSymbol)\n        )\n        .to(\n            tradingTable,\n            Produced.with(Serdes.String(), Serdes.String())\n        );\n```\n\nNotice that, previously, we wrote straight to `outTopic`. However, we are now writing to\na KTable which we can query in our tests by using the State Store it is materialised as.\n\n- Before we create a test for the new functionality, remove or comment out the previous existing tests cases as these do no longer apply.\n\n- Create a new test with the code below:\n\n```java\n    @Test\n    public void filterAndMapNewPair() {\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n\n        KeyValueStore<String,ValueAndTimestamp<String>> tableStore = testDriver.getTimestampedKeyValueStore(tradingStoreName);\n        Assertions.assertEquals(1, tableStore.approximateNumEntries());\n        Assertions.assertEquals(\"MET\", tableStore.get(\"1\").value());\n    }\n```\n\nThe first assertion checks whether the store has a record. The second assertion checks that the mock record that we\ninserted has the correct value as our map function created new KeyValue pairs of type `<userId, stockSymbol>`.\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the tests pass with the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 20:47:06,478 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.086s. Listening on: http://localhost:8081\n2021-01-16 20:47:06,479 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 20:47:06,479 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.611 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\n2021-01-16 20:47:08,248 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.276 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 20:47:08,290 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 20:47:08,292 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 20:47:08,325 INFO  [io.quarkus] (main) Quarkus stopped in 0.026s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n```\n\nNow we are going to do something a little bit more advanced. We are going to join a KStream with a KTable. The Streams API\nhas an inner join, left join, and an outer join. [KStream-KTable joins](https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#kstream-ktable-join) are [non-windowed](https://docs.confluent.io/platform/current/streams/concepts.html#windowing) and asymmetric.\nBy asymmetric we mean that a join only gets triggered if the left input KStream gets a new record while the right KTable holds the latest input records materialized.\n\n- Add the following new attributes:\n\n```java\n    private static String joinedTopicName = \"joinedTopic\";\n    private static TestOutputTopic<String, String> joinedTopic;\n    private static String joinedStoreName = \"joinedStore\";\n```\n\n- Replace the `buildTopology()` function for the following new one:\n\n```java\npublic static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n        KeyValueBytesStoreSupplier tradingStoreSupplier = Stores.persistentKeyValueStore(tradingStoreName);\n        KeyValueBytesStoreSupplier joinedStoreSupplier = Stores.persistentKeyValueStore(joinedStoreName);\n\n        KStream<String, FinancialMessage> transactionStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        KTable<String, String> stockTradingStore = builder.table(tradingTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(tradingStoreSupplier));\n\n        KTable<String, String> joinedMessageStore = builder.table(joinedTopicName,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(joinedStoreSupplier));\n\n        KStream<String, String> joinedStream = transactionStream.join(\n            stockTradingStore,\n            (financialMessage, companyName) -> \"userId = \" + financialMessage.userId + \" companyName = \" + companyName);\n\n        joinedStream.to(\n            joinedTopicName,\n            Produced.with(Serdes.String(), Serdes.String()));\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer<FinancialMessage>());\n        tradingTableTopic = testDriver.createInputTopic(tradingTable, new StringSerializer(), new StringSerializer());\n        joinedTopic = testDriver.createOutputTopic(joinedTopicName, new StringDeserializer(), new StringDeserializer());\n    }\n```\n\nWe can see that our `buildTopology()` function still contains the `transactionsStream` KStream that will contain the stream of `FinancialMessages` being received through the `inTopicName`. Then, We can see a KTable called `stockTradingStore`, which will get materialized as a State Store, that will contain the messages comming in through the input topic called `tradingTable`. This KTable will hold data about the tradding companies that will serve to enhance the incoming `FinancialMessages` with. The join between the KStream and the KTable is being done below and is called `joinedStream`. The result of this join is being outputed into a topic called `joinedTopicName`. Finally, this output topic is being store in a KTable called `joinedMessageStore`, and materialized in its respective State Store, in order to be able to query it later on in our tests. The inner join is performed on matching keys between the KStream and the KTable and the matched records\nproduce a new `<String, String>` pair with the value of `userId` and `companyName`.\n\nIn order to test the above new functionality of the `buildTopology()` function, remove or comment out the existing test and create the following new one:\n\n```java\n    @Test\n    public void checkStreamAndTableJoinHasOneRecord() {\n\n        tradingTableTopic.pipeInput(\"1\", \"Metropolitan Museum of Art\");\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n\n        KeyValueStore<String,ValueAndTimestamp<String>> joinedTableStore = testDriver.getTimestampedKeyValueStore(joinedStoreName);\n        Assertions.assertEquals(1, joinedTableStore.approximateNumEntries());\n        System.out.println(joinedTableStore.get(\"1\").value());\n    }\n```\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 21:53:03,682 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.134s. Listening on: http://localhost:8081\n2021-01-16 21:53:03,684 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 21:53:03,685 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.574 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-16 21:53:05,432 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.294 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 21:53:05,482 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 21:53:05,484 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.044 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 21:53:05,518 INFO  [io.quarkus] (main) Quarkus stopped in 0.026s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n```\n\nWe can see that our test passes successfully as the `FinancialMessage` with `userId=1` and the KTable record with key `1` successfully joined to produce the output message: `userId = 1 companyName = Metropolitan Museum of Art`\n\nWe are now going to create yet another new test class. But first, we are going to create two new POJO classes to work with. These are `EnrichedMessage.java` and `AggregatedMessage.java`. Both of them should be placed where the previos POJO class is: `src/main/java/com/ibm/garage/cpat/domain`.\n\n- Create the `EnrichedMessage.java` Java file with the following code:\n\n```java\npackage com.ibm.garage.cpat.domain;\n\n\npublic class EnrichedMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public double adjustedCost;\n    public boolean technicalValidation;\n    public String companyName;\n\n    public EnrichedMessage (FinancialMessage message, String companyName) {\n        this.userId = message.userId;\n        this.stockSymbol = message.stockSymbol;\n        this.quantity = message.quantity;\n        this.stockPrice = message.stockPrice;\n        this.totalCost = message.totalCost;\n        this.companyName = companyName;\n\n        if (message.technicalValidation)\n        {\n            this.technicalValidation = message.technicalValidation;\n            this.adjustedCost = message.totalCost * 1.15;\n        }\n\n        else {\n            this.technicalValidation = message.technicalValidation;\n            this.adjustedCost = message.totalCost;\n        }\n    }\n}\n```\n\n- Create the `AggregatedMessage.java` Java file with the following code:\n\n```java\npackage com.ibm.garage.cpat.domain;\n\nimport java.math.BigDecimal;\nimport java.math.RoundingMode;\n\n\npublic class AggregatedMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public double adjustedCost;\n    public boolean technicalValidation;\n    public String companyName;\n    public int count;\n    public double sum;\n    public double average;\n\n    public AggregatedMessage updateFrom(EnrichedMessage message) {\n        this.userId = message.userId;\n        this.stockSymbol = message.stockSymbol;\n        this.quantity = message.quantity;\n        this.stockPrice = message.stockPrice;\n        this.totalCost = message.totalCost;\n        this.companyName = message.companyName;\n        this.adjustedCost = message.adjustedCost;\n        this.technicalValidation = message.technicalValidation;\n\n        this.count ++;\n        this.sum += message.adjustedCost;\n        this.average = BigDecimal.valueOf(sum / count)\n                    .setScale(1, RoundingMode.HALF_UP).doubleValue();\n\n        return this;\n    }\n}\n```\n\n- Now create the new test class named `TestAggregate.java` in the same path we have the other test classes (`src/test/java/com/ibm/garage/cpat`) and paste the following code:\n\n```java\npackage com.ibm.garage.cpat.lab;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.LongDeserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KGroupedStream;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.KTable;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Windowed;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\nimport org.apache.kafka.streams.processor.StateStore;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueIterator;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.kafka.client.serialization.JsonbDeserializer;\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\nimport io.quarkus.kafka.client.serialization.JsonbSerializer;\nimport io.quarkus.test.junit.QuarkusTest;\n\nimport com.ibm.garage.cpat.domain.*;\n\n\n@QuarkusTest\npublic class TestAggregate {\n\n    private static TopologyTestDriver testDriver;\n    private static String inTopicName = \"financialMessages\";\n    private static String outTopicName = \"enrichedMessages\";\n    private static String storeName = \"financialStore\";\n    private static String aggregatedTopicName = \"aggregatedMessages\";\n\n    private static String companyTable = \"companyTable\";\n    private static String companyStoreName = \"companyStore\";\n\n    private static TestInputTopic<String, FinancialMessage> inTopic;\n    private static TestOutputTopic<String, EnrichedMessage> outTopic;\n    private static TestOutputTopic<String, AggregatedMessage> aggregatedTopic;\n    private static TestInputTopic<String, String> companyTableTopic;\n\n    private static final JsonbSerde<FinancialMessage> financialMessageSerde = new JsonbSerde<>(FinancialMessage.class);\n    private static final JsonbSerde<EnrichedMessage> enrichedMessageSerde = new JsonbSerde<>(EnrichedMessage.class);\n    private static final JsonbSerde<AggregatedMessage> aggregatedMessageSerde = new JsonbSerde<>(AggregatedMessage.class);\n\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab3\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:3456\");\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde);\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n        KeyValueBytesStoreSupplier companyStoreSupplier = Stores.persistentKeyValueStore(companyStoreName);\n\n        // create a KStream for financial messages.\n        KStream<String, FinancialMessage> financialStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        // create a KTable from a topic for companies.\n        KTable<String, String> companyStore = builder.table(companyTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(companyStoreSupplier));\n\n        // join KStream with KTable and use aggregate.\n        KStream<String, EnrichedMessage> enrichedStream = financialStream.join(\n                companyStore,\n                //(financialMessage, companyName) -> financialMessage.userId,\n                (financialMessage, companyName) -> {\n                    return new EnrichedMessage(financialMessage, companyName);\n                }\n            );\n\n        enrichedStream.groupByKey()\n            .aggregate(\n                AggregatedMessage::new,\n                (userId, value, aggregatedMessage) -> aggregatedMessage.updateFrom(value),\n                Materialized.<String, AggregatedMessage> as(storeSupplier)\n                                .withKeySerde(Serdes.String())\n                                .withValueSerde(aggregatedMessageSerde)\n            )\n            .toStream()\n            .to(\n                aggregatedTopicName,\n                Produced.with(Serdes.String(), aggregatedMessageSerde)\n            );\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer<FinancialMessage>());\n        //outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new JsonbDeserializer<>(EnrichedMessage.class));\n        companyTableTopic = testDriver.createInputTopic(companyTable, new StringSerializer(), new StringSerializer());\n        aggregatedTopic = testDriver.createOutputTopic(aggregatedTopicName, new StringDeserializer(), new JsonbDeserializer<>(AggregatedMessage.class));\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n}\n\n```\n\nEven though the code above might seem a completely new one at first glance, it is just an extra step of the previous code we have been working with. We can see that we still have both our `financialStream` KStream receiving `FinancialMessage` from the `inTopicName` topic and our `companyStore` KTable holding the latest being received from the `companyTable` input topic. Then, we also still have the join of the previous two KStream and KTable in our `enrichedStream` KStream that will be a KStream of `EnrichedMessage`. What is new in this code is the following aggregation we are doing on the resulting `enrichedStream` KStream. We are grouping `EnrichedMessage` objects by key and doing an aggregation of that grouping. The aggregation result will be an `AggregatedMessage` which will get materialized as `storeSupplier` so that we can query it later on. We are also converting that KTable to a KStream that will get outputed to `aggregatedTopicName`. The aggregate logic will simply work out some average and count of `EnrichedMessage` you can check out in the `EnrichedMessage.java` Java file.\n\n\n- Now add a test that will make sure the functionality we have implemented in the `buildTopology()` function works as desired:\n\n```java\n    @Test\n    public void aggregatedMessageExists() {\n\n        companyTableTopic.pipeInput(\"1\", \"Metropolitan Museum of Art\");\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 6634.56, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n        inTopic.pipeInput(\"1\", mock2);\n\n        KeyValueStore<String,ValueAndTimestamp<AggregatedMessage>> aggregatedTableStore = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(2, aggregatedTableStore.approximateNumEntries());\n        System.out.println(\"Average = \" + aggregatedTableStore.get(\"1\").value().average);\n        Assertions.assertEquals(16389.3, aggregatedTableStore.get(\"2\").value().average);\n    }\n```\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-17 13:13:02,533 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.178s. Listening on: http://localhost:8081\n2021-01-17 13:13:02,535 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-17 13:13:02,535 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.441 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestAggregate\nAverage = 16389.3\n2021-01-17 13:13:04,216 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.339 s <<< FAILURE! - in com.ibm.garage.cpat.lab.TestAggregate\n[ERROR] aggregatedMessageExists  Time elapsed: 0.122 s  <<< ERROR!\njava.lang.NullPointerException\n\tat com.ibm.garage.cpat.lab.TestAggregate.aggregatedMessageExists(TestAggregate.java:145)\n\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-17 13:13:04,283 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.055 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-17 13:13:04,333 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-17 13:13:04,335 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.043 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-17 13:13:04,372 INFO  [io.quarkus] (main) Quarkus stopped in 0.029s\n[INFO] \n[INFO] Results:\n[INFO] \n[ERROR] Errors: \n[ERROR]   TestAggregate.aggregatedMessageExists:145 NullPointer\n[INFO] \n[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0\n```\n\nWe see the test fails, but that is expected. In this test we have two different `financialMessage` inserted with a key of `\"1\"` and there is only one entry\nin the KTable `(\"1\", \"Metropolitan Museum of Art\")`. Those two `financialMessage` will get enriched with the only record in the KTable. Later on, those two `EnrichedMessage` should get grouped by as they have the same key and result in an `AggregatedMessage`. There are two assertions in this test. The first one passes as the `aggregatedTableStore` contains two entries: The first `EnrichedMessage` that eas aggregated with a new empty `AggregatedMessage` as the initializer and then the second `EnrichedMessage` that it was aggregated with the resulting `AggregatedMessage` of the previous `EnrichedMessage`. However, the second second assertion fails. The reason for this is that even though there are two records in the `aggregatedTableStore`, this store is key based and will return, as a result, the latest `AggregatedMessage` it holds for a particular key. Then, if we want to retrieve the latest `AggregatedMessage` for out key `1` we need to change the assetion:\n\n```java\nAssertions.assertEquals(16389.3, aggregatedTableStore.get(\"2\").value().average);\n```\n\nto use the appropriate key:\n\n```java\nAssertions.assertEquals(16389.3, aggregatedTableStore.get(\"1\").value().average);\n```\n\n- Test the application by running the following:\n\n```shell\n./mvnw clean verify\n```\n\n- You should see the following output:\n\n```shell\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-17 13:14:40,370 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.025s. Listening on: http://localhost:8081\n2021-01-17 13:14:40,371 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-17 13:14:40,372 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.495 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestAggregate\nAverage = 16389.3\n2021-01-17 13:14:42,114 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.33 s - in com.ibm.garage.cpat.lab.TestAggregate\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-17 13:14:42,177 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.051 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-17 13:14:42,227 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-17 13:14:42,229 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-17 13:14:42,272 INFO  [io.quarkus] (main) Quarkus stopped in 0.035s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0\n```\n\n## Producing to and Consuming from a Kafka Topic on Event Streams\n\nWhen using the Kafka Streams API, we usually do it against a Kafka instance containing data already in, at least, one of its topics. In order to build up that scenario, we are going to use the MicroProfile Reactive Messaging library to send messages to a topic.\n\n- For using the MicroProfile Reactive Messaging library, we first need to add such dependency to our `pom.xml` file:\n\n```xml\n<dependency>\n    <groupId>io.quarkus</groupId>\n    <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>\n</dependency>\n```\n\n- Create the `MockProducer.java` Java file in `src/main/java/com/ibm/garage/cpat/infrastructure` with the following code:\n\n```java\npackage com.ibm.garage.cpat.infrastructure;\n\nimport javax.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.reactivex.Flowable;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\n\nimport java.util.concurrent.TimeUnit;\nimport java.util.Random;\n\nimport com.ibm.garage.cpat.domain.*;\n\n\n@ApplicationScoped\npublic class MockProducer {\n\n    private Random random = new Random();\n\n    FinancialMessage mock = new FinancialMessage(\n        \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n\n    @Outgoing(\"mock-messages\")\n    public Flowable<KafkaRecord<String,FinancialMessage>> produceMock() {\n        return Flowable.interval(5, TimeUnit.SECONDS)\n                       .map(tick -> {\n                            return setRandomUserId(mock);\n                        });\n    }\n\n    public KafkaRecord<String, FinancialMessage> setRandomUserId(FinancialMessage mock) {\n        mock.userId = String.valueOf(random.nextInt(100));\n\n        return KafkaRecord.of(mock.userId, mock);\n    }\n}\n```\n\nThe producer code above produces a `FinancialMessage` every 5 seconds to the `mock-messages` channel\nwith a random userId (out of 100). We will see later on how the `mock-messages` channel relates to a Kafka topic through configuration.\n\n- Next, create the topology that we are going to build for processing the messages sent by our previous producer. Create a `FinancialMessageTopology.java` Java file in `src/main/java/com/ibm/garage/cpat/domain` with the following code:\n\n```java\npackage com.ibm.garage.cpat.domain;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Produces;\n\nimport org.eclipse.microprofile.config.inject.ConfigProperty;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.Produced;\n\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\n\n\n@ApplicationScoped\npublic class FinancialMessageTopology {\n\n    @ConfigProperty(name = \"START_TOPIC_NAME\")\n    private String INCOMING_TOPIC;\n\n    @ConfigProperty(name = \"TARGET_TOPIC_NAME\")\n    private String OUTGOING_TOPIC;\n\n\n    @Produces\n    public Topology buildTopology() {\n\n        StreamsBuilder builder = new StreamsBuilder();\n\n        JsonbSerde<FinancialMessage> financialMessageSerde = new JsonbSerde<>(FinancialMessage.class);\n\n        // Stream reads from input topic, filters it by checking the boolean field on the message.\n        // If the boolean is true, it gets passed to the mapValues function which will then send that record\n        // to an outgoing topic.\n\n        builder.stream(\n            INCOMING_TOPIC,\n            Consumed.with(Serdes.String(), financialMessageSerde)\n        )\n        .filter (\n            (key, message) -> checkValidation(message)\n        )\n        .mapValues (\n            checkedMessage -> adjustPostValidation(checkedMessage)\n        )\n        .to (\n            OUTGOING_TOPIC,\n            Produced.with(Serdes.String(), financialMessageSerde)\n        );\n\n        return builder.build();\n    }\n\n    public boolean checkValidation(FinancialMessage message) {\n        return (message.technicalValidation);\n    }\n\n    public FinancialMessage adjustPostValidation(FinancialMessage message) {\n        message.totalCost = message.totalCost * 1.15;\n\n        return message;\n    }\n\n}\n```\n\nThe code above builds a KStream from the messages in `INCOMING_TOPIC`. It will filter the `FinanacialMessage` based on a boolean property of them. Then, it will do some adjustment of the cost attribute within that `FinancialMessage` post message validation and send that out to the `OUTGOING_TOPIC`.\n\nNow, we have seen the code for both the mock producer and the topology we want to build are dependant on certain configuration variables such as the input and output topics they are meant to produce to and consume from. In\na Quarkus application, all the configuration settings is done through a properties file, which makes the application portable across different environments. This property file, which we already had to configure in previous labs is called `application.properties` and is located in `src/main/resources`. \n\n- In order to configure the application to work with an IBM Event Streams v10 or later, paste the following configuration in the `application.properties` file:\n\n```properties\nquarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=${CERT_PASSWORD}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.type=PKCS12\n\n\n# Initial mock JSON message producer configuration\nmp.messaging.outgoing.mock-messages.connector=smallrye-kafka\nmp.messaging.outgoing.mock-messages.topic=${START_TOPIC_NAME}\nmp.messaging.outgoing.mock-messages.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n\n\n\n# Quarkus Kafka Streams configuration settings\nquarkus.kafka-streams.bootstrap-servers=${BOOTSTRAP_SERVERS}\nquarkus.kafka-streams.application-id=financial-stream\nquarkus.kafka-streams.application-server=localhost:8080\nquarkus.kafka-streams.topics=${START_TOPIC_NAME},${TARGET_TOPIC_NAME}\nquarkus.kafka-streams.health.enabled=true\n\nquarkus.kafka-streams.security.protocol=SASL_SSL\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\nquarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\nquarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nquarkus.kafka-streams.ssl.truststore.location=${CERT_LOCATION}\nquarkus.kafka-streams.ssl.truststore.password=${CERT_PASSWORD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n\n# pass-through options\nkafka-streams.cache.max.bytes.buffering=10240\nkafka-streams.commit.interval.ms=1000\nkafka-streams.metadata.max.age.ms=500\nkafka-streams.auto.offset.reset=latest\nkafka-streams.metrics.recording.level=DEBUG\n```\n\n- If using a previous IBM Event Streams version (such as v2019.4.2) or on IBM Cloud, use the following configuration in the `application.properties` file:\n\n```properties\nquarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=PLAIN\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.PlainLoginModule required \\\n                username=\"token\" \\\n                password=${API_KEY};\n# If connecting to Event Streams on IBM Cloud the following truststore options are not needed.\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=password\n\n\n# Initial mock JSON message producer configuration\nmp.messaging.outgoing.mock-messages.connector=smallrye-kafka\nmp.messaging.outgoing.mock-messages.topic=${START_TOPIC_NAME}\nmp.messaging.outgoing.mock-messages.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n\n\n\n# Quarkus Kafka Streams configuration settings\nquarkus.kafka-streams.bootstrap-servers=${BOOTSTRAP_SERVERS}\nquarkus.kafka-streams.application-id=financial-stream\nquarkus.kafka-streams.application-server=localhost:8080\nquarkus.kafka-streams.topics=${START_TOPIC_NAME},${TARGET_TOPIC_NAME}\nquarkus.kafka-streams.health.enabled=true\n\nquarkus.kafka-streams.security.protocol=SASL_SSL\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\nquarkus.kafka-streams.sasl.mechanism=PLAIN\nquarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.PlainLoginModule required \\\n                username=\"token\" \\\n                password=${API_KEY};\n# If connecting to Event Streams on IBM Cloud the following truststore options are not needed.\nquarkus.kafka-streams.ssl.truststore.location=${CERT_LOCATION}\nquarkus.kafka-streams.ssl.truststore.password=password\n\n# pass-through options\nkafka-streams.cache.max.bytes.buffering=10240\nkafka-streams.commit.interval.ms=1000\nkafka-streams.metadata.max.age.ms=500\nkafka-streams.auto.offset.reset=latest\nkafka-streams.metrics.recording.level=DEBUG\n```\n\nThere are some environment variables our `application.properties` file depends on:\n\n- `START_TOPIC_NAME`: The Kafka topic the mock producer will produce `FinancialMessage` to and the topology consume messages from for processing. **IMPORTANT:** Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you **must** create this topic in IBM Event Streams. See [here](/use-cases/overview/pre-requisites) for more details as to how to create a topic.\n- `TARGET_TOPIC_NAME`: The Kafka topic the topology will produce the processed `FinancialMessage` to. **IMPORTANT:** Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you **must** create this topic in IBM Event Streams. See [here]((/use-cases/overview/pre-requisites) for more details as to how to create a topic\n- `BOOTSTRAP_SERVERS`: Your IBM Event Streams bootstrap server. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n- `CERT_LOCATION`: The location where the PKCS12 certificate for the SSL connection to the IBM Event Streams instance is. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n- `CERT_PASSWORD`: The password of the PKCS12 certificate. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n- `API_KEY` if you are using an IBM Event Streams instance in IBM Cloud or\n- `SCRAM_USERNAME` and `SCRAM_PASSWORD`: The SCRAM credentials for your application to get authenticated and authorized to work with IBM Event Streams. See [here](/use-cases/overview/pre-requisites) for more details as to how to obtain these.\n\nExport the variables and values on the terminal you will run the application from:\n\n- IBM Event Strams v10 or later:\n\n```shell\nexport BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \\\nexport CERT_LOCATION=/path-to-pkcs12-cert/es-cert.p12 \\\nexport CERT_PASSWORD=certificate-password \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-password\n```\n\n- Previous IBM Event Streams versions:\n\n```shell\nexport BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \\\nexport CERT_LOCATION=/path-to-jks-cert/es-cert.jks \\\nexport API_KEY=your-api-key\n```\n\n- Local Kafka Cluster:\n\n```shell\nexport BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \n```\n\n- You can now test the Quarkus application\n\n```shell\n./mvnw quarkus:dev\n```\n\n- You should now be able to see `FinancialMessage` events in the input topic and those processed messages in the output topic you have specified above in the IBM Event Streams user interface.","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-streams/lab-2/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}