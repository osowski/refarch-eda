{"componentChunkName":"component---src-pages-use-cases-kafka-streams-lab-0-index-mdx","path":"/use-cases/kafka-streams/lab-0/","result":{"pageContext":{"frontmatter":{"title":"Kafka Streams Test Lab 0","description":"An introduction to using test Kafka Streams Test Suite to test Kafka Streams Topologies."},"relativePagePath":"/use-cases/kafka-streams/lab-0/index.mdx","titleType":"append","MdxNode":{"id":"b47073dc-3213-5361-a561-6dd5c3218219","children":[],"parent":"05ff32fe-a0f5-5e69-8db3-ffeb81507036","internal":{"content":"---\ntitle: Kafka Streams Test Lab 0\ndescription: An introduction to using test Kafka Streams Test Suite to test Kafka Streams Topologies.\n---\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Setting up the Quarkus Application</AnchorLink>\n    <AnchorLink>Creating your first Test Class</AnchorLink>\n    <AnchorLink>Creating your first Tests</AnchorLink>\n    <AnchorLink>Next Steps</AnchorLink>\n</AnchorLinks>\n\n## Overview\n- In this lab scenario we're going to use [Quarkus](https://quarkus.io) - a subatomic and supersonic framework for Java for\nthe purposes of this lab.\n- We will be testing using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TestDriver to mimic a Topology, a Stream and Table.\n- While using the TestDriver we will perform basic stateless operations and understand the testing infrastructure.\n\n## Scenario Prerequisites\n**Java**\n- For the purposes of this lab we suggest Java 8+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n## Setting up the Quarkus Application\n- We will bootstrap the Quarkus application with the following Maven command\n\n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.10.5.Final:create \\\n    -DprojectGroupId=com.ibm \\\n    -DprojectArtifactId=quarkus-kstreams-lab-zero \\\n    -Dextensions=\"resteasy-jsonb,quarkus-kafka-streams\"\n```\n\n- Since we will be using the Kafka Streams testing functionality, we will need to edit the `pom.xml` to add the dependency to our project. Open `pom.xml` and add the following:\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n<dependency>\n\t<groupId>org.hamcrest</groupId>\n\t<artifactId>hamcrest</artifactId>\n\t<version>2.2</version>\n</dependency>\n```\n\nThe second dependency is for the hamcrest Domain Specific Language for test assertion.\n\n## Creating your first Test Class\n\n- Now let's create our first Test Class.\n\n- Create the directory structure you will need for your Java file. _(**NOTE:** If you are working in an IDE, this may be done for you when you create your package and classes.)_\n\n   ```mkdir -p src/test/java/eda/kafka/streams```\n\n- Create a new file named `src/test/java/eda/kafka/streams/FirstKafkaStreamsTest.java`.\n\n- Paste the following content into the `FirstKafkaStreamsTest` class:\n```java\npackage eda.kafka.streams;\n\nimport static org.hamcrest.CoreMatchers.equalTo;\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.MatcherAssert.assertThat;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.test.junit.QuarkusTest;\n\n@QuarkusTest\npublic class FirstKafkaStreamsTest {\n\n\tprivate static TopologyTestDriver testDriver;\n\tprivate static String inTopicName = \"my-input-topic\";\n\tprivate static String outTopicName = \"my-output-topic\";\n\n\tprivate static TestInputTopic<String, String> inTopic;\n\tprivate static TestOutputTopic<String, String> outTopic;\n\n\t@BeforeEach\n\tpublic void buildTopology() {\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab0\");\n\t\tprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:2345\");\n\t\tprops.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n\t\tprops.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n\n\t\tfinal StreamsBuilder builder = new StreamsBuilder();\n\t\tKStream<String, String> basicColors = builder.stream(inTopicName,Consumed.with(Serdes.String(), Serdes.String()));\n\t\tbasicColors.peek((key, value) -> System.out.println(\"PRE-FILTER: key=\" + key + \", value=\" + value))\n\t\t\t.filter((key, value) -> (\"BLUE\".equalsIgnoreCase(value)))\n\t\t\t.peek((key, value) -> System.out.println(\"POST-FILTER: key=\" + key + \", value=\" + value))\n\t\t\t.to(outTopicName);\n\n\t\tTopology topology = builder.build();\n\n\t\ttestDriver = new TopologyTestDriver(topology, props);\n\t\tinTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new StringSerializer());\n\t\toutTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new StringDeserializer());\n\n\t}\n\n\t@AfterEach\n\tpublic void teardown() {\n\t\ttestDriver.close();\n\t}\n\n}\n```\n\n- The above code does a lot in a few lines, so we'll walk through some of that here.\n  - The `@BeforeEach` annotation on the `buildTopology` method means that it will be run each time before each test is executed, while the `@AfterEach` annotation on the `teardown` method ensures that it will be run each time after each test execution. This allows us to spin up and tear down all the necessary components to test in isolation with each test case.\n  - The `buildTopology` method utilizes the `StreamsBuilder` class to construct a simple topology, reading from the input Kafka topic defined by the `inTopicName` String.\n  - The topology, we build here, utilizes three of the stateless processors the Kafka Streams API:\n      - `peek` allows us to look at the key and the value of the record passing through the stream and continue processing it unaffected _(so we leverage this before and after the next processor used to see what is making its way through the topology)_\n      - `filter` allows us to drop records that do not meet the criteria specified _(either for the key or the value)_. In this test class, we are filtering on any value that does not match the word `\"BLUE\"` _(using a case-insensitive search)_\n      - `to` is the final processor used and to write the contents of the topology at that point to an output Kafka topic\n   - The Kafka Streams Test infrastructure provides us the capability to leverage driver classes that function as their own input and output topics, removing the need from connecting directly to a live Kafka instance. The `inTopic` and `outTopic` instantiation at the bottom of the `buildTopology` method hooks into this test infrastructure, so that our test methods can use them to write to and read from the topology.\n  - The `teardown` method cleans up the topology and all the data that has been sent through it for any given test run, allowing us to reset and rerun test cases as needed.\n\n\n\n- Build the application by running the following:\n```shell\n./mvnw clean verify\n```\n\n- You should see output similar to the following:\n```\n...\n[INFO]\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO]\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0\n[INFO]\n...\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  29.470 s\n[INFO] Finished at: 2020-09-17T09:34:26-05:00\n[INFO] ------------------------------------------------------------------------\n```\n\n**Note:** Depending upon versions of the packages brought in, you may see an initial test failure due to needing to update the `src/main/resources/application.properties` file with the following properties: _(The values are insignifcant for the execution of our tests, but he existence of the property is required by the underlying Quarkus & Kafka Streams integration)_\n\n```properties\nquarkus.kafka-streams.application-id=my-kafka-streams\nquarkus.kafka-streams.topics=topic1\n```\n\n- The build compiled and the test topology was successfully created. But no tests were run, because no tests were written!\n\n## Creating your first Tests\n\n- Open `src/test/java/eda/kafka/streams/FirstKafkaStreamsTest.java` and add the following tests to the bottom of the `FirstKafkaStreamsTest` class:\n\n```java\n\t@Test\n\tpublic void isEmpty() {\n\t\tassertThat(outTopic.isEmpty(), is(true));\n\t}\n\n\t@Test\n\tpublic void isNotEmpty() {\n\t\tassertThat(outTopic.isEmpty(), is(true));\n\t\tinTopic.pipeInput(\"C01\", \"blue\");\n\t\tassertThat(outTopic.getQueueSize(), equalTo(1L) );\n\t\tassertThat(outTopic.readValue(), equalTo(\"blue\"));\n\t\tassertThat(outTopic.getQueueSize(), equalTo(0L) );\n\t}\n\n\t@Test\n\tpublic void selectBlues() {\n\t\tassertThat(outTopic.isEmpty(), is(true));\n\n\t\tinTopic.pipeInput(\"C01\", \"blue\");\n\t\tinTopic.pipeInput(\"C02\", \"red\");\n\t\tinTopic.pipeInput(\"C03\", \"green\");\n\t\tinTopic.pipeInput(\"C04\", \"Blue\");\n\n\t\tassertThat(outTopic.getQueueSize(), equalTo(2L) );\n\n\t\tassertThat(outTopic.isEmpty(), is(false));\n\n\t\tassertThat(outTopic.readValue(), equalTo(\"blue\"));\n\t\tassertThat(outTopic.readValue(), equalTo(\"Blue\"));\n\n\t\tassertThat(outTopic.getQueueSize(), equalTo(0L) );\n\n\t}\n```\n\n- These are three simple tests:\n  - The `isEmpty` test method checks to make sure the output topic is empty when nothing is sent through the topology\n  - The `isNotEmpty` test method checks to make sure the output topic is not empty when an item matching our filters is sent through the topology\n  - The `selectBlues` test method checks to make sure that our topology is filtering correctly when we send multiple items through the topology and the output topic empties correctly when the testing infrastructure reads from it.\n\n- You should see the tests pass with the following output:\n```shell\n[INFO]\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running eda.kafka.streams.FirstKafkaStreamsTest\n2020-09-17 09:44:33,247 INFO  [io.sma.rea.mes.provider] (main) SRMSG00208: Deployment done... start processing\n2020-09-17 09:44:33,250 INFO  [io.sma.rea.mes.provider] (main) SRMSG00226: Found incoming connectors: [smallrye-kafka]\n2020-09-17 09:44:33,251 INFO  [io.sma.rea.mes.provider] (main) SRMSG00227: Found outgoing connectors: [smallrye-kafka]\n2020-09-17 09:44:33,252 INFO  [io.sma.rea.mes.provider] (main) SRMSG00229: Channel manager initializing...\n2020-09-17 09:44:33,254 INFO  [io.sma.rea.mes.provider] (main) SRMSG00209: Initializing mediators\n2020-09-17 09:44:33,255 INFO  [io.sma.rea.mes.provider] (main) SRMSG00215: Connecting mediators\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Quarkus 1.8.0.Final on JVM started in 2.029s. Listening on: http://0.0.0.0:8081\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Profile test activated.\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, mutiny, resteasy-jsonb, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]\nPRE-FILTER: key=C01, value=blue\nPOST-FILTER: key=C01, value=blue\nPRE-FILTER: key=C02, value=red\nPRE-FILTER: key=C03, value=green\nPRE-FILTER: key=C04, value=Blue\nPOST-FILTER: key=C04, value=Blue\nPRE-FILTER: key=C01, value=blue\nPOST-FILTER: key=C01, value=blue\n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.722 s - in eda.kafka.streams.FirstKafkaStreamsTest\n2020-09-17 09:44:34,026 INFO  [io.sma.rea.mes.provider] (main) SRMSG00207: Cancel subscriptions\n2020-09-17 09:44:34,038 INFO  [io.quarkus] (main) Quarkus stopped in 0.024s\n[INFO]\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n[INFO]\n[INFO]\n```\n\n## Next Steps\n\n- Now that you have finished the foundational Kafka Streams testing lab, you can proceed to [Lab 1](/use-cases/kafka-streams/lab-1/) for a deeper dive into more robust real-world Kafka Streams testing use cases!\n","type":"Mdx","contentDigest":"dd40ed91045547fef00ee5b4f84725bf","owner":"gatsby-plugin-mdx","counter":771},"frontmatter":{"title":"Kafka Streams Test Lab 0","description":"An introduction to using test Kafka Streams Test Suite to test Kafka Streams Topologies."},"exports":{},"rawBody":"---\ntitle: Kafka Streams Test Lab 0\ndescription: An introduction to using test Kafka Streams Test Suite to test Kafka Streams Topologies.\n---\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Setting up the Quarkus Application</AnchorLink>\n    <AnchorLink>Creating your first Test Class</AnchorLink>\n    <AnchorLink>Creating your first Tests</AnchorLink>\n    <AnchorLink>Next Steps</AnchorLink>\n</AnchorLinks>\n\n## Overview\n- In this lab scenario we're going to use [Quarkus](https://quarkus.io) - a subatomic and supersonic framework for Java for\nthe purposes of this lab.\n- We will be testing using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TestDriver to mimic a Topology, a Stream and Table.\n- While using the TestDriver we will perform basic stateless operations and understand the testing infrastructure.\n\n## Scenario Prerequisites\n**Java**\n- For the purposes of this lab we suggest Java 8+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n## Setting up the Quarkus Application\n- We will bootstrap the Quarkus application with the following Maven command\n\n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.10.5.Final:create \\\n    -DprojectGroupId=com.ibm \\\n    -DprojectArtifactId=quarkus-kstreams-lab-zero \\\n    -Dextensions=\"resteasy-jsonb,quarkus-kafka-streams\"\n```\n\n- Since we will be using the Kafka Streams testing functionality, we will need to edit the `pom.xml` to add the dependency to our project. Open `pom.xml` and add the following:\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n<dependency>\n\t<groupId>org.hamcrest</groupId>\n\t<artifactId>hamcrest</artifactId>\n\t<version>2.2</version>\n</dependency>\n```\n\nThe second dependency is for the hamcrest Domain Specific Language for test assertion.\n\n## Creating your first Test Class\n\n- Now let's create our first Test Class.\n\n- Create the directory structure you will need for your Java file. _(**NOTE:** If you are working in an IDE, this may be done for you when you create your package and classes.)_\n\n   ```mkdir -p src/test/java/eda/kafka/streams```\n\n- Create a new file named `src/test/java/eda/kafka/streams/FirstKafkaStreamsTest.java`.\n\n- Paste the following content into the `FirstKafkaStreamsTest` class:\n```java\npackage eda.kafka.streams;\n\nimport static org.hamcrest.CoreMatchers.equalTo;\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.MatcherAssert.assertThat;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.test.junit.QuarkusTest;\n\n@QuarkusTest\npublic class FirstKafkaStreamsTest {\n\n\tprivate static TopologyTestDriver testDriver;\n\tprivate static String inTopicName = \"my-input-topic\";\n\tprivate static String outTopicName = \"my-output-topic\";\n\n\tprivate static TestInputTopic<String, String> inTopic;\n\tprivate static TestOutputTopic<String, String> outTopic;\n\n\t@BeforeEach\n\tpublic void buildTopology() {\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab0\");\n\t\tprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:2345\");\n\t\tprops.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n\t\tprops.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n\n\t\tfinal StreamsBuilder builder = new StreamsBuilder();\n\t\tKStream<String, String> basicColors = builder.stream(inTopicName,Consumed.with(Serdes.String(), Serdes.String()));\n\t\tbasicColors.peek((key, value) -> System.out.println(\"PRE-FILTER: key=\" + key + \", value=\" + value))\n\t\t\t.filter((key, value) -> (\"BLUE\".equalsIgnoreCase(value)))\n\t\t\t.peek((key, value) -> System.out.println(\"POST-FILTER: key=\" + key + \", value=\" + value))\n\t\t\t.to(outTopicName);\n\n\t\tTopology topology = builder.build();\n\n\t\ttestDriver = new TopologyTestDriver(topology, props);\n\t\tinTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new StringSerializer());\n\t\toutTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new StringDeserializer());\n\n\t}\n\n\t@AfterEach\n\tpublic void teardown() {\n\t\ttestDriver.close();\n\t}\n\n}\n```\n\n- The above code does a lot in a few lines, so we'll walk through some of that here.\n  - The `@BeforeEach` annotation on the `buildTopology` method means that it will be run each time before each test is executed, while the `@AfterEach` annotation on the `teardown` method ensures that it will be run each time after each test execution. This allows us to spin up and tear down all the necessary components to test in isolation with each test case.\n  - The `buildTopology` method utilizes the `StreamsBuilder` class to construct a simple topology, reading from the input Kafka topic defined by the `inTopicName` String.\n  - The topology, we build here, utilizes three of the stateless processors the Kafka Streams API:\n      - `peek` allows us to look at the key and the value of the record passing through the stream and continue processing it unaffected _(so we leverage this before and after the next processor used to see what is making its way through the topology)_\n      - `filter` allows us to drop records that do not meet the criteria specified _(either for the key or the value)_. In this test class, we are filtering on any value that does not match the word `\"BLUE\"` _(using a case-insensitive search)_\n      - `to` is the final processor used and to write the contents of the topology at that point to an output Kafka topic\n   - The Kafka Streams Test infrastructure provides us the capability to leverage driver classes that function as their own input and output topics, removing the need from connecting directly to a live Kafka instance. The `inTopic` and `outTopic` instantiation at the bottom of the `buildTopology` method hooks into this test infrastructure, so that our test methods can use them to write to and read from the topology.\n  - The `teardown` method cleans up the topology and all the data that has been sent through it for any given test run, allowing us to reset and rerun test cases as needed.\n\n\n\n- Build the application by running the following:\n```shell\n./mvnw clean verify\n```\n\n- You should see output similar to the following:\n```\n...\n[INFO]\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO]\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0\n[INFO]\n...\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  29.470 s\n[INFO] Finished at: 2020-09-17T09:34:26-05:00\n[INFO] ------------------------------------------------------------------------\n```\n\n**Note:** Depending upon versions of the packages brought in, you may see an initial test failure due to needing to update the `src/main/resources/application.properties` file with the following properties: _(The values are insignifcant for the execution of our tests, but he existence of the property is required by the underlying Quarkus & Kafka Streams integration)_\n\n```properties\nquarkus.kafka-streams.application-id=my-kafka-streams\nquarkus.kafka-streams.topics=topic1\n```\n\n- The build compiled and the test topology was successfully created. But no tests were run, because no tests were written!\n\n## Creating your first Tests\n\n- Open `src/test/java/eda/kafka/streams/FirstKafkaStreamsTest.java` and add the following tests to the bottom of the `FirstKafkaStreamsTest` class:\n\n```java\n\t@Test\n\tpublic void isEmpty() {\n\t\tassertThat(outTopic.isEmpty(), is(true));\n\t}\n\n\t@Test\n\tpublic void isNotEmpty() {\n\t\tassertThat(outTopic.isEmpty(), is(true));\n\t\tinTopic.pipeInput(\"C01\", \"blue\");\n\t\tassertThat(outTopic.getQueueSize(), equalTo(1L) );\n\t\tassertThat(outTopic.readValue(), equalTo(\"blue\"));\n\t\tassertThat(outTopic.getQueueSize(), equalTo(0L) );\n\t}\n\n\t@Test\n\tpublic void selectBlues() {\n\t\tassertThat(outTopic.isEmpty(), is(true));\n\n\t\tinTopic.pipeInput(\"C01\", \"blue\");\n\t\tinTopic.pipeInput(\"C02\", \"red\");\n\t\tinTopic.pipeInput(\"C03\", \"green\");\n\t\tinTopic.pipeInput(\"C04\", \"Blue\");\n\n\t\tassertThat(outTopic.getQueueSize(), equalTo(2L) );\n\n\t\tassertThat(outTopic.isEmpty(), is(false));\n\n\t\tassertThat(outTopic.readValue(), equalTo(\"blue\"));\n\t\tassertThat(outTopic.readValue(), equalTo(\"Blue\"));\n\n\t\tassertThat(outTopic.getQueueSize(), equalTo(0L) );\n\n\t}\n```\n\n- These are three simple tests:\n  - The `isEmpty` test method checks to make sure the output topic is empty when nothing is sent through the topology\n  - The `isNotEmpty` test method checks to make sure the output topic is not empty when an item matching our filters is sent through the topology\n  - The `selectBlues` test method checks to make sure that our topology is filtering correctly when we send multiple items through the topology and the output topic empties correctly when the testing infrastructure reads from it.\n\n- You should see the tests pass with the following output:\n```shell\n[INFO]\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running eda.kafka.streams.FirstKafkaStreamsTest\n2020-09-17 09:44:33,247 INFO  [io.sma.rea.mes.provider] (main) SRMSG00208: Deployment done... start processing\n2020-09-17 09:44:33,250 INFO  [io.sma.rea.mes.provider] (main) SRMSG00226: Found incoming connectors: [smallrye-kafka]\n2020-09-17 09:44:33,251 INFO  [io.sma.rea.mes.provider] (main) SRMSG00227: Found outgoing connectors: [smallrye-kafka]\n2020-09-17 09:44:33,252 INFO  [io.sma.rea.mes.provider] (main) SRMSG00229: Channel manager initializing...\n2020-09-17 09:44:33,254 INFO  [io.sma.rea.mes.provider] (main) SRMSG00209: Initializing mediators\n2020-09-17 09:44:33,255 INFO  [io.sma.rea.mes.provider] (main) SRMSG00215: Connecting mediators\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Quarkus 1.8.0.Final on JVM started in 2.029s. Listening on: http://0.0.0.0:8081\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Profile test activated.\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, mutiny, resteasy-jsonb, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]\nPRE-FILTER: key=C01, value=blue\nPOST-FILTER: key=C01, value=blue\nPRE-FILTER: key=C02, value=red\nPRE-FILTER: key=C03, value=green\nPRE-FILTER: key=C04, value=Blue\nPOST-FILTER: key=C04, value=Blue\nPRE-FILTER: key=C01, value=blue\nPOST-FILTER: key=C01, value=blue\n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.722 s - in eda.kafka.streams.FirstKafkaStreamsTest\n2020-09-17 09:44:34,026 INFO  [io.sma.rea.mes.provider] (main) SRMSG00207: Cancel subscriptions\n2020-09-17 09:44:34,038 INFO  [io.quarkus] (main) Quarkus stopped in 0.024s\n[INFO]\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n[INFO]\n[INFO]\n```\n\n## Next Steps\n\n- Now that you have finished the foundational Kafka Streams testing lab, you can proceed to [Lab 1](/use-cases/kafka-streams/lab-1/) for a deeper dive into more robust real-world Kafka Streams testing use cases!\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-streams/lab-0/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}