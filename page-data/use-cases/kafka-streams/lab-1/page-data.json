{"componentChunkName":"component---src-pages-use-cases-kafka-streams-lab-1-index-mdx","path":"/use-cases/kafka-streams/lab-1/","result":{"pageContext":{"frontmatter":{"title":"Kafka Streams Test Lab 1","description":"Using Kafka Streams Test Suite to test Streams"},"relativePagePath":"/use-cases/kafka-streams/lab-1/index.mdx","titleType":"append","MdxNode":{"id":"f9d2489b-fcf9-5996-a651-4e971989e64d","children":[],"parent":"8f88b717-54f0-5681-84cf-013d362be8c1","internal":{"content":"---\ntitle: Kafka Streams Test Lab 1\ndescription: Using Kafka Streams Test Suite to test Streams\n---\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Setting up the Quarkus Application</AnchorLink>\n    <AnchorLink>Creating your first Test Class</AnchorLink>\n    <AnchorLink>More Robust Kafka Streams Testing</AnchorLink>\n    <AnchorLink>Next Steps</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n- In this lab scenario we're going to use [Quarkus](https://quarkus.io) - a subatomic and supersonic framework for Java for\nthe purposes of this lab.\n- We will be testing using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TestDriver to mimic a Topology, a Stream and Table.\n- While using the TestDriver we will perform operations such as groupBy, join with another Stream or Kafka Table.\n- Lastly and optionally (in Lab 2) we will use Kafka Streams to send events to a Kafka Topic on IBM Event Streams on Cloud Pak for Integration.\n\n\n\n## Scenario Prerequisites\n**Java**\n- For the purposes of this lab we suggest Java 8+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n\n## Setting up the Quarkus Application\n- We will bootstrap the Quarkus application with the following Maven command\n\n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.6.0.Final:create \\\n    -DprojectGroupId={com.ibm} \\\n    -DprojectArtifactId={quarkus-kstreams-lab} \\\n    -Dextensions=\"kafka,kafka-streams,resteasy-jsonb,quarkus-kafka-streams\"\n```\n\nYou can replace the fields within {} as you like.\n\n- Since we will be using the Kafka Streams testing functionality we will need to edit the `pom.xml` to add\nthe dependency to our project. Open `pom.xml` and add the following.\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n```\n\n## Creating your first Test Class\n\n- Now let's create our first Test Class.\n```src/test/java/.../TestLoadKtableFromTopic.java```\nYou can customize this file path post `src/test/java` to however you see fit.\n\n\n- Open the `TestLoadKtableFromTopic.java` file and paste the following content.\n```java\npackage com.ibm.garage.cpat.lab;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KTable;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.processor.StateStore;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueIterator;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.test.junit.QuarkusTest;\n\n/**\n * This is a simple example of loading some reference data from stream into a Ktable for\n * lookup. It uses a persistent state store.\n */\n@QuarkusTest\npublic class TestLoadKtableFromTopic {\n    private static TopologyTestDriver testDriver;\n    private static String companySectorsTopic = \"sector-types\";\n    private static String storeName = \"sector-types-store\";\n\n    private static TestInputTopic<String, String> inTopic;\n    private static TestOutputTopic<String, Long> outTopic;\n    private static TestOutputTopic<String, String> errorTopic;\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab1\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:1234\");\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology(){\n        final StreamsBuilder builder = new StreamsBuilder();\n        // Adding a state store is a simple matter of creating a StoreSupplier\n        // instance with one of the static factory methods on the Stores class.\n        // all persistent StateStore instances provide local storage using RocksDB\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n\n        // A KTable is created from the companySectorsTopic, with key and value deserialized.\n        // With Materialized.as() causing the Table to force a state store materialization (storeSupplier).\n        KTable<String, String> sectorTypeTable = builder.table(companySectorsTopic,\n                Consumed.with(Serdes.String(), Serdes.String()),\n                Materialized.as(storeSupplier));\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(companySectorsTopic, new StringSerializer(), new StringSerializer());\n\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n\n    @Test\n    public void shouldHaveSixSectorTypes(){\n        inTopic.pipeInput(\"C01\",\"Health Care\");\n        inTopic.pipeInput(\"C02\",\"Finance\");\n        inTopic.pipeInput(\"C03\",\"Consumer Services\");\n        inTopic.pipeInput(\"C04\",\"Transportation\");\n        inTopic.pipeInput(\"C05\",\"Capital Goods\");\n        inTopic.pipeInput(\"C06\",\"Public Utilities\");\n\n        KeyValueStore<String,ValueAndTimestamp<String>> store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertNotNull(store);\n\n        ValueAndTimestamp<String> sector = store.get(\"C02\");\n        Assertions.assertNotNull(sector);\n        Assertions.assertEquals(\"Finance\", sector.value());\n        Assertions.assertEquals(6, store.approximateNumEntries());\n\n\n        // demonstrate how to get all the values from the table:\n        KeyValueIterator<String, ValueAndTimestamp<String>> sectors = store.all();\n        while (sectors.hasNext()) {\n            KeyValue<String,ValueAndTimestamp<String>> s = sectors.next();\n            System.out.println(s.key + \":\" + s.value.value());\n        }\n        for ( StateStore s: testDriver.getAllStateStores().values()) {\n            System.out.println(s.name());\n        }\n    }\n}\n```\n\n- What the above code does is it uses TopologyTestDriver to mimic a Topology. A Topology is basically a graph of\nstream processors (nodes) and the edges between these nodes are the streams. In the first sectinon we instantiate\nour `TopologyTestDriver` named `testDriver`, as well as the topic name and store name.\n\n\n\n- Test the application by running the following\n*Note* You might need to comment out the Quarkus Kafka Streams dependency in `pom.xml` as that dependency requires some\nconfigration in a properties file to pass. This is configured in the `Producing and Consuming to a Kafka Topic on Event Streams` section\nfurther down in the lab.\n\n```shell\n./mvnw clean verify\n```\n\n- Depending upon versions of the packages brought in, you may see an initial test failure due to needing to update the `src/main/resources/application.properties` file with the following properties: _(The values are insignifcant for the execution of our tests, but are existence of the property is required by the underlying Quarkus & Kafka Streams integration)_\n```properties\nquarkus.kafka-streams.application-id=my-kafka-streams\nquarkus.kafka-streams.topics=topic1\n```\n\n- How this test topology creation flow works:\n    - A StreamsBuilder object (builder) from the Kafka Streams DSL API is created.\n    - A KeyValueBytesStoreSupplier (storeSupplier) is configured with String variable (storeName).\n    - A KTable is created reading from the topic (companySectorsTopic), deserialized and materialized as\n    the previously create (storeSupplier).\n    - A TopologyTestDriver (testDriver) is built from the provided config properties and the KTable within the builder topology.\n    - Lastly test input topic (inTopic) is created from the testDriver topology.\n    - When `inTopic.pipeInput(\"C01\",\"Health Care\");` is invoked, it populates the topic, which then populates the KTable\n    which ultimately persists in a KeyValue State Store.\n\n- You should see the tests pass. These are three simple tests. The first of which checks that the value fetched from\nthe Kafka Table is not null,the second makes sure that value retrieved from key `C02` is equal to `Finance` and lastly\nwe make sure that the our state store (which was piped by ways of the Kafka Topic) indeed has six key-value pairs.\n\n\n\n## More Robust Kafka Streams Testing\n\n- Now that we have tested some simple functionality by using the Kafka Streams API let's check out some other\noperators that we can use.\n\n- Let's create a new class for our Plain Old Java Object (POJO) named FinancialMessage. You can place this where you want\nbut for simplicity's sake I will use this path. Make sure you remember the path for when you import this class\n`src/main/java/com/ibm/garage/cpat/FinancialMessage/FinancialMessage.java`\n\nNow copy and paste the following content into the newly created file.\n\n```java\npublic class FinancialMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public String exchangeId;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public int institutionId;\n    public int countryId;\n    public boolean technicalValidation;\n\n    public FinancialMessage() {\n\n    }\n\n    public FinancialMessage(String userId, String stockSymbol, String exchangeId,\n                            int quantity, double stockPrice, double totalCost,\n                            int institutionId, int countryId, boolean technicalValidation) {\n\n        this.userId = userId;\n        this.stockSymbol = stockSymbol;\n        this.exchangeId = exchangeId;\n        this.quantity = quantity;\n        this.stockPrice = stockPrice;\n        this.totalCost = totalCost;\n        this.institutionId = institutionId;\n        this.countryId = countryId;\n        this.technicalValidation = technicalValidation;\n    }\n}\n```\n\n**Note** - For brevity and simplicity's sake I did not provide any accessors (getters) or mutators (setters). You can\nset those at your own discretion.\n\n- Now that we have our Java class, let's create a new and separate Java Test class separate from the `TestLoadKtableFromTopic.java`\nfor separation of logic. `src/test/java/.../lab/TestFinancialMessage.java`. Again you may use your own\nfilepath. Copy the contents below.\n\n```java\npackage com.ibm.garage.cpat.lab;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.LongDeserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Windowed;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\nimport io.quarkus.kafka.client.serialization.JsonbSerializer;\nimport io.quarkus.test.junit.QuarkusTest;\n\nimport com.ibm.garage.cpat.Domain.*;\n\n\n@QuarkusTest\npublic class TestFinancialMessage {\n\n    private static TopologyTestDriver testDriver;\n    private static String inTopicName = \"transactions\";\n    private static String outTopicName = \"output\";\n    private static String errorTopicName = \"errors\";\n    private static String storeName = \"transactionCount\";\n    private static TestInputTopic<String, FinancialMessage> inTopic;\n    private static TestOutputTopic<String, Long> outTopic;\n    private static TestOutputTopic<String, String> errorTopic;\n\n    private static final JsonbSerde<FinancialMessage> financialMessageSerde = new JsonbSerde<>(FinancialMessage.class);\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab2\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:2345\");\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde);\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n\n        KStream<String, FinancialMessage> transactionStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        // First verify user id is present, if not route to error\n        KStream<String, FinancialMessage>[] branches =\n                transactionStream.branch(\n                    (key, value) -> value.userId == null,\n                    (key, value) -> true\n                );\n\n        // Handle error by sending to the errors topic.\n        branches[0].map(\n                 (key, value) -> { return KeyValue.pair(key, \"No customer id provided\");}\n                 )\n                .to(\n                    errorTopicName, Produced.with(Serdes.String(), Serdes.String())\n                );\n\n\n        // use groupBy to swap the key, then count by customer id,\n        branches[1].groupBy(\n                    (key, value) -> value.userId\n                )\n                .count(\n                    Materialized.as(storeSupplier)\n                )\n                .toStream()\n                .to(\n                    outTopicName,\n                    Produced.with(Serdes.String(), Serdes.Long())\n            );\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer<FinancialMessage>());\n        //outTopic = testDriver.createOutputTopic(outTopicName,windowedSerde.deserializer(), new LongDeserializer());\n        outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new LongDeserializer());\n        errorTopic = testDriver.createOutputTopic(errorTopicName, new StringDeserializer(), new StringDeserializer());\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n}\n```\n\n- We have the setup for the TestTopology. Now we can add a test that will insert two events into the topic.\n\n```java\n    @Test\n    public void shouldHaveOneTransaction() {\n        // A FinancialMessage is mocked and set to the input topic. Within the Topology,\n        // this gets sent to the outTopic because a userId exists for the incoming message.\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"2\", \"ASDF\", \"HELLO\", 5, 1000.22, 4444.12, 38, 6, true\n        );\n\n        inTopic.pipeInput(\"T01\", mock);\n        inTopic.pipeInput(\"T02\", mock2);\n\n        Assertions.assertFalse(outTopic.isEmpty());\n        Assertions.assertEquals(1, outTopic.readKeyValue().value);\n\n        KeyValueStore<String,ValueAndTimestamp<FinancialMessage>> store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(1, store.approximateNumEntries());\n    }\n```\n\n- The state store (storeSupplier) has two records input. We have a value count when a groupBy is performed. This particular\ntest will fail of course, due to the fact that we inserted two records but our test expects one. To remedy this test we can change\n`Assertions.assertEquals(1, store.approximateNumEntries());` the 1 to a 2.\n\n- Next let's add another very simple test.\n\n```java\n    @Test\n    public void testErrorTopicIsNotEmpty() {\n        FinancialMessage mock = new FinancialMessage(\n            null, \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n\n        inTopic.pipeInput(\"T03\", mock);\n\n        Assertions.assertFalse(errorTopic.isEmpty());\n    }\n}\n```\n\nAs you can see here our message payload is created with `null` for the userId field and the purpose of the test\nis to check if our `errorTopic` is empty. Since our `errorTopic.isEmpty()` resolves to false and our assertion\nis asserting that it is false as well, thus the test passes.\n\n\n- Now that we have two simple tests, let's update our first branch to allow us to filter the stream on a condition\nthat we want. Let's edit our `branches[1]` statement so that it will filter out and retain only the records where\nthe `totalCost` is greater than 5000.\n\n*Note* Since we are changing the logic of how `branches[1]` functions the `shouldHaveOneTransaction` test function will no longer\nfunction as we intended. Comment out the `@Test` annotation or the whole function before proceeding.\n\n```java\nbranches[1].filter(\n            (key, value) -> (value.totalCost > 5000)\n        )\n        .groupBy(\n            (key, value) -> value.userId\n        )\n        .count(\n            Materialized.as(storeSupplier)\n        )\n        .toStream()\n        .to(\n            outTopicName,\n            Produced.with(Serdes.String(), Serdes.Long())\n        );\n```\n\n- Now let's create a test that will insert two records and check if we have two records in the output topic.\n\n```java\n    @Test\n    public void filteredStreamHasTwoRecords() {\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"2\", \"ASDF\", \"HELLO\", 5, 1000.22, 4444.12, 38, 6, true\n        );\n        inTopic.pipeInput(\"T01\", mock);\n        inTopic.pipeInput(\"T02\", mock2);\n\n        KeyValueStore<String,ValueAndTimestamp<FinancialMessage>> store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(2, store.approximateNumEntries());\n    }\n```\n\nThis test fails. Why? The `mock2` message has a `totalCost` of `4444.12` which is less than the filter condition of\n`totalCost` being greater than 5000. To make this test pass change that value to something like `5001`.\n\n\n## Next Steps\n\n- Now that you have finished this initial part of Lab 1 you can optionally proceed to [Lab 2](/use-cases/kafka-streams/lab-2/)\n","type":"Mdx","contentDigest":"1eb90a8d80ddd2ce5071fa2a80de3231","counter":584,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Streams Test Lab 1\ndescription: Using Kafka Streams Test Suite to test Streams\n---\n\n<AnchorLinks>\n    <AnchorLink>Overview</AnchorLink>\n    <AnchorLink>Scenario Prerequisites</AnchorLink>\n    <AnchorLink>Setting up the Quarkus Application</AnchorLink>\n    <AnchorLink>Creating your first Test Class</AnchorLink>\n    <AnchorLink>More Robust Kafka Streams Testing</AnchorLink>\n    <AnchorLink>Next Steps</AnchorLink>\n</AnchorLinks>\n\n\n## Overview\n- In this lab scenario we're going to use [Quarkus](https://quarkus.io) - a subatomic and supersonic framework for Java for\nthe purposes of this lab.\n- We will be testing using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TestDriver to mimic a Topology, a Stream and Table.\n- While using the TestDriver we will perform operations such as groupBy, join with another Stream or Kafka Table.\n- Lastly and optionally (in Lab 2) we will use Kafka Streams to send events to a Kafka Topic on IBM Event Streams on Cloud Pak for Integration.\n\n\n\n## Scenario Prerequisites\n**Java**\n- For the purposes of this lab we suggest Java 8+\n\n**Maven**\n- Maven will be needed for bootstrapping our application from the command-line and running\nour application.\n\n**An IDE of your choice**\n- Ideally an IDE that supports Quarkus (such as Visual Studio Code)\n\n\n## Setting up the Quarkus Application\n- We will bootstrap the Quarkus application with the following Maven command\n\n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.6.0.Final:create \\\n    -DprojectGroupId={com.ibm} \\\n    -DprojectArtifactId={quarkus-kstreams-lab} \\\n    -Dextensions=\"kafka,kafka-streams,resteasy-jsonb,quarkus-kafka-streams\"\n```\n\nYou can replace the fields within {} as you like.\n\n- Since we will be using the Kafka Streams testing functionality we will need to edit the `pom.xml` to add\nthe dependency to our project. Open `pom.xml` and add the following.\n\n```xml\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n```\n\n## Creating your first Test Class\n\n- Now let's create our first Test Class.\n```src/test/java/.../TestLoadKtableFromTopic.java```\nYou can customize this file path post `src/test/java` to however you see fit.\n\n\n- Open the `TestLoadKtableFromTopic.java` file and paste the following content.\n```java\npackage com.ibm.garage.cpat.lab;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KTable;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.processor.StateStore;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueIterator;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.test.junit.QuarkusTest;\n\n/**\n * This is a simple example of loading some reference data from stream into a Ktable for\n * lookup. It uses a persistent state store.\n */\n@QuarkusTest\npublic class TestLoadKtableFromTopic {\n    private static TopologyTestDriver testDriver;\n    private static String companySectorsTopic = \"sector-types\";\n    private static String storeName = \"sector-types-store\";\n\n    private static TestInputTopic<String, String> inTopic;\n    private static TestOutputTopic<String, Long> outTopic;\n    private static TestOutputTopic<String, String> errorTopic;\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab1\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:1234\");\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology(){\n        final StreamsBuilder builder = new StreamsBuilder();\n        // Adding a state store is a simple matter of creating a StoreSupplier\n        // instance with one of the static factory methods on the Stores class.\n        // all persistent StateStore instances provide local storage using RocksDB\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n\n        // A KTable is created from the companySectorsTopic, with key and value deserialized.\n        // With Materialized.as() causing the Table to force a state store materialization (storeSupplier).\n        KTable<String, String> sectorTypeTable = builder.table(companySectorsTopic,\n                Consumed.with(Serdes.String(), Serdes.String()),\n                Materialized.as(storeSupplier));\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(companySectorsTopic, new StringSerializer(), new StringSerializer());\n\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n\n    @Test\n    public void shouldHaveSixSectorTypes(){\n        inTopic.pipeInput(\"C01\",\"Health Care\");\n        inTopic.pipeInput(\"C02\",\"Finance\");\n        inTopic.pipeInput(\"C03\",\"Consumer Services\");\n        inTopic.pipeInput(\"C04\",\"Transportation\");\n        inTopic.pipeInput(\"C05\",\"Capital Goods\");\n        inTopic.pipeInput(\"C06\",\"Public Utilities\");\n\n        KeyValueStore<String,ValueAndTimestamp<String>> store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertNotNull(store);\n\n        ValueAndTimestamp<String> sector = store.get(\"C02\");\n        Assertions.assertNotNull(sector);\n        Assertions.assertEquals(\"Finance\", sector.value());\n        Assertions.assertEquals(6, store.approximateNumEntries());\n\n\n        // demonstrate how to get all the values from the table:\n        KeyValueIterator<String, ValueAndTimestamp<String>> sectors = store.all();\n        while (sectors.hasNext()) {\n            KeyValue<String,ValueAndTimestamp<String>> s = sectors.next();\n            System.out.println(s.key + \":\" + s.value.value());\n        }\n        for ( StateStore s: testDriver.getAllStateStores().values()) {\n            System.out.println(s.name());\n        }\n    }\n}\n```\n\n- What the above code does is it uses TopologyTestDriver to mimic a Topology. A Topology is basically a graph of\nstream processors (nodes) and the edges between these nodes are the streams. In the first sectinon we instantiate\nour `TopologyTestDriver` named `testDriver`, as well as the topic name and store name.\n\n\n\n- Test the application by running the following\n*Note* You might need to comment out the Quarkus Kafka Streams dependency in `pom.xml` as that dependency requires some\nconfigration in a properties file to pass. This is configured in the `Producing and Consuming to a Kafka Topic on Event Streams` section\nfurther down in the lab.\n\n```shell\n./mvnw clean verify\n```\n\n- Depending upon versions of the packages brought in, you may see an initial test failure due to needing to update the `src/main/resources/application.properties` file with the following properties: _(The values are insignifcant for the execution of our tests, but are existence of the property is required by the underlying Quarkus & Kafka Streams integration)_\n```properties\nquarkus.kafka-streams.application-id=my-kafka-streams\nquarkus.kafka-streams.topics=topic1\n```\n\n- How this test topology creation flow works:\n    - A StreamsBuilder object (builder) from the Kafka Streams DSL API is created.\n    - A KeyValueBytesStoreSupplier (storeSupplier) is configured with String variable (storeName).\n    - A KTable is created reading from the topic (companySectorsTopic), deserialized and materialized as\n    the previously create (storeSupplier).\n    - A TopologyTestDriver (testDriver) is built from the provided config properties and the KTable within the builder topology.\n    - Lastly test input topic (inTopic) is created from the testDriver topology.\n    - When `inTopic.pipeInput(\"C01\",\"Health Care\");` is invoked, it populates the topic, which then populates the KTable\n    which ultimately persists in a KeyValue State Store.\n\n- You should see the tests pass. These are three simple tests. The first of which checks that the value fetched from\nthe Kafka Table is not null,the second makes sure that value retrieved from key `C02` is equal to `Finance` and lastly\nwe make sure that the our state store (which was piped by ways of the Kafka Topic) indeed has six key-value pairs.\n\n\n\n## More Robust Kafka Streams Testing\n\n- Now that we have tested some simple functionality by using the Kafka Streams API let's check out some other\noperators that we can use.\n\n- Let's create a new class for our Plain Old Java Object (POJO) named FinancialMessage. You can place this where you want\nbut for simplicity's sake I will use this path. Make sure you remember the path for when you import this class\n`src/main/java/com/ibm/garage/cpat/FinancialMessage/FinancialMessage.java`\n\nNow copy and paste the following content into the newly created file.\n\n```java\npublic class FinancialMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public String exchangeId;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public int institutionId;\n    public int countryId;\n    public boolean technicalValidation;\n\n    public FinancialMessage() {\n\n    }\n\n    public FinancialMessage(String userId, String stockSymbol, String exchangeId,\n                            int quantity, double stockPrice, double totalCost,\n                            int institutionId, int countryId, boolean technicalValidation) {\n\n        this.userId = userId;\n        this.stockSymbol = stockSymbol;\n        this.exchangeId = exchangeId;\n        this.quantity = quantity;\n        this.stockPrice = stockPrice;\n        this.totalCost = totalCost;\n        this.institutionId = institutionId;\n        this.countryId = countryId;\n        this.technicalValidation = technicalValidation;\n    }\n}\n```\n\n**Note** - For brevity and simplicity's sake I did not provide any accessors (getters) or mutators (setters). You can\nset those at your own discretion.\n\n- Now that we have our Java class, let's create a new and separate Java Test class separate from the `TestLoadKtableFromTopic.java`\nfor separation of logic. `src/test/java/.../lab/TestFinancialMessage.java`. Again you may use your own\nfilepath. Copy the contents below.\n\n```java\npackage com.ibm.garage.cpat.lab;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.LongDeserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Windowed;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\nimport io.quarkus.kafka.client.serialization.JsonbSerializer;\nimport io.quarkus.test.junit.QuarkusTest;\n\nimport com.ibm.garage.cpat.Domain.*;\n\n\n@QuarkusTest\npublic class TestFinancialMessage {\n\n    private static TopologyTestDriver testDriver;\n    private static String inTopicName = \"transactions\";\n    private static String outTopicName = \"output\";\n    private static String errorTopicName = \"errors\";\n    private static String storeName = \"transactionCount\";\n    private static TestInputTopic<String, FinancialMessage> inTopic;\n    private static TestOutputTopic<String, Long> outTopic;\n    private static TestOutputTopic<String, String> errorTopic;\n\n    private static final JsonbSerde<FinancialMessage> financialMessageSerde = new JsonbSerde<>(FinancialMessage.class);\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab2\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:2345\");\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde);\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n\n        KStream<String, FinancialMessage> transactionStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        // First verify user id is present, if not route to error\n        KStream<String, FinancialMessage>[] branches =\n                transactionStream.branch(\n                    (key, value) -> value.userId == null,\n                    (key, value) -> true\n                );\n\n        // Handle error by sending to the errors topic.\n        branches[0].map(\n                 (key, value) -> { return KeyValue.pair(key, \"No customer id provided\");}\n                 )\n                .to(\n                    errorTopicName, Produced.with(Serdes.String(), Serdes.String())\n                );\n\n\n        // use groupBy to swap the key, then count by customer id,\n        branches[1].groupBy(\n                    (key, value) -> value.userId\n                )\n                .count(\n                    Materialized.as(storeSupplier)\n                )\n                .toStream()\n                .to(\n                    outTopicName,\n                    Produced.with(Serdes.String(), Serdes.Long())\n            );\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer<FinancialMessage>());\n        //outTopic = testDriver.createOutputTopic(outTopicName,windowedSerde.deserializer(), new LongDeserializer());\n        outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new LongDeserializer());\n        errorTopic = testDriver.createOutputTopic(errorTopicName, new StringDeserializer(), new StringDeserializer());\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n}\n```\n\n- We have the setup for the TestTopology. Now we can add a test that will insert two events into the topic.\n\n```java\n    @Test\n    public void shouldHaveOneTransaction() {\n        // A FinancialMessage is mocked and set to the input topic. Within the Topology,\n        // this gets sent to the outTopic because a userId exists for the incoming message.\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"2\", \"ASDF\", \"HELLO\", 5, 1000.22, 4444.12, 38, 6, true\n        );\n\n        inTopic.pipeInput(\"T01\", mock);\n        inTopic.pipeInput(\"T02\", mock2);\n\n        Assertions.assertFalse(outTopic.isEmpty());\n        Assertions.assertEquals(1, outTopic.readKeyValue().value);\n\n        KeyValueStore<String,ValueAndTimestamp<FinancialMessage>> store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(1, store.approximateNumEntries());\n    }\n```\n\n- The state store (storeSupplier) has two records input. We have a value count when a groupBy is performed. This particular\ntest will fail of course, due to the fact that we inserted two records but our test expects one. To remedy this test we can change\n`Assertions.assertEquals(1, store.approximateNumEntries());` the 1 to a 2.\n\n- Next let's add another very simple test.\n\n```java\n    @Test\n    public void testErrorTopicIsNotEmpty() {\n        FinancialMessage mock = new FinancialMessage(\n            null, \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n\n        inTopic.pipeInput(\"T03\", mock);\n\n        Assertions.assertFalse(errorTopic.isEmpty());\n    }\n}\n```\n\nAs you can see here our message payload is created with `null` for the userId field and the purpose of the test\nis to check if our `errorTopic` is empty. Since our `errorTopic.isEmpty()` resolves to false and our assertion\nis asserting that it is false as well, thus the test passes.\n\n\n- Now that we have two simple tests, let's update our first branch to allow us to filter the stream on a condition\nthat we want. Let's edit our `branches[1]` statement so that it will filter out and retain only the records where\nthe `totalCost` is greater than 5000.\n\n*Note* Since we are changing the logic of how `branches[1]` functions the `shouldHaveOneTransaction` test function will no longer\nfunction as we intended. Comment out the `@Test` annotation or the whole function before proceeding.\n\n```java\nbranches[1].filter(\n            (key, value) -> (value.totalCost > 5000)\n        )\n        .groupBy(\n            (key, value) -> value.userId\n        )\n        .count(\n            Materialized.as(storeSupplier)\n        )\n        .toStream()\n        .to(\n            outTopicName,\n            Produced.with(Serdes.String(), Serdes.Long())\n        );\n```\n\n- Now let's create a test that will insert two records and check if we have two records in the output topic.\n\n```java\n    @Test\n    public void filteredStreamHasTwoRecords() {\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"2\", \"ASDF\", \"HELLO\", 5, 1000.22, 4444.12, 38, 6, true\n        );\n        inTopic.pipeInput(\"T01\", mock);\n        inTopic.pipeInput(\"T02\", mock2);\n\n        KeyValueStore<String,ValueAndTimestamp<FinancialMessage>> store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(2, store.approximateNumEntries());\n    }\n```\n\nThis test fails. Why? The `mock2` message has a `totalCost` of `4444.12` which is less than the filter condition of\n`totalCost` being greater than 5000. To make this test pass change that value to something like `5001`.\n\n\n## Next Steps\n\n- Now that you have finished this initial part of Lab 1 you can optionally proceed to [Lab 2](/use-cases/kafka-streams/lab-2/)\n","frontmatter":{"title":"Kafka Streams Test Lab 1","description":"Using Kafka Streams Test Suite to test Streams"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-streams/lab-1/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}