{"componentChunkName":"component---src-pages-use-cases-overview-pre-requisites-mdx","path":"/use-cases/overview/pre-requisites/","result":{"pageContext":{"frontmatter":{"title":"Common pre-requisites","description":"Common pre-requisites for the different labs and use cases"},"relativePagePath":"/use-cases/overview/pre-requisites.mdx","titleType":"append","MdxNode":{"id":"0c32a2cc-672f-585a-bee3-10df722db658","children":[],"parent":"ba8759b6-67e9-59e6-8559-1263014dc089","internal":{"content":"---\ntitle: Common pre-requisites\ndescription: Common pre-requisites for the different labs and use cases\n---\n\n<AnchorLinks>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Install Event Streams using operators</AnchorLink>\n  <AnchorLink>Log into Event Streams</AnchorLink>\n  <AnchorLink>Create Event Streams Topics</AnchorLink>\n  <AnchorLink>Get Kafka Bootstrap Url</AnchorLink>\n  <AnchorLink>Generate SCRAM Service Credentials</AnchorLink>\n  <AnchorLink>Get Event Streams TLS Certificates</AnchorLink>\n  <AnchorLink>Run the Starter Application</AnchorLink>\n  <AnchorLink>Using Kafdrop</AnchorLink>\n  <AnchorLink>Running docker in kubernetes pod</AnchorLink>\n</AnchorLinks>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Install Event Streams using operators\n\nSee the [product instructions](https://ibm.github.io/event-streams/installing/installing/) to deploy using IBM operators. Here is a summary of the steps using the CLI\n  \n* Verify operators are visibles in OpenShift market place \n\n ```\n oc get catalogsource -n openshift-marketplace\n NAME                   DISPLAY                TYPE      PUBLISHER   AGE\n certified-operators    Certified Operators    grpc      Red Hat     15d\n community-operators    Community Operators    grpc      Red Hat     15d\n ibm-operator-catalog   IBM Operator Catalog   grpc      IBM         6s\n opencloud-operators    IBMCS Operators        grpc      IBM         23h\n redhat-marketplace     Red Hat Marketplace    grpc      Red Hat     15d\n redhat-operators       Red Hat Operators      grpc      Red Hat     15d\n ```\n\n* If the IBM catalogs are not displayed add the following:\n\n ```shell\n oc apply -f \nhttps://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-tools/master/evenstreams-config/ibm-catalog.yaml\n # \n oc apply -f \n https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-tools/master/evenstreams-config/ibm-cs-catalog.yaml\n ```\n\n* Create a project to host eventstreams cluster:\n\n  ```shell\n  oc new-project eventstreams\n  ```\n* Use Openshift console -> Operators > OperatorHub to search for event streams, then install the operator. (The last test done was on 2.1.0 version). We define the kafka cluster operator to manage instances on any projects, which means the operator will be deployed to `openshift-operators`, to use channel version `2.1`. It takes some time before the operator pod is scheduled.\n* Get your license entitlement key from this web site: [https://myibm.ibm.com/products-services/containerlibrary](https://myibm.ibm.com/products-services/containerlibrary), and save it in a file named `key`.\n* Create a secret to access the IBM docker registry to get access to the product image:\n\n  ```shell\n  oc create secret docker-registry ibm-entitlement-key --docker-username=cp --docker-password=$(cat key) --docker-server=\"cp.icr.io\" -n eventstreams\n  ```\n* Install an Event Streams instance: Instances of Event Streams can be created after the Event Streams operator is installed. You can use te OpenShift console or our predefined cluster defintion:\n\n ```shell\n oc apply -f \nhttps://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-tools/master/evenstreams-config/eventstreams-light-insecure.yaml\n ```\n\n## Log into Event Streams\n\nIn this section we are going to see how to log into our IBM Event Streams console both through the UI and CLI.\n\n#### UI\n\nIn order to log into the IBM Event Streams console:\n\n1. go to your OpenShift console and click on `Operators --> Installed Operators` in the left hand side bar menu. Then select the project where your IBM Event Streams instance was installed into.\n\n  ![login1](./images/login1.png)\n\n1. Click on the IBM Event Streams Operator and then on the `Event Streams` option listed at the top bar\n\n  ![login2](./images/login2.png)\n\n1. Click on the IBM Event Streams instance you want to access to its console and scroll down until you see the `Admin UI` attribute that displays the route to this IBM Event Streams instance's console.\n\n  ![login3](./images/login3.png)\n\n1. Click on the route link and enter your IBM Event Streams credentials.\n\n#### CLI\n\nIn order to log into IBM Event Streams console through the CLI, we are going to use the `oc` OpenShift CLI, the `clouctl` Cloud Pak CLI and the `es` Cloud Pak CLI plugin. You can check above in this readme how to get these installed. We assume you are already logged into your OpenShift cluster through the `oc` Openshift CLI (If not, log into your OpenShift cluster console using the user interface and click on `Copy Login Command` option displayed when you click on your user avatar on the top right corner).\n\n1. Get your Cloud Pak console route (you may need cluster wide admin permissions to do so as the Cloud Pak is usually installed in the `ibm-common-services` namespace by the cluster admins)\n\n  ```shell\n  $ oc get routes -n ibm-common-services | grep console\n  cp-console                       cp-console.apps.eda-sandbox-delta.gse-ocp.net                                                  icp-management-ingress           https      reencrypt/Redirect     None\n  ```\n\n1. Log into IBM Event Streams using the Cloud Pak console route from the previous step:\n\n  ```shell\n  $ cloudctl login -a https://cp-console.apps.eda-sandbox-delta.gse-ocp.net --skip-ssl-validation\n\n  Username> user50\n\n  Password>\n  Authenticating...\n  OK\n\n  Targeted account mycluster Account\n\n  Enter a namespace > integration\n  Targeted namespace integration\n\n  Configuring kubectl ...\n  Property \"clusters.mycluster\" unset.\n  Property \"users.mycluster-user\" unset.\n  Property \"contexts.mycluster-context\" unset.\n  Cluster \"mycluster\" set.\n  User \"mycluster-user\" set.\n  Context \"mycluster-context\" created.\n  Switched to context \"mycluster-context\".\n  OK\n\n  Configuring helm: /Users/user/.helm\n  OK\n  ```\n\n1. Initialize the Event Streams CLI plugin (make sure you provide the namespace where your IBM Event Streams instance is installed on as the command will fail if you dont have cluster wide admin permissions)\n\n  ```shell\n  $ cloudctl es init -n integration\n                                                  \n  IBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox-delta.gse-ocp.net   \n  Namespace:                                     integration   \n  Name:                                          kafka   \n  IBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Event Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Event Streams API status:                      OK   \n  Event Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Apicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Event Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443   \n  OK\n  ``` \n\n## Create Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift. The example is to define a topic named INBOUND with 1 partition and a replica set to 3.\n\n#### UI\n\n1. Log into your IBM Event Streams instance through the UI as explained in the previous section.\n\n1. Click on the Topics option on the navigation bar on the left. \n\n1. In the topics page, click on the `Create topic` blue button on the top right corner\n\n  ![Create Topic](./images/create-topic.png)\n\n1. Provide a name for your topic.\n\n  ![Topic Name](./images/inbound-topic-name.png)\n\n1. Leave Partitions at 1.\n\n  ![Partition](./images/partitions.png)\n\n1. Depending on how long you want messages to persist you can change this.\n\n  ![Message Retention](./images/message-retention.png)\n\n1. You can leave Replication Factor at the default 3.\n\n  ![Replication](./images/replicas.png)\n\n1. Click Create.\n\n1. Make sure the topic has been created by navigating to the topics section on the IBM Event Streams user inteface you can find an option for in the left hand side menu bar. \n\n#### CLI\n\n1. Log into your IBM Event Streams instance using the CLI as explained in the previous section.\n\n1. Create the topic with the desirec specification.\n\n   ```shell\n   cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n   ```\n\n1. Make sure the topic has been created by listing the topics.\n\n  ```shell\n  $ cloudctl es topics\n  Topic name   \n  INBOUND   \n  OK\n  ```\n\n## Get Kafka Bootstrap Url\n\nIn this section we are going to see where to find your IBM Event Streams and Kafka bootstrap url/address your applications will need in order to connect to IBM Event Streams to consume messages from and produce messages to.\n\n#### UI\n\nYou can find your IBM Event Streams and Kakfa bootstrap url if you log into the IBM Event Streams user interface, as explained previously in this readme, and click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu where you will see a url to the left of the `Generate SCAM credentials` button. Make sure that you are on the **External Connection**.\n\n![Connect to this Cluster bootstrap](./images/esv10-connect-to-cluster-bootstrap.png)\n\n#### CLI\n\nYou can find your IBM Event Streams and Kakfa bootstrap url when you init the IBM Event Streams Cloud Pak CLI plugin on the last line:\n\n```shell\n$ cloudctl es init -n integration\n                                                  \nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox-delta.gse-ocp.net   \nNamespace:                                     integration   \nName:                                          kafka   \nIBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API status:                      OK   \nEvent Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net   \nApicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443   \nOK\n``` \n  \n## Generate SCRAM Service Credentials\n\nFor an application to connect to an Event Streams instance through the secured external listener, it needs SCRAM credentials to act as service credentials (as well as the TLS certificate we will talk about in the next section of this readme). \n\n#### UI\n\n1. Log into the IBM Event Streams user interface as explained previously in this readme and click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu. Make sure that you are on the **External Connection**.\n\n  ![Connect to this Cluster](./images/esv10-connect-to-cluster.png)\n\n1. Click on the `Generate SCRAM credentials` button.\n\n  ![Generate SCRAM1 ](./images/esv10-generate-scram.png)\n\n1. Introduce a name for your credentials and choose the option that better suits the needs of your applications (this will create RBAC permissions for you credentials so that a service credentials can do only what it needs to do). For this demo, select `Produce messages, consume messages and create topics and schemas` last option.\n\n  ![Generate SCRAM 2](./images/esv10-scram-1.png)\n\n1. Decide whether your service credentials need to have the ability to access all topics or certain topics only. For this demo, select `All Topics` and then click Next.\n\n  ![Generate SCRAM 3](./images/esv10-scram-2.png)\n\n1. Decide whether your service credentials need to have the ability to access all consumer groups or certain specific consumer groups only. For this demo, select `All Consumer Groups` and click Next.\n\n  ![Generate SCRAM 4](./images/esv10-scram-3.png)\n\n1. Decide whether your service credentials need to have the ability to access all transactional IDs or certain specific transactional IDs only. For this demo, select `All transaction IDs` and click on `Generate credentials`.\n\n  ![Generate SCRAM 5](./images/esv10-scram-4.png)\n\n1. **Take note** of the set of credentials displayed on screen. You will need to provide your applications with these in order to get authenticated and authorized with your IBM Event Streams instance.\n\n  ![Generate SCRAM 6](./images/esv10-scram-5.png)\n\n1. If you did not take note of your SCRAM credentials or you forgot these, the above will create a `KafkaUser` object in OpenShift that is interpreted by the IBM Event Streams Operator. You can see this `KafkaUser` if you go to the OpenShift console, click on `Operators --> Installed Operators` on the right hand side menu, then click on the `IBM Event Streams` operator and finally click on `Kafka Users` at the top bar menu.\n\n  ![Retrieve SCRAM 1](./images/esv10-retrieve-scram-1.png)\n\n1. If you click on your `Kafka User`, you will see what is the Kubernetes secret behind holding your SCRAM credentials details.\n\n  ![Retrieve SCRAM 2](./images/esv10-retrieve-scram-2.png)\n\n1. Click on that secret and you will be able to see again your `SCRAM password` (your `SCRAM username` is the same name as the `Kafka User` created or the secret holding your `SCRAM password`) \n\n  ![Retrieve SCRAM 3](./images/esv10-retrieve-scram-3.png)\n\n\n#### CLI\n\n1. Log into your IBM Event Streams instance through the CLI as already explained before in this readme.\n\n1. Create your SCRAM service credentials with the following command (adjust the topics, consumer groups, transaction IDs, etc permissions your SCRAM service credentials should have in order to satisfy your application requirements):\n\n  ```shell\n  $ cloudctl es kafka-user-create --name test-credentials-cli --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n  \n  KafkaUser name         Authentication   Authorization   Username                                                Secret   \n  test-credentials-cli   scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret   \n  \n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  \n  Created KafkaUser test-credentials-cli.\n  OK\n  ```\n\n1. List the `KafkaUser` objects to make sure yours has been created:\n\n  ```shell\n  $ cloudctl es kafka-users\n  KafkaUser name                    Authentication   Authorization   \n  test-credentials                  scram-sha-512    simple   \n  test-credentials-cli              scram-sha-512    simple   \n  OK\n  ```\n\n1. To retrieve your credentials execute the following command:\n\n  ```shell\n  $ cloudctl es kafka-user test-credentials-cli \n  KafkaUser name         Authentication   Authorization   Username               Secret   \n  test-credentials-cli   scram-sha-512    simple          test-credentials-cli   test-credentials-cli   \n\n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  OK\n  ```\n\n1. Above you can see your `SCRAM username` under `Username` and the secret holding your `SCRAM password` under `Secret`. In order to retrieve the password, execute the following command:\n\n  ```shell\n  $ oc get secret test-credentials-cli -o jsonpath='{.data.password}' | base64 --decode \n  \n  *******\n  ```\n\n**NEXT:** For more information about how to connect to your cluste, read the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/getting-started/connecting/)\n\n## Get Event Streams TLS Certificates\n\nIn this section we are going to see how to download the TLS certificats to securely connect to our IBM Event Streams instance.\n\n#### UI\n\n1. Log into the IBM Event Streams console user interface as explained before in this readme.\n\n1. Click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu where you will see a `Certificates` section:\n\n  ![PKCS12 Truststore](./images/esv10-pkcs12.png)\n\n1. Depending on what language your application is written into, you will need a `PKCS12 certificate` or a `PEM certificate`. Click on `Download certificate` for any of the options you need. If it is the `PKCS12 certificate` bear in mind it comes with a password for the truststore. You don't need to write this down as it will display any time you click on `Download certificate` button.\n\n#### CLI\n\n1. Log into IBM Event Streams through the CLI as already explained before in this readme.\n\n1. To retrieve the `PKCS12 certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format p12\n  Trustore password is ********\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.p12.\n  OK\n  ```\n\n1. To retrieve the `PEM certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format pem\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.pem.\n  OK\n  ```\n\n**NEXT:** For more information about how to connect to your cluste, read the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/getting-started/connecting/)\n\n## Run the Starter Application\n\nThis section details walking through the generation of a starter application for usage with IBM Event Streams, as documented in the [official product documentation](https://ibm.github.io/event-streams/getting-started/generating-starter-app/).\n\n1. Log into the IBM Event Streams Dashboard.\n\n  ![Monitoring1](./images/monitoring1.png)\n\n1. Click the **Try the starter application** button from the _Getting Started_ page\n\n  ![Monitoring2](./images/monitoring2.png)\n\n1. Click **Download JAR from GitHub**. This will open a new window to <https://github.com/ibm-messaging/kafka-java-vertx-starter/releases>\n  \n  ![Monitoring3](./images/monitoring3.png)\n\n   - Click the link for `demo-all.jar` from the latest release available. At the time of this writing, the latest version was `1.0.0`.\n  \n   ![Monitoring4](./images/monitoring4.png)\n\n1. Return to the `Configure & run starter application` window and click **Generate properties**.\n\n  ![Monitoring5](./images/monitoring5.png)\n\n1. In dialog that pops up from the right-hand side of the screen, enter the following information:\n   - **Starter application name:** `monitoring-lab-[your-initials]`\n   - Leave **New topic** selected and enter a **Topic name** of `monitoring-lab-topic-[your-initials]`.\n   - Click **Generate and download .zip**\n  \n   ![Monitoring6](./images/monitoring6.png)\n\n1. In a Terminal window, unzip the generated ZIP file from the previous window and move `demo-all.jar` file into the same folder.\n\n1. Review the extracted `kafka.properties` to understand how Event Streams has generated credentials and configuration information for this sample application to connect.\n\n1. Run the command `java -Dproperties_path=./kafka.properties -jar demo-all.jar`.\n\n1.  Wait until you see the string `Application started in X ms` in the output and then visit the application's user interface via `http://localhost:8080`.\n\n  ![Monitoring7](./images/monitoring7.png)\n\n1. Once in the User Interface, enter a message to be contained for the Kafka record value then click **Start producing**.\n\n1. Wait a few moments until the UI updates to show some of the confirmed produced messages and offsets, then click on **Start consuming** on the right side of the application.\n\n  ![Monitoring8](./images/monitoring8.png)\n\n1. In the IBM Event Streams user interface, go to the topic where you send the messages to and make sure messages have actually made it.\n\n  ![Monitoring9](./images/monitoring9.png)\n\n1. You can do the following actions on the application\n   - If you would like to stop the application from producing, you can click **Stop producing**.\n   - If you would like to stop the application from consuming, you can click **Stop consuming**.\n   - If you would like to stop the application entirely, you can input `Control+C` in the Terminal session where the application is running.\n\nAn [alternative sample application](https://ibm.github.io/event-streams/getting-started/testing-loads/) can be leveraged from the official documentation to generate higher amounts of load.\n\n## Using Kafdrop\n\n[Kafdrop](https://github.com/obsidiandynamics/kafdrop) is a web UI for viewing Kafka topics and browsing consumer groups. It is very helpful for development purpose. \n\nHere are scripts that can be useful to start a local Kafdrop webserver.\n\n```shell\nsource .env\nsed 's/KAFKA_USER/'$KAFKA_USER'/g' ./scripts/kafka.properties > ./scripts/output.properties\nsed -i '' 's/KAFKA_PASSWORD/'$KAFKA_PASSWORD'/g' ./scripts/output.properties\nsed -i '' 's/KAFKA_CERT_PWD/'$KAFKA_CERT_PWD'/g' ./scripts/output.properties\ndocker run -d --rm -p 9000:9000 \\\n    --name kafdrop \\\n    -v $(pwd)/certs:/home/certs \\\n    -e KAFKA_BROKERCONNECT=$KAFKA_BROKERS \\\n    -e KAFKA_PROPERTIES=$(cat ./scripts/output.properties | base64) \\\n    -e JVM_OPTS=\"-Xms32M -Xmx64M\" \\\n    -e SERVER_SERVLET_CONTEXTPATH=\"/\" \\\n    obsidiandynamics/kafdrop\n```\n\n[See also those scripts: startKafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/startKafdrop.sh)\n\nand [to stop kafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/stopKafdrop.sh)\n\n```shell\ndocker stop $(docker stop kafdrop)\n```\n\nThe Web console is at [http://localhost:9000/](http://localhost:9000/)\n\n## Running docker in kubernetes pod\n\nIf you need to run some of the lab within a shell session with docker, you can create a pod, using OpenShift console under one of your project:\n\n ![1](./images/create-pod.png)\n\nIn the yaml editor, copy [this yaml file](https://gist.githubusercontent.com/osowski/4c9b6eb8e63b93e97ad6fecd8d9c8ff4/raw/8c2720c572458838942ccdba87271c61d4764671/dind.yaml). This operation will download the docker image from dockerhub ibmcase account.\n\nYou can also download the yaml and do an `oc apply -f dind.yaml`.\n\nThen remote exec a shell within the pod: `oc exec -it dind bash`. You should be able to run another docker image in this pod, like for example our python environment.\n","type":"Mdx","contentDigest":"d3ebfe9831a51c4e36a4e73770b6279e","owner":"gatsby-plugin-mdx","counter":727},"frontmatter":{"title":"Common pre-requisites","description":"Common pre-requisites for the different labs and use cases"},"exports":{},"rawBody":"---\ntitle: Common pre-requisites\ndescription: Common pre-requisites for the different labs and use cases\n---\n\n<AnchorLinks>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Install Event Streams using operators</AnchorLink>\n  <AnchorLink>Log into Event Streams</AnchorLink>\n  <AnchorLink>Create Event Streams Topics</AnchorLink>\n  <AnchorLink>Get Kafka Bootstrap Url</AnchorLink>\n  <AnchorLink>Generate SCRAM Service Credentials</AnchorLink>\n  <AnchorLink>Get Event Streams TLS Certificates</AnchorLink>\n  <AnchorLink>Run the Starter Application</AnchorLink>\n  <AnchorLink>Using Kafdrop</AnchorLink>\n  <AnchorLink>Running docker in kubernetes pod</AnchorLink>\n</AnchorLinks>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Install Event Streams using operators\n\nSee the [product instructions](https://ibm.github.io/event-streams/installing/installing/) to deploy using IBM operators. Here is a summary of the steps using the CLI\n  \n* Verify operators are visibles in OpenShift market place \n\n ```\n oc get catalogsource -n openshift-marketplace\n NAME                   DISPLAY                TYPE      PUBLISHER   AGE\n certified-operators    Certified Operators    grpc      Red Hat     15d\n community-operators    Community Operators    grpc      Red Hat     15d\n ibm-operator-catalog   IBM Operator Catalog   grpc      IBM         6s\n opencloud-operators    IBMCS Operators        grpc      IBM         23h\n redhat-marketplace     Red Hat Marketplace    grpc      Red Hat     15d\n redhat-operators       Red Hat Operators      grpc      Red Hat     15d\n ```\n\n* If the IBM catalogs are not displayed add the following:\n\n ```shell\n oc apply -f \nhttps://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-tools/master/evenstreams-config/ibm-catalog.yaml\n # \n oc apply -f \n https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-tools/master/evenstreams-config/ibm-cs-catalog.yaml\n ```\n\n* Create a project to host eventstreams cluster:\n\n  ```shell\n  oc new-project eventstreams\n  ```\n* Use Openshift console -> Operators > OperatorHub to search for event streams, then install the operator. (The last test done was on 2.1.0 version). We define the kafka cluster operator to manage instances on any projects, which means the operator will be deployed to `openshift-operators`, to use channel version `2.1`. It takes some time before the operator pod is scheduled.\n* Get your license entitlement key from this web site: [https://myibm.ibm.com/products-services/containerlibrary](https://myibm.ibm.com/products-services/containerlibrary), and save it in a file named `key`.\n* Create a secret to access the IBM docker registry to get access to the product image:\n\n  ```shell\n  oc create secret docker-registry ibm-entitlement-key --docker-username=cp --docker-password=$(cat key) --docker-server=\"cp.icr.io\" -n eventstreams\n  ```\n* Install an Event Streams instance: Instances of Event Streams can be created after the Event Streams operator is installed. You can use te OpenShift console or our predefined cluster defintion:\n\n ```shell\n oc apply -f \nhttps://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-tools/master/evenstreams-config/eventstreams-light-insecure.yaml\n ```\n\n## Log into Event Streams\n\nIn this section we are going to see how to log into our IBM Event Streams console both through the UI and CLI.\n\n#### UI\n\nIn order to log into the IBM Event Streams console:\n\n1. go to your OpenShift console and click on `Operators --> Installed Operators` in the left hand side bar menu. Then select the project where your IBM Event Streams instance was installed into.\n\n  ![login1](./images/login1.png)\n\n1. Click on the IBM Event Streams Operator and then on the `Event Streams` option listed at the top bar\n\n  ![login2](./images/login2.png)\n\n1. Click on the IBM Event Streams instance you want to access to its console and scroll down until you see the `Admin UI` attribute that displays the route to this IBM Event Streams instance's console.\n\n  ![login3](./images/login3.png)\n\n1. Click on the route link and enter your IBM Event Streams credentials.\n\n#### CLI\n\nIn order to log into IBM Event Streams console through the CLI, we are going to use the `oc` OpenShift CLI, the `clouctl` Cloud Pak CLI and the `es` Cloud Pak CLI plugin. You can check above in this readme how to get these installed. We assume you are already logged into your OpenShift cluster through the `oc` Openshift CLI (If not, log into your OpenShift cluster console using the user interface and click on `Copy Login Command` option displayed when you click on your user avatar on the top right corner).\n\n1. Get your Cloud Pak console route (you may need cluster wide admin permissions to do so as the Cloud Pak is usually installed in the `ibm-common-services` namespace by the cluster admins)\n\n  ```shell\n  $ oc get routes -n ibm-common-services | grep console\n  cp-console                       cp-console.apps.eda-sandbox-delta.gse-ocp.net                                                  icp-management-ingress           https      reencrypt/Redirect     None\n  ```\n\n1. Log into IBM Event Streams using the Cloud Pak console route from the previous step:\n\n  ```shell\n  $ cloudctl login -a https://cp-console.apps.eda-sandbox-delta.gse-ocp.net --skip-ssl-validation\n\n  Username> user50\n\n  Password>\n  Authenticating...\n  OK\n\n  Targeted account mycluster Account\n\n  Enter a namespace > integration\n  Targeted namespace integration\n\n  Configuring kubectl ...\n  Property \"clusters.mycluster\" unset.\n  Property \"users.mycluster-user\" unset.\n  Property \"contexts.mycluster-context\" unset.\n  Cluster \"mycluster\" set.\n  User \"mycluster-user\" set.\n  Context \"mycluster-context\" created.\n  Switched to context \"mycluster-context\".\n  OK\n\n  Configuring helm: /Users/user/.helm\n  OK\n  ```\n\n1. Initialize the Event Streams CLI plugin (make sure you provide the namespace where your IBM Event Streams instance is installed on as the command will fail if you dont have cluster wide admin permissions)\n\n  ```shell\n  $ cloudctl es init -n integration\n                                                  \n  IBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox-delta.gse-ocp.net   \n  Namespace:                                     integration   \n  Name:                                          kafka   \n  IBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Event Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Event Streams API status:                      OK   \n  Event Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Apicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \n  Event Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443   \n  OK\n  ``` \n\n## Create Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift. The example is to define a topic named INBOUND with 1 partition and a replica set to 3.\n\n#### UI\n\n1. Log into your IBM Event Streams instance through the UI as explained in the previous section.\n\n1. Click on the Topics option on the navigation bar on the left. \n\n1. In the topics page, click on the `Create topic` blue button on the top right corner\n\n  ![Create Topic](./images/create-topic.png)\n\n1. Provide a name for your topic.\n\n  ![Topic Name](./images/inbound-topic-name.png)\n\n1. Leave Partitions at 1.\n\n  ![Partition](./images/partitions.png)\n\n1. Depending on how long you want messages to persist you can change this.\n\n  ![Message Retention](./images/message-retention.png)\n\n1. You can leave Replication Factor at the default 3.\n\n  ![Replication](./images/replicas.png)\n\n1. Click Create.\n\n1. Make sure the topic has been created by navigating to the topics section on the IBM Event Streams user inteface you can find an option for in the left hand side menu bar. \n\n#### CLI\n\n1. Log into your IBM Event Streams instance using the CLI as explained in the previous section.\n\n1. Create the topic with the desirec specification.\n\n   ```shell\n   cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n   ```\n\n1. Make sure the topic has been created by listing the topics.\n\n  ```shell\n  $ cloudctl es topics\n  Topic name   \n  INBOUND   \n  OK\n  ```\n\n## Get Kafka Bootstrap Url\n\nIn this section we are going to see where to find your IBM Event Streams and Kafka bootstrap url/address your applications will need in order to connect to IBM Event Streams to consume messages from and produce messages to.\n\n#### UI\n\nYou can find your IBM Event Streams and Kakfa bootstrap url if you log into the IBM Event Streams user interface, as explained previously in this readme, and click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu where you will see a url to the left of the `Generate SCAM credentials` button. Make sure that you are on the **External Connection**.\n\n![Connect to this Cluster bootstrap](./images/esv10-connect-to-cluster-bootstrap.png)\n\n#### CLI\n\nYou can find your IBM Event Streams and Kakfa bootstrap url when you init the IBM Event Streams Cloud Pak CLI plugin on the last line:\n\n```shell\n$ cloudctl es init -n integration\n                                                  \nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox-delta.gse-ocp.net   \nNamespace:                                     integration   \nName:                                          kafka   \nIBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API status:                      OK   \nEvent Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net   \nApicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443   \nOK\n``` \n  \n## Generate SCRAM Service Credentials\n\nFor an application to connect to an Event Streams instance through the secured external listener, it needs SCRAM credentials to act as service credentials (as well as the TLS certificate we will talk about in the next section of this readme). \n\n#### UI\n\n1. Log into the IBM Event Streams user interface as explained previously in this readme and click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu. Make sure that you are on the **External Connection**.\n\n  ![Connect to this Cluster](./images/esv10-connect-to-cluster.png)\n\n1. Click on the `Generate SCRAM credentials` button.\n\n  ![Generate SCRAM1 ](./images/esv10-generate-scram.png)\n\n1. Introduce a name for your credentials and choose the option that better suits the needs of your applications (this will create RBAC permissions for you credentials so that a service credentials can do only what it needs to do). For this demo, select `Produce messages, consume messages and create topics and schemas` last option.\n\n  ![Generate SCRAM 2](./images/esv10-scram-1.png)\n\n1. Decide whether your service credentials need to have the ability to access all topics or certain topics only. For this demo, select `All Topics` and then click Next.\n\n  ![Generate SCRAM 3](./images/esv10-scram-2.png)\n\n1. Decide whether your service credentials need to have the ability to access all consumer groups or certain specific consumer groups only. For this demo, select `All Consumer Groups` and click Next.\n\n  ![Generate SCRAM 4](./images/esv10-scram-3.png)\n\n1. Decide whether your service credentials need to have the ability to access all transactional IDs or certain specific transactional IDs only. For this demo, select `All transaction IDs` and click on `Generate credentials`.\n\n  ![Generate SCRAM 5](./images/esv10-scram-4.png)\n\n1. **Take note** of the set of credentials displayed on screen. You will need to provide your applications with these in order to get authenticated and authorized with your IBM Event Streams instance.\n\n  ![Generate SCRAM 6](./images/esv10-scram-5.png)\n\n1. If you did not take note of your SCRAM credentials or you forgot these, the above will create a `KafkaUser` object in OpenShift that is interpreted by the IBM Event Streams Operator. You can see this `KafkaUser` if you go to the OpenShift console, click on `Operators --> Installed Operators` on the right hand side menu, then click on the `IBM Event Streams` operator and finally click on `Kafka Users` at the top bar menu.\n\n  ![Retrieve SCRAM 1](./images/esv10-retrieve-scram-1.png)\n\n1. If you click on your `Kafka User`, you will see what is the Kubernetes secret behind holding your SCRAM credentials details.\n\n  ![Retrieve SCRAM 2](./images/esv10-retrieve-scram-2.png)\n\n1. Click on that secret and you will be able to see again your `SCRAM password` (your `SCRAM username` is the same name as the `Kafka User` created or the secret holding your `SCRAM password`) \n\n  ![Retrieve SCRAM 3](./images/esv10-retrieve-scram-3.png)\n\n\n#### CLI\n\n1. Log into your IBM Event Streams instance through the CLI as already explained before in this readme.\n\n1. Create your SCRAM service credentials with the following command (adjust the topics, consumer groups, transaction IDs, etc permissions your SCRAM service credentials should have in order to satisfy your application requirements):\n\n  ```shell\n  $ cloudctl es kafka-user-create --name test-credentials-cli --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n  \n  KafkaUser name         Authentication   Authorization   Username                                                Secret   \n  test-credentials-cli   scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret   \n  \n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  \n  Created KafkaUser test-credentials-cli.\n  OK\n  ```\n\n1. List the `KafkaUser` objects to make sure yours has been created:\n\n  ```shell\n  $ cloudctl es kafka-users\n  KafkaUser name                    Authentication   Authorization   \n  test-credentials                  scram-sha-512    simple   \n  test-credentials-cli              scram-sha-512    simple   \n  OK\n  ```\n\n1. To retrieve your credentials execute the following command:\n\n  ```shell\n  $ cloudctl es kafka-user test-credentials-cli \n  KafkaUser name         Authentication   Authorization   Username               Secret   \n  test-credentials-cli   scram-sha-512    simple          test-credentials-cli   test-credentials-cli   \n\n  Resource type     Name        Pattern type   Host   Operation   \n  topic             *           literal        *      Read   \n  topic             __schema_   prefix         *      Read   \n  topic             *           literal        *      Write   \n  topic             *           literal        *      Create   \n  topic             __schema_   prefix         *      Alter   \n  group             *           literal        *      Read   \n  transactionalId   *           literal        *      Write   \n  OK\n  ```\n\n1. Above you can see your `SCRAM username` under `Username` and the secret holding your `SCRAM password` under `Secret`. In order to retrieve the password, execute the following command:\n\n  ```shell\n  $ oc get secret test-credentials-cli -o jsonpath='{.data.password}' | base64 --decode \n  \n  *******\n  ```\n\n**NEXT:** For more information about how to connect to your cluste, read the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/getting-started/connecting/)\n\n## Get Event Streams TLS Certificates\n\nIn this section we are going to see how to download the TLS certificats to securely connect to our IBM Event Streams instance.\n\n#### UI\n\n1. Log into the IBM Event Streams console user interface as explained before in this readme.\n\n1. Click on the `Connect to this cluster` option displayed on the dashboard. This will display a menu where you will see a `Certificates` section:\n\n  ![PKCS12 Truststore](./images/esv10-pkcs12.png)\n\n1. Depending on what language your application is written into, you will need a `PKCS12 certificate` or a `PEM certificate`. Click on `Download certificate` for any of the options you need. If it is the `PKCS12 certificate` bear in mind it comes with a password for the truststore. You don't need to write this down as it will display any time you click on `Download certificate` button.\n\n#### CLI\n\n1. Log into IBM Event Streams through the CLI as already explained before in this readme.\n\n1. To retrieve the `PKCS12 certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format p12\n  Trustore password is ********\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.p12.\n  OK\n  ```\n\n1. To retrieve the `PEM certificate` execute the following command:\n\n  ```shell\n  $ cloudctl es certificates --format pem\n  Certificate successfully written to /Users/testUser/Downloads/es-cert.pem.\n  OK\n  ```\n\n**NEXT:** For more information about how to connect to your cluste, read the [IBM Event Streams product documentation](https://ibm.github.io/event-streams/getting-started/connecting/)\n\n## Run the Starter Application\n\nThis section details walking through the generation of a starter application for usage with IBM Event Streams, as documented in the [official product documentation](https://ibm.github.io/event-streams/getting-started/generating-starter-app/).\n\n1. Log into the IBM Event Streams Dashboard.\n\n  ![Monitoring1](./images/monitoring1.png)\n\n1. Click the **Try the starter application** button from the _Getting Started_ page\n\n  ![Monitoring2](./images/monitoring2.png)\n\n1. Click **Download JAR from GitHub**. This will open a new window to <https://github.com/ibm-messaging/kafka-java-vertx-starter/releases>\n  \n  ![Monitoring3](./images/monitoring3.png)\n\n   - Click the link for `demo-all.jar` from the latest release available. At the time of this writing, the latest version was `1.0.0`.\n  \n   ![Monitoring4](./images/monitoring4.png)\n\n1. Return to the `Configure & run starter application` window and click **Generate properties**.\n\n  ![Monitoring5](./images/monitoring5.png)\n\n1. In dialog that pops up from the right-hand side of the screen, enter the following information:\n   - **Starter application name:** `monitoring-lab-[your-initials]`\n   - Leave **New topic** selected and enter a **Topic name** of `monitoring-lab-topic-[your-initials]`.\n   - Click **Generate and download .zip**\n  \n   ![Monitoring6](./images/monitoring6.png)\n\n1. In a Terminal window, unzip the generated ZIP file from the previous window and move `demo-all.jar` file into the same folder.\n\n1. Review the extracted `kafka.properties` to understand how Event Streams has generated credentials and configuration information for this sample application to connect.\n\n1. Run the command `java -Dproperties_path=./kafka.properties -jar demo-all.jar`.\n\n1.  Wait until you see the string `Application started in X ms` in the output and then visit the application's user interface via `http://localhost:8080`.\n\n  ![Monitoring7](./images/monitoring7.png)\n\n1. Once in the User Interface, enter a message to be contained for the Kafka record value then click **Start producing**.\n\n1. Wait a few moments until the UI updates to show some of the confirmed produced messages and offsets, then click on **Start consuming** on the right side of the application.\n\n  ![Monitoring8](./images/monitoring8.png)\n\n1. In the IBM Event Streams user interface, go to the topic where you send the messages to and make sure messages have actually made it.\n\n  ![Monitoring9](./images/monitoring9.png)\n\n1. You can do the following actions on the application\n   - If you would like to stop the application from producing, you can click **Stop producing**.\n   - If you would like to stop the application from consuming, you can click **Stop consuming**.\n   - If you would like to stop the application entirely, you can input `Control+C` in the Terminal session where the application is running.\n\nAn [alternative sample application](https://ibm.github.io/event-streams/getting-started/testing-loads/) can be leveraged from the official documentation to generate higher amounts of load.\n\n## Using Kafdrop\n\n[Kafdrop](https://github.com/obsidiandynamics/kafdrop) is a web UI for viewing Kafka topics and browsing consumer groups. It is very helpful for development purpose. \n\nHere are scripts that can be useful to start a local Kafdrop webserver.\n\n```shell\nsource .env\nsed 's/KAFKA_USER/'$KAFKA_USER'/g' ./scripts/kafka.properties > ./scripts/output.properties\nsed -i '' 's/KAFKA_PASSWORD/'$KAFKA_PASSWORD'/g' ./scripts/output.properties\nsed -i '' 's/KAFKA_CERT_PWD/'$KAFKA_CERT_PWD'/g' ./scripts/output.properties\ndocker run -d --rm -p 9000:9000 \\\n    --name kafdrop \\\n    -v $(pwd)/certs:/home/certs \\\n    -e KAFKA_BROKERCONNECT=$KAFKA_BROKERS \\\n    -e KAFKA_PROPERTIES=$(cat ./scripts/output.properties | base64) \\\n    -e JVM_OPTS=\"-Xms32M -Xmx64M\" \\\n    -e SERVER_SERVLET_CONTEXTPATH=\"/\" \\\n    obsidiandynamics/kafdrop\n```\n\n[See also those scripts: startKafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/startKafdrop.sh)\n\nand [to stop kafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/stopKafdrop.sh)\n\n```shell\ndocker stop $(docker stop kafdrop)\n```\n\nThe Web console is at [http://localhost:9000/](http://localhost:9000/)\n\n## Running docker in kubernetes pod\n\nIf you need to run some of the lab within a shell session with docker, you can create a pod, using OpenShift console under one of your project:\n\n ![1](./images/create-pod.png)\n\nIn the yaml editor, copy [this yaml file](https://gist.githubusercontent.com/osowski/4c9b6eb8e63b93e97ad6fecd8d9c8ff4/raw/8c2720c572458838942ccdba87271c61d4764671/dind.yaml). This operation will download the docker image from dockerhub ibmcase account.\n\nYou can also download the yaml and do an `oc apply -f dind.yaml`.\n\nThen remote exec a shell within the pod: `oc exec -it dind bash`. You should be able to run another docker image in this pod, like for example our python environment.\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/overview/pre-requisites.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}