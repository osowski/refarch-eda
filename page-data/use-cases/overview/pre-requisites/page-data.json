{"componentChunkName":"component---src-pages-use-cases-overview-pre-requisites-mdx","path":"/use-cases/overview/pre-requisites/","result":{"pageContext":{"frontmatter":{"title":"Common pre-requisites","description":"Common pre-requisites for the different labs and use cases"},"relativePagePath":"/use-cases/overview/pre-requisites.mdx","titleType":"append","MdxNode":{"id":"0c32a2cc-672f-585a-bee3-10df722db658","children":[],"parent":"ba8759b6-67e9-59e6-8559-1263014dc089","internal":{"content":"---\ntitle: Common pre-requisites\ndescription: Common pre-requisites for the different labs and use cases\n---\n\n<AnchorLinks>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Creating Event Streams Topics</AnchorLink>\n  <AnchorLink>Getting scram authentication from Event Streams on OpenShift</AnchorLink>\n  <AnchorLink>Getting TLS authentication from Event Streams on OpenShift</AnchorLink>\n  <AnchorLink>Using Kafdrop</AnchorLink>\n</AnchorLinks>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Creating Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift using the User Interface. The example is to define a topic named INBOUND with 1 partition and a replica set to 3. Change to any other topic you want to create.\n\n- Get the route for event streams. We assume the namespace is `eventstreams`\n \n ```shell\n  oc get routes -n eventstreams | grep ui\n  ```\n\n- Navigate to the Event Streams Console using the exposed route URL.\n\n- Click the Topics option on the navigation bar on the left. Create the INBOUND topic.\n\n![Create Topic](./images/create-topic.png)\n\n![Topic Name](./images/inbound-topic-name.png)\n\n- Leave Partitions at 1.\n\n![Partition](./images/partitions.png)\n\n- Depending on how long you want messages to persist you can change this.\n\n![Message Retention](./images/message-retention.png)\n\n- You can leave Replication Factor at the default 3.\n\n![Replication](./images/replicas.png)\n\n- Click Create.\n\n### Create topic using CLI\n\nYou can do the samething using CLI, [see the product documentation](https://ibm.github.io/event-streams/getting-started/creating-topics/).\n\n ```shell\n cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n ```\n\n## Getting scram authentication from Event Streams on OpenShift\n\n- To connect to an Event Streams Instance from an application running out side of the OpenShift cluster, we need to get external access credential. This is done by getting a scram user and TLS server certificate. \n\n- While viewing our Event Streams Instance, navigate to the 'Connect to this cluster' menu from the left. Click Connect to this Cluster - \n\n- Make sure that you're on **External Connection** as we will need to test/connect from an application not running on OCP cluster. Keep note of your **Bootstrap Server Address**. Save this somewhere as we will need this later to configure our application's connection to the Event Streams instance.\n\n![Connect to this Cluster](./images/esv10-connect-to-cluster.png)\n\n- Generate your **SCRAM Credentials**. Click the `Generate SCRAM Credentials` button.\n\n![Generate SCRAM1 ](./images/esv10-generate-scram.png)\n\n- Select a name for your secret and make sure you remember it as this also represents the user identifier for the connection. Also choose the Produce, Consume, Create Topics and Schema Option.\n\n![Generate SCRAM 2](./images/esv10-scram-1.png)\n\n- Select `All Topics` and then click Next.\n\n![Generate SCRAM 3](./images/esv10-scram-2.png)\n\n- Leave it with `All Consumer Groups` and click Next.\n\n![Generate SCRAM 4](./images/esv10-scram-3.png)\n\n- Select all transaction IDs and then generate your **API Key**. Click the Generate API Key button.\n\n![Generate SCRAM 5](./images/esv10-scram-4.png)\n\n- Select a name for your application. It doesn't really matter too much what you name it. \n\n- If the following error occurs, log into your OpenShift cluster and go to the project/namespace that your Event Streams v10 instance is installed in. If you run these commands you will see your user created.\n\n`oc get kafkauser`\n\n`oc get secrets`\n\n- Your `SCRAM Username` that we will need for later is the name of your secret. The `SCRAM Password` can be either obtained by going to your OpenShift Console Web UI and revealing the password through the secrets menu or through the CLI. Run the following commands to see the value of the password.\n\n`oc get secret your-secret -jsonpath '{.data.password}' -n eventstreams | base64 --decode `\n\n- This will return you a base64 encoded value. Decode this like so\n\n`echo \"dWpPN3ZSSEF1clRK\" | base64 -d`\n\n![SCRAM Secret](./images/esv10-scram-secret.png)\n\n\n- Lastly download the PKCS12 truststore .pkcs12 certificate. Once you hit the Download button the password to the truststore will be revealed. **Copy this down** as we will need it.\n\n![PKCS12 Truststore](./images/esv10-pkcs12.png)\n\n\n**Summary** \n\nWe now have the bootstrap server address, SCRAM Username (also the name of the KafkaUser/Secret CRs) and Password, .pcks12 truststore certificate, and the truststore password associated with that certificate to allow us the ability to connect to our Event Streams instance.\n\n\n## Getting TLS authentication from Event Streams on OpenShift\n\n### Get TLS server public certificate\n\nThe cluster public certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. We can also get those certificate and password via CLI:\n\nSee [product documentation for detail](https://ibm.github.io/event-streams/getting-started/connecting/). A quick set of commands:\n\n ```shell\n oc get secret -n eventstreams | grep cluster-ca-cert\n # use the ca-cert for the instance you target \n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams -o jsonpath='{.data.ca\\.p12}' | base64 --decode > es-cert.p12\n # or the command:\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.p12 -n eventstreams\n # and password\n oc get secret minimal-prod-cluster-ca-cert -n eventstreams -o jsonpath='{.data.ca\\.password}' | base64 --decode\n # or the command\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.password -n eventstreams\n ```\n\nCopy the server certificate into your OpenShift project where consumer and producer code will be deployed:\n\n```shell\n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f - \n```\n\n### Get Scram-sha-512 user \n\nSelect one of the Kafka users with Scram authentication defined or create a new one with the produce, consume messages and create topic and schemas authorizations, on all topics or topic with a specific prefix, on all consumer groups or again with a specific prefix, all transaction IDs.\n\n  ```shell\n  # if not logged yes to your openshift cluster where the docker private registry resides do:\n  oc login --token=... --server=https://c...\n  oc get kafkausers -n eventstreams\n\n  NAME                                     AUTHENTICATION   AUTHORIZATION\n  demo-app                                 scram-sha-512    simple\n  ```\n\n  We use a user with scram-sha-512 authentication named: `demo-app`\n\n* Copy user's secret to the current project where the application will run\n\n ```shell\n oc get secret  demo-app -n eventstreams --export -o yaml | oc apply -f -\n ```\n\n* Get the user password:\n\n ```shell\n oc get secret demo-app -n eventstreams -o jsonpath='{.data.password}' | base64 --decode\n ```\n\n* Set the KAFKA_USER and KAFKA_PASSWORD, and SECURE_PROTOCOL=SASL_SSL environment variables for your application.\n\n### Get TLS user \n\nIf you want to use mutual authentication with client certificate, you need a tls user. And use the same commands as in previous section to get user name and user's password. The differences are in the password access and that you need to download client certificate as keystore to be integrated in the Java app.\n\n ```shell\n # password is in another key\n oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.password}' | base64 --decode  \n ```\n\n* Get the user client certificate and password\n\n  ```shell\n  cd certs\n  oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.p12}' | base64 --decode > user.p12\n  ```\n\nModify USER_CERT_PWD and USER_CERT_PATH=${PWD}/certs/user.p12 in the `.env` file\nWhile starting your application if you get an exception with message like: `Caused by: java.io.IOException: toDerInputStream rejects tag type 77`, this is due to keystore p12 file was not decoded with base64.  \n\nAlso set the env variable as SECURE_PROTOCOL=SSL\n\n## Using Kafdrop\n\n[Kafdrop](https://github.com/obsidiandynamics/kafdrop) is a web UI for viewing Kafka topics and browsing consumer groups. It is very helpful for development purpose. \n\nHere are scripts that can be useful to start a local Kafdrop webserver.\n\n```shell\nsource .env\nsed 's/KAFKA_USER/'$KAFKA_USER'/g' ./scripts/kafka.properties > ./scripts/output.properties\nsed -i '' 's/KAFKA_PASSWORD/'$KAFKA_PASSWORD'/g' ./scripts/output.properties\nsed -i '' 's/KAFKA_CERT_PWD/'$KAFKA_CERT_PWD'/g' ./scripts/output.properties\ndocker run -d --rm -p 9000:9000 \\\n    --name kafdrop \\\n    -v $(pwd)/certs:/home/certs \\\n    -e KAFKA_BROKERCONNECT=$KAFKA_BROKERS \\\n    -e KAFKA_PROPERTIES=$(cat ./scripts/output.properties | base64) \\\n    -e JVM_OPTS=\"-Xms32M -Xmx64M\" \\\n    -e SERVER_SERVLET_CONTEXTPATH=\"/\" \\\n    obsidiandynamics/kafdrop\n```\n\n[See also those scripts: startKafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/startKafdrop.sh)\n\nand [to stop kafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/stopKafdrop.sh)\n\n```shell\ndocker stop $(docker stop kafdrop)\n```\n\nThe Web console is at [http://localhost:9000/](http://localhost:9000/)","type":"Mdx","contentDigest":"c4adca5a4cdfe4189a5740b562680a1a","counter":638,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Common pre-requisites\ndescription: Common pre-requisites for the different labs and use cases\n---\n\n<AnchorLinks>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Creating Event Streams Topics</AnchorLink>\n  <AnchorLink>Getting scram authentication from Event Streams on OpenShift</AnchorLink>\n  <AnchorLink>Getting TLS authentication from Event Streams on OpenShift</AnchorLink>\n  <AnchorLink>Using Kafdrop</AnchorLink>\n</AnchorLinks>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Creating Event Streams Topics\n\nThis section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift using the User Interface. The example is to define a topic named INBOUND with 1 partition and a replica set to 3. Change to any other topic you want to create.\n\n- Get the route for event streams. We assume the namespace is `eventstreams`\n \n ```shell\n  oc get routes -n eventstreams | grep ui\n  ```\n\n- Navigate to the Event Streams Console using the exposed route URL.\n\n- Click the Topics option on the navigation bar on the left. Create the INBOUND topic.\n\n![Create Topic](./images/create-topic.png)\n\n![Topic Name](./images/inbound-topic-name.png)\n\n- Leave Partitions at 1.\n\n![Partition](./images/partitions.png)\n\n- Depending on how long you want messages to persist you can change this.\n\n![Message Retention](./images/message-retention.png)\n\n- You can leave Replication Factor at the default 3.\n\n![Replication](./images/replicas.png)\n\n- Click Create.\n\n### Create topic using CLI\n\nYou can do the samething using CLI, [see the product documentation](https://ibm.github.io/event-streams/getting-started/creating-topics/).\n\n ```shell\n cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n ```\n\n## Getting scram authentication from Event Streams on OpenShift\n\n- To connect to an Event Streams Instance from an application running out side of the OpenShift cluster, we need to get external access credential. This is done by getting a scram user and TLS server certificate. \n\n- While viewing our Event Streams Instance, navigate to the 'Connect to this cluster' menu from the left. Click Connect to this Cluster - \n\n- Make sure that you're on **External Connection** as we will need to test/connect from an application not running on OCP cluster. Keep note of your **Bootstrap Server Address**. Save this somewhere as we will need this later to configure our application's connection to the Event Streams instance.\n\n![Connect to this Cluster](./images/esv10-connect-to-cluster.png)\n\n- Generate your **SCRAM Credentials**. Click the `Generate SCRAM Credentials` button.\n\n![Generate SCRAM1 ](./images/esv10-generate-scram.png)\n\n- Select a name for your secret and make sure you remember it as this also represents the user identifier for the connection. Also choose the Produce, Consume, Create Topics and Schema Option.\n\n![Generate SCRAM 2](./images/esv10-scram-1.png)\n\n- Select `All Topics` and then click Next.\n\n![Generate SCRAM 3](./images/esv10-scram-2.png)\n\n- Leave it with `All Consumer Groups` and click Next.\n\n![Generate SCRAM 4](./images/esv10-scram-3.png)\n\n- Select all transaction IDs and then generate your **API Key**. Click the Generate API Key button.\n\n![Generate SCRAM 5](./images/esv10-scram-4.png)\n\n- Select a name for your application. It doesn't really matter too much what you name it. \n\n- If the following error occurs, log into your OpenShift cluster and go to the project/namespace that your Event Streams v10 instance is installed in. If you run these commands you will see your user created.\n\n`oc get kafkauser`\n\n`oc get secrets`\n\n- Your `SCRAM Username` that we will need for later is the name of your secret. The `SCRAM Password` can be either obtained by going to your OpenShift Console Web UI and revealing the password through the secrets menu or through the CLI. Run the following commands to see the value of the password.\n\n`oc get secret your-secret -jsonpath '{.data.password}' -n eventstreams | base64 --decode `\n\n- This will return you a base64 encoded value. Decode this like so\n\n`echo \"dWpPN3ZSSEF1clRK\" | base64 -d`\n\n![SCRAM Secret](./images/esv10-scram-secret.png)\n\n\n- Lastly download the PKCS12 truststore .pkcs12 certificate. Once you hit the Download button the password to the truststore will be revealed. **Copy this down** as we will need it.\n\n![PKCS12 Truststore](./images/esv10-pkcs12.png)\n\n\n**Summary** \n\nWe now have the bootstrap server address, SCRAM Username (also the name of the KafkaUser/Secret CRs) and Password, .pcks12 truststore certificate, and the truststore password associated with that certificate to allow us the ability to connect to our Event Streams instance.\n\n\n## Getting TLS authentication from Event Streams on OpenShift\n\n### Get TLS server public certificate\n\nThe cluster public certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. We can also get those certificate and password via CLI:\n\nSee [product documentation for detail](https://ibm.github.io/event-streams/getting-started/connecting/). A quick set of commands:\n\n ```shell\n oc get secret -n eventstreams | grep cluster-ca-cert\n # use the ca-cert for the instance you target \n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams -o jsonpath='{.data.ca\\.p12}' | base64 --decode > es-cert.p12\n # or the command:\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.p12 -n eventstreams\n # and password\n oc get secret minimal-prod-cluster-ca-cert -n eventstreams -o jsonpath='{.data.ca\\.password}' | base64 --decode\n # or the command\n oc extract secret/minimal-prod-cluster-ca-cert --keys=ca.password -n eventstreams\n ```\n\nCopy the server certificate into your OpenShift project where consumer and producer code will be deployed:\n\n```shell\n oc get secret minimal-prod-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f - \n```\n\n### Get Scram-sha-512 user \n\nSelect one of the Kafka users with Scram authentication defined or create a new one with the produce, consume messages and create topic and schemas authorizations, on all topics or topic with a specific prefix, on all consumer groups or again with a specific prefix, all transaction IDs.\n\n  ```shell\n  # if not logged yes to your openshift cluster where the docker private registry resides do:\n  oc login --token=... --server=https://c...\n  oc get kafkausers -n eventstreams\n\n  NAME                                     AUTHENTICATION   AUTHORIZATION\n  demo-app                                 scram-sha-512    simple\n  ```\n\n  We use a user with scram-sha-512 authentication named: `demo-app`\n\n* Copy user's secret to the current project where the application will run\n\n ```shell\n oc get secret  demo-app -n eventstreams --export -o yaml | oc apply -f -\n ```\n\n* Get the user password:\n\n ```shell\n oc get secret demo-app -n eventstreams -o jsonpath='{.data.password}' | base64 --decode\n ```\n\n* Set the KAFKA_USER and KAFKA_PASSWORD, and SECURE_PROTOCOL=SASL_SSL environment variables for your application.\n\n### Get TLS user \n\nIf you want to use mutual authentication with client certificate, you need a tls user. And use the same commands as in previous section to get user name and user's password. The differences are in the password access and that you need to download client certificate as keystore to be integrated in the Java app.\n\n ```shell\n # password is in another key\n oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.password}' | base64 --decode  \n ```\n\n* Get the user client certificate and password\n\n  ```shell\n  cd certs\n  oc get secret tls-user -n eventstreams -o jsonpath='{.data.user\\.p12}' | base64 --decode > user.p12\n  ```\n\nModify USER_CERT_PWD and USER_CERT_PATH=${PWD}/certs/user.p12 in the `.env` file\nWhile starting your application if you get an exception with message like: `Caused by: java.io.IOException: toDerInputStream rejects tag type 77`, this is due to keystore p12 file was not decoded with base64.  \n\nAlso set the env variable as SECURE_PROTOCOL=SSL\n\n## Using Kafdrop\n\n[Kafdrop](https://github.com/obsidiandynamics/kafdrop) is a web UI for viewing Kafka topics and browsing consumer groups. It is very helpful for development purpose. \n\nHere are scripts that can be useful to start a local Kafdrop webserver.\n\n```shell\nsource .env\nsed 's/KAFKA_USER/'$KAFKA_USER'/g' ./scripts/kafka.properties > ./scripts/output.properties\nsed -i '' 's/KAFKA_PASSWORD/'$KAFKA_PASSWORD'/g' ./scripts/output.properties\nsed -i '' 's/KAFKA_CERT_PWD/'$KAFKA_CERT_PWD'/g' ./scripts/output.properties\ndocker run -d --rm -p 9000:9000 \\\n    --name kafdrop \\\n    -v $(pwd)/certs:/home/certs \\\n    -e KAFKA_BROKERCONNECT=$KAFKA_BROKERS \\\n    -e KAFKA_PROPERTIES=$(cat ./scripts/output.properties | base64) \\\n    -e JVM_OPTS=\"-Xms32M -Xmx64M\" \\\n    -e SERVER_SERVLET_CONTEXTPATH=\"/\" \\\n    obsidiandynamics/kafdrop\n```\n\n[See also those scripts: startKafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/startKafdrop.sh)\n\nand [to stop kafdrop](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-item-inventory/master/scripts/stopKafdrop.sh)\n\n```shell\ndocker stop $(docker stop kafdrop)\n```\n\nThe Web console is at [http://localhost:9000/](http://localhost:9000/)","frontmatter":{"title":"Common pre-requisites","description":"Common pre-requisites for the different labs and use cases"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/overview/pre-requisites.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}