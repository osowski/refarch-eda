{"componentChunkName":"component---src-pages-use-cases-overview-product-migration-mdx","path":"/use-cases/overview/product-migration/","result":{"pageContext":{"frontmatter":{"title":"Product migration practices","description":"Considerations to address to migrate between Kafka Providers"},"relativePagePath":"/use-cases/overview/product-migration.mdx","titleType":"append","MdxNode":{"id":"8500dd68-5651-5bc1-bb3a-61dd9396e9d0","children":[],"parent":"7bce2f57-1d5c-526a-a3d8-fc396930e028","internal":{"content":"---\ntitle: Product migration practices\ndescription: Considerations to address to migrate between Kafka Providers\n---\n\nThis note tries to regroup considerations and practices on how to migrate from one Kafka deployment to another: this is not version to version migration but product to product, or from on-premise VM base or bare metal to cloud, for example.\n\n## Things to consider before migrating\n\nThe important dimensions that impact migration:\n\n* The number of Kafka cluster to migrate: Production clusters can have different semantic like federation with other clusters or completely independent cluster. Which one to migrate first?. The following table can be populated:\n\n| Cluster name | Location | Kafka Version | Bootstrap URL | # AZ | Type (Bare metal, VM, AWS, k8s...) |\n| --- | --- | --- | --- |\n| Dal04-eda-1 | Dallas-03  | 2.6  |  |  3 |  VM |\n|  |  |  |\n     \n  * Build a list of clusters, with number of brokers, zookeepers, product version and expected cores, memory and storage characteristics.\n  * Here is a table to fill for each cluster described above\n\n**Cluster name: Dal04-eda-1**\n\n| # Brokers | # Zookeepers |  # Cores/serv | Memory | Disk | Storage Type | Schema registry | Mirroring |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|   5 |  3  |  16  |   64G  |  300G   | block - NFS | Apicurio | MM2 |\n|     |     |     |     |     |     |  |   |\n\n* Assess the consequence to stop the cluster, the maintenance window (rebuild time) or full availabilty needs\n* Current physical resource consumption on exiting clusters, and linked to the type of deployment: bare metal, VM based, or k8s based. Moving from Bare metal, with performance optimized cluster to a shared deploy on OpenShift will impact the sizing of the target\n* New server availability to deploy the new product, or cloud instances to host the new version.\n    * If the target deployment is Kubernetes based, like OpenShift, the k8s cluster needs to have new mostly dedicated worker nodes to support the new Kafka brokers. Expect 60% of the worker node allocated to Kafka Broker. So is it possible to extend the k8s cluster easily\n* The type of persistence storage used for append log and if the storage is based on network filesystem so can we reuse them between clusters\n* We assume no disturbance and co-existence of the existing cluster for a time period. So this means we need to list the producers for each cluster, and if they are designed to easily change broker bootstrap end points.\n   * Assess how those producers support rolling out\n\nHere a table that can be used to gather\n\n| Producer ref | Type (native, REST proxy, Kafka connector| Avg Throughput msg/s | Message size | Message Format | # of instances |\n| --- | --- | --- | --- | --- | --- | \n| order app | quarkus app - native API |  60 msg/s | 300k  | Avro | 3  |\n|  MQ to Kafka   | KConnector |   30 msg/s  |  15k    |  none   |  1   |\n\n* Topic metadata like: Retention time, and message size:\n\n| Topic Ref | # Partition | Replica | Retention | Type (compact/log) | Replicated to Other Cluster |\n| --- | --- | --- | --- | --- | --- |\n| order | 3 | 3 / 2 in sync | 20 days | log | replicated via mirror maker 2 |\n|  |  |  |   |   |  | \n\n* For schema registry, should it be possible to coexiste or should it needs to be migrated too?\n  * How easy will it be possible to swap schema registry URL and credential for each producer and consumer?\n* Number of users / service accounts\n* Authentication security mechanism used per cluster\n* RBAC policies in place\n* How TLS certificates are managed now, how they are shareable between server? \n* Number of stream / stateful operator doing consumer - process - produce? How long will this processor take to process a single message? Can they be stopped and recovered? What percentage of inbound throughput will be outputted back into Apache Kafka? For each processing the following information can be gathered\n\n| App name | Techno Type | Processing Type | Processing Time | Passthrough % |\n| --- | --- | --- | --- | ---| --- |\n|     | Kstreams | Stateful with Ktable | 120ms | 100% |\n|     | Ksql |  |  | \n|     | Flynk |  |  |\n\n* Consumer information, type and processing time\n\n| App name | Techno Type | Processing Type | Processing Time |\n| --- | --- | --- | --- |\n|  |   |   | \n\n## Migration strategies\n\n### Deploy - mirror - rollout\n\n* Deploy new cluster with new product\n* Redefine target users and security setting\n* Start mirroring data to new cluster\n\n  ![mirroring](./images/mirroring.png)\n\n* Stop consumer on source cluster, and move them to consumer from mirrored topics\n* Stop producers from source, and connect to new cluster, on the new topics (not the mirrored ones)\n\n  ![Connect producers](./images/producertocluster.png)\n\n* Drain data from mirrored topic via the consumers\n* Connect consumers to original topic but on the new cluster.\n\n  ![Connect consumers](./images/consumertocluster.png)","type":"Mdx","contentDigest":"5df2a6112df1d45a22cb8257937c6b13","owner":"gatsby-plugin-mdx","counter":738},"frontmatter":{"title":"Product migration practices","description":"Considerations to address to migrate between Kafka Providers"},"exports":{},"rawBody":"---\ntitle: Product migration practices\ndescription: Considerations to address to migrate between Kafka Providers\n---\n\nThis note tries to regroup considerations and practices on how to migrate from one Kafka deployment to another: this is not version to version migration but product to product, or from on-premise VM base or bare metal to cloud, for example.\n\n## Things to consider before migrating\n\nThe important dimensions that impact migration:\n\n* The number of Kafka cluster to migrate: Production clusters can have different semantic like federation with other clusters or completely independent cluster. Which one to migrate first?. The following table can be populated:\n\n| Cluster name | Location | Kafka Version | Bootstrap URL | # AZ | Type (Bare metal, VM, AWS, k8s...) |\n| --- | --- | --- | --- |\n| Dal04-eda-1 | Dallas-03  | 2.6  |  |  3 |  VM |\n|  |  |  |\n     \n  * Build a list of clusters, with number of brokers, zookeepers, product version and expected cores, memory and storage characteristics.\n  * Here is a table to fill for each cluster described above\n\n**Cluster name: Dal04-eda-1**\n\n| # Brokers | # Zookeepers |  # Cores/serv | Memory | Disk | Storage Type | Schema registry | Mirroring |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|   5 |  3  |  16  |   64G  |  300G   | block - NFS | Apicurio | MM2 |\n|     |     |     |     |     |     |  |   |\n\n* Assess the consequence to stop the cluster, the maintenance window (rebuild time) or full availabilty needs\n* Current physical resource consumption on exiting clusters, and linked to the type of deployment: bare metal, VM based, or k8s based. Moving from Bare metal, with performance optimized cluster to a shared deploy on OpenShift will impact the sizing of the target\n* New server availability to deploy the new product, or cloud instances to host the new version.\n    * If the target deployment is Kubernetes based, like OpenShift, the k8s cluster needs to have new mostly dedicated worker nodes to support the new Kafka brokers. Expect 60% of the worker node allocated to Kafka Broker. So is it possible to extend the k8s cluster easily\n* The type of persistence storage used for append log and if the storage is based on network filesystem so can we reuse them between clusters\n* We assume no disturbance and co-existence of the existing cluster for a time period. So this means we need to list the producers for each cluster, and if they are designed to easily change broker bootstrap end points.\n   * Assess how those producers support rolling out\n\nHere a table that can be used to gather\n\n| Producer ref | Type (native, REST proxy, Kafka connector| Avg Throughput msg/s | Message size | Message Format | # of instances |\n| --- | --- | --- | --- | --- | --- | \n| order app | quarkus app - native API |  60 msg/s | 300k  | Avro | 3  |\n|  MQ to Kafka   | KConnector |   30 msg/s  |  15k    |  none   |  1   |\n\n* Topic metadata like: Retention time, and message size:\n\n| Topic Ref | # Partition | Replica | Retention | Type (compact/log) | Replicated to Other Cluster |\n| --- | --- | --- | --- | --- | --- |\n| order | 3 | 3 / 2 in sync | 20 days | log | replicated via mirror maker 2 |\n|  |  |  |   |   |  | \n\n* For schema registry, should it be possible to coexiste or should it needs to be migrated too?\n  * How easy will it be possible to swap schema registry URL and credential for each producer and consumer?\n* Number of users / service accounts\n* Authentication security mechanism used per cluster\n* RBAC policies in place\n* How TLS certificates are managed now, how they are shareable between server? \n* Number of stream / stateful operator doing consumer - process - produce? How long will this processor take to process a single message? Can they be stopped and recovered? What percentage of inbound throughput will be outputted back into Apache Kafka? For each processing the following information can be gathered\n\n| App name | Techno Type | Processing Type | Processing Time | Passthrough % |\n| --- | --- | --- | --- | ---| --- |\n|     | Kstreams | Stateful with Ktable | 120ms | 100% |\n|     | Ksql |  |  | \n|     | Flynk |  |  |\n\n* Consumer information, type and processing time\n\n| App name | Techno Type | Processing Type | Processing Time |\n| --- | --- | --- | --- |\n|  |   |   | \n\n## Migration strategies\n\n### Deploy - mirror - rollout\n\n* Deploy new cluster with new product\n* Redefine target users and security setting\n* Start mirroring data to new cluster\n\n  ![mirroring](./images/mirroring.png)\n\n* Stop consumer on source cluster, and move them to consumer from mirrored topics\n* Stop producers from source, and connect to new cluster, on the new topics (not the mirrored ones)\n\n  ![Connect producers](./images/producertocluster.png)\n\n* Drain data from mirrored topic via the consumers\n* Connect consumers to original topic but on the new cluster.\n\n  ![Connect consumers](./images/consumertocluster.png)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/overview/product-migration.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}