{"componentChunkName":"component---src-pages-use-cases-connect-mq-index-copy-mdx","path":"/use-cases/connect-mq/index copy/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to IBM MQ Sink Connector","description":"Apache Kafka to IBM MQ Sink Connector use case"},"relativePagePath":"/use-cases/connect-mq/index copy.mdx","titleType":"append","MdxNode":{"id":"3cf22dda-a8d9-51b2-8dd4-5817239eab61","children":[],"parent":"a90d9c0b-a9fb-585c-8340-64b771ce098f","internal":{"content":"---\ntitle: Kafka Connect to IBM MQ Sink Connector\ndescription: Apache Kafka to IBM MQ Sink Connector use case\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\nThis scenario uses the [IBM Kafka Connect sink connector for IBM MQ](https://github.com/ibm-messaging/kafka-connect-mq-sink) to pull streaming data into a MQ queue. We can have multiple deployment combinations but we will limit to two patterns:\n\n<AnchorLinks>\n<AnchorLink>MQ Sink Connector, MQ, Kafka on OpenShift</AnchorLink>\n<AnchorLink>MQ Sink Connector on virtual or baremetal server, MQ and Event Streams on IBM Cloud</AnchorLink>\n</AnchorLinks>\n\n\nIn this example we are using IBM Event Streams running on Openshift as the Kafka data source and different MQ deployment as the destination.\n\n## Deploying MQ Sink Connector on virtual or baremetal server\n\nWe are using our own laptop for the baremetal dedployment, but this chapter will work the same on virtual server.\n\n### Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\nWe assume that you have an instance of Event Streams already running on IBM Cloud with at least on manager-level credentials created.  The credentials will come in the form of a JSON document as seen in the previous section.\nYou will need the `kafka_brokers_sasl` and `password` atribute to configure the sink connector.\n\nThis scenario uses the `inventory` topic created in the Scenario Setup in previous section.\n\n### Create Local IBM MQ Instance\n\nHere we will use Docker to create a local MQ instance.  First create a data directory to mount in the container.\n\n`mkdir qm1data`\n\nThen create the container.\n\n```shell\ndocker run                     \\\n  --name mq                    \\\n  --detach                     \\\n  --publish 1414:1414          \\\n  --publish 9443:9443          \\\n  --publish 9157:9157          \\\n  --volume qm1data:/mnt/mqm    \\\n  --env LICENSE=accept         \\\n  --env MQ_QMGR_NAME=QM1       \\\n  --env MQ_APP_PASSWORD=admin  \\\n  --env MQ_ENABLE_METRICS=true \\\n  ibmcom/mq\n```\n\nYou should be able to log into the MQ server on port 9443 with default user `admin` and password `passw0rd`.\n\nConnect to the running MQ instance to create a Channel and Queue as described on the [Using IBM MQ with Kafka Connect](https://github.com/ibm-messaging/kafka-connect-mq-sink/blob/master/UsingMQwithKafkaConnect.md) page.\n\n```shell\ndocker exec -ti mq bash\nstrmqm QM1\nrunmqsc QM1\nDEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\nALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\nREFRESH SECURITY TYPE(CONNAUTH)\nDEFINE QLOCAL(INVENTORY)\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\nSET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\nEND\n```\n\nExit the session and continue on to create the MQ Connector Sink.\n\n### Create MQ Kafka Connector Sink\n\nThe MQ Connector Sink can be downloaded from [Github](https://github.com/ibm-messaging/kafka-connect-mq-sink).  The Github site includes exhaustive instructions and an abridged version follows.\n\nClone the repository with the following command:\n\n`git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git`\n\nChange directory into the kafka-connect-mq-sink directory:\n\n`cd kafka-connect-mq-sink`\n\nBuild the connector using Maven:\n\n`mvn clean package`\n\nNext, create a directory to contain the Kafka Connector configuration.\n\n`mkdir config && cd config`\n\nCreate a configuration file called `connect-distributed.properties` for Kafka Connect based on the template below.\n\n```properties\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Producer Side\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\nproducer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. T\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\nstatus.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\nSave this file in the `config` directory.\n\nNext, create a log4j configuration file named `connect-log4j.properties` based on the template below.\n\n```properties\nlog4j.rootLogger=DEBUG, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n\nlog4j.logger.org.apache.kafka=INFO\n```\n\nSave this file to the `config` directory as well.\n\nFinally, create a JSON configuraiton file for the MQ sink.  This can be stored anywhere but it can be conveniently created in the `config` directory.  We name this file `mq-sink.json`.\n\n```json\n{\n    \"name\": \"mq-sink\",\n    \"config\":\n    {\n        \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n        \"tasks.max\": \"1\",\n        \"topics\": \"inventory\",\n\n        \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n        \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n        \"mq.queue.manager\": \"QM1\",\n        \"mq.connection.name.list\": \"mq(1414)\",\n        \"mq.user.name\": \"admin\",\n        \"mq.password\": \"passw0rd\",\n        \"mq.user.authentication.mqcsp\": true,\n        \"mq.channel.name\": \"KAFKA.CHANNEL\",\n        \"mq.queue\": \"INVENTORY\",\n        \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n    }\n}\n```\n\nBack out one directory to the `kafka-connect-mq-sink` directory.\n\n`cd ..`\n\nBuild docker image\n`docker build -t kafkaconnect-with-mq-sink:1.3.0 .`\n\nFinally, run the Kafka Connect MQ Sink container.\n\n```\ndocker run                                 \\\n  --name mq-sink                           \\\n  --detach                                 \\\n  --volume $(pwd)/config:/opt/kafka/config \\\n  --publish 8083:8083                      \\\n  --link mq:mq                             \\\n  kafkaconnect-with-mq-sink:1.3.0\n```\n\nYou should now have a working MQ sink.\n\nAs an alternate approach, when you have a Kafka Connect isntance up and running, with the dependant jar files, it is possible to configure the connector with a POST operation like:\n\n```Shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./mq-sink.json\"\n\n# The response returns the metadata about the connector\n{\"name\":\"mq-sink\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"ibmmq(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink\"},\"tasks\":[{\"connector\":\"mq-sink\",\"task\":0}],\"type\":\"sink\"}\n```\n\nOnce the connector is up and running, we can use some tool to send inventory message. In the `integration-tests` folder we have some python code to produce message. If you have a python environment with kafka api you can use yours, or we have also provided a Dockerfile to prepare a local python environment, which will not impact yours.\n\n```shell\n# if you change the name of the image\ndocker build -t ibmcase/python37 .\n# ... then update the script ./startPython.sh\n./startPython.sh\n# Now in the new bash session you should see ProduceInventoryEvent.py,... start it by sending 2 events\npython ProduceInventoryEvent.py --size 2\n# Events are random but use stores and items known by the database downstream.\n sending -> {'storeName': 'NYC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 163, 'id': 1, 'timestamp': '23-Jun-2020 04:32:38'}\n# the following trace demonstrates Kafka received the message\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'SC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 178, 'id': 2, 'timestamp': '23-Jun-2020 04:32:38'}\n[KafkaProducer] - Message delivered to inventory [0]\n```\n\nIn the Kafka Connect trace we can see:\n\n```shell\nkconnect_1  | [2020-06-23 04:23:16,270] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 26: {inventory-0=OffsetAndMetadata{offset=44, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\nkconnect_1  | [2020-06-23 04:32:46,382] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 83: {inventory-0=OffsetAndMetadata{offset=48, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\n```\n\nAnd in the IBM MQ Console, under the Local Queue: Inventory we can see the messages:\n\n![](./images/ibmq-q-inventory.png)\n\nTo remove the connector do the following command. Do this specially if you go to scenario 2 next.\n\n```shell\ncurl -X DELETE http://localhost:8083/connectors/mq-sink\n```\n\n## Deploying MQ Sink Connector to OpenShift\n We could have used MQ broker as part of Cloud Pak for integration or [as a service in IBM Cloud](https://cloud.ibm.com/docs/mqcloud/index.html).\n### Prerequisites\n\nWe are assuming you already have an instance of IBM EventStreams running on IBM Cloud from previous scenarios.  Also, we assume you have a running instance of OpenShift with a project created to run the MQ Sink.  Finally, we assume you're familiar with OpenShift and Kubernetes and know how to work with the configuration files provided below.\n\n### MQ on OpenShift\n\nStrictly speaking you don't need to move the instance of MQ previously used onto OpenShift for the MQ Sink to work however the configuration to do so is provided.  Note that this is not a production configuration and is intended for POC purposes only.\n\nCreate a ConfigMap on OpenShift with the following definition:\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-config\n  namespace: mq-demo\ndata:\n    LICENSE: accept\n    MQ_QMGR_NAME: QM1\n    MQ_APP_PASSWORD: admin\n    MQ_ENABLE_METRICS: \"true\"\n```\n\nThis will make it easier to update the MQ configuration if needed without editing everything in the Pod definition.\n\nNext, create the MQ Pod with the following definition:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ibm-mq\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: ibm-mq\n      image: ibmcom/mq\n      ports:\n        - containerPort: 1414\n          protocol: TCP\n        - containerPort: 9443\n          protocol: TCP\n        - containerPort: 9157\n          protocol: TCP\n      envFrom:\n        - configMapRef:\n            name: mq-config\n```\n\nNext, define a Service to point to the MQ Pod.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mq-service\n  namespace: mq-demo\nspec:\n  selector:\n    app: mq-kafka-sink-demo-app\n  ports:\n    - name: mq-port\n      protocol: TCP\n      port: 1414\n      targetPort: 1414\n    - name: mq-portal\n      protocol: TCP\n      port: 9443\n      targetPort: 9443\n    - name: mq-dunno\n      protocol: TCP\n      port: 9157\n      targetPort: 9157\n```\n\nFinally, define a Route to be able to access the admin UI.\n\n```yaml\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: mq-route\n  namespace: mq-demo\nspec:\n  host: ibmmq.bnpp.apps.openshift.proxmox.lab\n  to:\n    kind: Service\n    name: mq-service\n    weight: 100\n  port:\n    targetPort: mq-portal\n  tls:\n    termination: passthrough\n    insecureEdgeTerminationPolicy: Redirect\n  wildcardPolicy: None\n```\n\nYou will want to connect to the container and run the setup commands as described in a previous scenario.  At this point, IBM MQ should be running and available on OpenShift.\n\nTo run the Kafka Connect MQ Sink on OpenShift or any container platform, you will need to build a container that has Kafka installed as well as the MQ Sink and proper configuration.  Typically we would use the Strimzi containerized Kafka solution to run on OpenShift, but in this case to illustrate all the components we are building a container from scratch.  The following is the Dockerfile:\n\n```Dockerfile\nFROM ubuntu:20.04\n\nADD https://mirrors.koehn.com/apache/kafka/2.5.0/kafka_2.12-2.5.0.tgz /tmp/\n\nRUN apt update                                                                                                     && \\\n    apt install -y curl git maven                                                                                  && \\\n    tar -C /opt -xvf /tmp/kafka_2.12-2.5.0.tgz                                                                     && \\\n    rm -f /tmp/kafka_2.12-2.5.0.tgz                                                                                && \\\n    ln -s /opt/kafka_2.12-2.5.0 /opt/kafka                                                                         && \\\n    mv -f /opt/kafka/config/connect-distributed.properties /opt/kafka/config/connect-distributed.properties.bak    && \\\n    cd /opt                                                                                                        && \\\n    git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git                                           && \\\n    cd /opt/kafka-connect-mq-sink                                                                                  && \\\n    mvn clean package                                                                                              && \\\n    ln -s /opt/kafka-connect-mq-sink/target/kafka-connect-mq-sink-1.3.0-jar-with-dependencies.jar /opt/kafka/libs/ && \\\n    mv -f /opt/kafka-connect-mq-sink/config/mq-sink.json /opt/kafka-connect-mq-sink/config/mq-sink.json.bak\n\nCOPY connect-distributed.properties /opt/kafka/config/connect-distributed.properties\nCOPY mq-sink-connector-config.json /opt/kafka-connect-mq-sink/config/mq-sink.json\nCOPY entrypoint.sh /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\nWe start with a vanilla Linux container, install the binary distribution of Kafka for Linux, clone the MQ Sink repository from Github, build the MQ Sink, and finally copy in some template files.  Finally the container runs a custom entrypoint script as shown below:\n\n```shell\n#!/bin/sh\n\nset -x\n\nsed -i \"s/KAFKA_BOOTSTRAP_SERVERS/${KAFKA_BOOTSTRAP_SERVERS}/g\" /opt/kafka/config/connect-distributed.properties\nsed -i \"s/KAFKA_API_KEY/${KAFKA_API_KEY}/g\"                     /opt/kafka/config/connect-distributed.properties\n\n\nsed -i \"s/KAFKA_TOPICS/${KAFKA_TOPICS}/g\"         /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE_MANAGER/${MQ_QUEUE_MANAGER}/g\" /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_HOST/${MQ_HOST}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PORT/${MQ_PORT}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_USER/${MQ_USER}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PASSWORD/${MQ_PASSWORD}/g\"           /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_CHANNEL/${MQ_CHANNEL}/g\"             /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE/${MQ_QUEUE}/g\"                 /opt/kafka-connect-mq-sink/config/mq-sink.json\n\n/opt/kafka/bin/connect-distributed.sh /opt/kafka/config/connect-distributed.properties &\n\nsleep 60\ncurl -X DELETE -H \"Content-Type: application/json\" http://localhost:8083/connectors/mq-sink-connector\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@/opt/kafka-connect-mq-sink/config/mq-sink.json\"\n\ntail -f /dev/null\n```\n\nAgain this is not a production-ready entrypoint container script; it's intended for POC purposes.  The script updates the template files copied into the container with values from the environment (either `--env` using Docker, or from a `ConfigMap` in OpenShift) and then starts Kafka Connect in distributed mode.  It pauses the script for 1 minute to let Kafka Connect start, then finally activates the MQ Sink by POSTing the MQ Sink configuration using `cURL`.\n\nThe two template configuration files that are copied into the container are shown below.\n\n### connect-distributed.properties:\n\n```shell\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Producer Side\nproducer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,\n# and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\n\n#status.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\n### mq-sink-connector-config.json:\n\n```json\n{\n  \"name\": \"mq-sink-connector\",\n  \"config\":\n  {\n      \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n      \"tasks.max\": \"1\",\n      \"topics\": \"KAFKA_TOPICS\",\n\n      \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n      \"mq.queue.manager\": \"MQ_QUEUE_MANAGER\",\n      \"mq.connection.name.list\": \"MQ_HOST(MQ_PORT)\",\n      \"mq.user.name\": \"MQ_USER\",\n      \"mq.password\": \"MQ_PASSWORD\",\n      \"mq.user.authentication.mqcsp\": true,\n      \"mq.channel.name\": \"MQ_CHANNEL\",\n      \"mq.queue\": \"MQ_QUEUE\",\n      \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n  }\n}\n```\n\nNotice in these two files there are several capitalized variables which are replaced by the entrypoint.sh script at container startup.\n\nTo deploy the container on OpenShift, we create a `ConfigMap` with information about the IBM EventStreams on IBM Cloud instance as well as the local instance of MQ on OpenShift.\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-kafka-sink-demo-config\n  namespace: mq-demo\ndata:\n  KAFKA_API_KEY: bA ... Qp\n  KAFKA_BOOTSTRAP_SERVERS: >-\n    broker-1- ... eventstreams.cloud.ibm.com:9093,broker-0- ... eventstreams.cloud.ibm.com:9093,broker-4- ... eventstreams.cloud.ibm.com:9093,broker-2- ... eventstreams.cloud.ibm.com:9093,broker-5- ... eventstreams.cloud.ibm.com:9093,broker-3- ... eventstreams.cloud.ibm.com:9093\n  KAFKA_TOPICS: inventory\n  MQ_HOST: mq-service\n  MQ_PORT: \"1414\"\n  MQ_USER: admin\n  MQ_QUEUE_MANAGER: QM1\n  MQ_PASSWORD: passw0rd\n  MQ_CHANNEL: KAFKA.CHANNEL\n  MQ_QUEUE: INVENTORY\n```\n\nFinally to deploy the MQ Sink container, we create a Pod definition on OpenShift:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mq-kafka-sink-demo\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: mq-kafka-sink-demo\n      image: registry/mq-kafka-sink-demo:0.0.1\n      envFrom:\n        - configMapRef:\n            name: mq-kafka-sink-demo-config\n```\n\nWith the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured.  You will see signs of success in the container output (via oc logs, or in the UI):\n\n```shell\n+ curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json\n...\n{\"name\":\"mq-sink-connector\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"mq-service(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink-connector\"},\"tasks\":[{\"connector\":\"mq-sink-connector\",\"task\":0}],\"type\":\"sink\"}\n...\n[2020-06-23 04:26:26,054] INFO Creating task mq-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:419)\n...[2020-06-23 04:26:26,449] INFO Connection to MQ established (com.ibm.eventstreams.connect.mqsink.JMSWriter:229)\n[2020-06-23 04:26:26,449] INFO WorkerSinkTask{id=mq-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)\n```\n\nYou should now have the Kafka Connector MQ Sink running on OpenShift.\n","type":"Mdx","contentDigest":"c70995329afbb8e2ef541f1664c37d1f","owner":"gatsby-plugin-mdx","counter":716},"frontmatter":{"title":"Kafka Connect to IBM MQ Sink Connector","description":"Apache Kafka to IBM MQ Sink Connector use case"},"exports":{},"rawBody":"---\ntitle: Kafka Connect to IBM MQ Sink Connector\ndescription: Apache Kafka to IBM MQ Sink Connector use case\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\nThis scenario uses the [IBM Kafka Connect sink connector for IBM MQ](https://github.com/ibm-messaging/kafka-connect-mq-sink) to pull streaming data into a MQ queue. We can have multiple deployment combinations but we will limit to two patterns:\n\n<AnchorLinks>\n<AnchorLink>MQ Sink Connector, MQ, Kafka on OpenShift</AnchorLink>\n<AnchorLink>MQ Sink Connector on virtual or baremetal server, MQ and Event Streams on IBM Cloud</AnchorLink>\n</AnchorLinks>\n\n\nIn this example we are using IBM Event Streams running on Openshift as the Kafka data source and different MQ deployment as the destination.\n\n## Deploying MQ Sink Connector on virtual or baremetal server\n\nWe are using our own laptop for the baremetal dedployment, but this chapter will work the same on virtual server.\n\n### Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\nWe assume that you have an instance of Event Streams already running on IBM Cloud with at least on manager-level credentials created.  The credentials will come in the form of a JSON document as seen in the previous section.\nYou will need the `kafka_brokers_sasl` and `password` atribute to configure the sink connector.\n\nThis scenario uses the `inventory` topic created in the Scenario Setup in previous section.\n\n### Create Local IBM MQ Instance\n\nHere we will use Docker to create a local MQ instance.  First create a data directory to mount in the container.\n\n`mkdir qm1data`\n\nThen create the container.\n\n```shell\ndocker run                     \\\n  --name mq                    \\\n  --detach                     \\\n  --publish 1414:1414          \\\n  --publish 9443:9443          \\\n  --publish 9157:9157          \\\n  --volume qm1data:/mnt/mqm    \\\n  --env LICENSE=accept         \\\n  --env MQ_QMGR_NAME=QM1       \\\n  --env MQ_APP_PASSWORD=admin  \\\n  --env MQ_ENABLE_METRICS=true \\\n  ibmcom/mq\n```\n\nYou should be able to log into the MQ server on port 9443 with default user `admin` and password `passw0rd`.\n\nConnect to the running MQ instance to create a Channel and Queue as described on the [Using IBM MQ with Kafka Connect](https://github.com/ibm-messaging/kafka-connect-mq-sink/blob/master/UsingMQwithKafkaConnect.md) page.\n\n```shell\ndocker exec -ti mq bash\nstrmqm QM1\nrunmqsc QM1\nDEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\nALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\nREFRESH SECURITY TYPE(CONNAUTH)\nDEFINE QLOCAL(INVENTORY)\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\nSET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\nEND\n```\n\nExit the session and continue on to create the MQ Connector Sink.\n\n### Create MQ Kafka Connector Sink\n\nThe MQ Connector Sink can be downloaded from [Github](https://github.com/ibm-messaging/kafka-connect-mq-sink).  The Github site includes exhaustive instructions and an abridged version follows.\n\nClone the repository with the following command:\n\n`git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git`\n\nChange directory into the kafka-connect-mq-sink directory:\n\n`cd kafka-connect-mq-sink`\n\nBuild the connector using Maven:\n\n`mvn clean package`\n\nNext, create a directory to contain the Kafka Connector configuration.\n\n`mkdir config && cd config`\n\nCreate a configuration file called `connect-distributed.properties` for Kafka Connect based on the template below.\n\n```properties\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Producer Side\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\nproducer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. T\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\nstatus.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\nSave this file in the `config` directory.\n\nNext, create a log4j configuration file named `connect-log4j.properties` based on the template below.\n\n```properties\nlog4j.rootLogger=DEBUG, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n\nlog4j.logger.org.apache.kafka=INFO\n```\n\nSave this file to the `config` directory as well.\n\nFinally, create a JSON configuraiton file for the MQ sink.  This can be stored anywhere but it can be conveniently created in the `config` directory.  We name this file `mq-sink.json`.\n\n```json\n{\n    \"name\": \"mq-sink\",\n    \"config\":\n    {\n        \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n        \"tasks.max\": \"1\",\n        \"topics\": \"inventory\",\n\n        \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n        \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n        \"mq.queue.manager\": \"QM1\",\n        \"mq.connection.name.list\": \"mq(1414)\",\n        \"mq.user.name\": \"admin\",\n        \"mq.password\": \"passw0rd\",\n        \"mq.user.authentication.mqcsp\": true,\n        \"mq.channel.name\": \"KAFKA.CHANNEL\",\n        \"mq.queue\": \"INVENTORY\",\n        \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n    }\n}\n```\n\nBack out one directory to the `kafka-connect-mq-sink` directory.\n\n`cd ..`\n\nBuild docker image\n`docker build -t kafkaconnect-with-mq-sink:1.3.0 .`\n\nFinally, run the Kafka Connect MQ Sink container.\n\n```\ndocker run                                 \\\n  --name mq-sink                           \\\n  --detach                                 \\\n  --volume $(pwd)/config:/opt/kafka/config \\\n  --publish 8083:8083                      \\\n  --link mq:mq                             \\\n  kafkaconnect-with-mq-sink:1.3.0\n```\n\nYou should now have a working MQ sink.\n\nAs an alternate approach, when you have a Kafka Connect isntance up and running, with the dependant jar files, it is possible to configure the connector with a POST operation like:\n\n```Shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./mq-sink.json\"\n\n# The response returns the metadata about the connector\n{\"name\":\"mq-sink\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"ibmmq(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink\"},\"tasks\":[{\"connector\":\"mq-sink\",\"task\":0}],\"type\":\"sink\"}\n```\n\nOnce the connector is up and running, we can use some tool to send inventory message. In the `integration-tests` folder we have some python code to produce message. If you have a python environment with kafka api you can use yours, or we have also provided a Dockerfile to prepare a local python environment, which will not impact yours.\n\n```shell\n# if you change the name of the image\ndocker build -t ibmcase/python37 .\n# ... then update the script ./startPython.sh\n./startPython.sh\n# Now in the new bash session you should see ProduceInventoryEvent.py,... start it by sending 2 events\npython ProduceInventoryEvent.py --size 2\n# Events are random but use stores and items known by the database downstream.\n sending -> {'storeName': 'NYC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 163, 'id': 1, 'timestamp': '23-Jun-2020 04:32:38'}\n# the following trace demonstrates Kafka received the message\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'SC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 178, 'id': 2, 'timestamp': '23-Jun-2020 04:32:38'}\n[KafkaProducer] - Message delivered to inventory [0]\n```\n\nIn the Kafka Connect trace we can see:\n\n```shell\nkconnect_1  | [2020-06-23 04:23:16,270] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 26: {inventory-0=OffsetAndMetadata{offset=44, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\nkconnect_1  | [2020-06-23 04:32:46,382] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 83: {inventory-0=OffsetAndMetadata{offset=48, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\n```\n\nAnd in the IBM MQ Console, under the Local Queue: Inventory we can see the messages:\n\n![](./images/ibmq-q-inventory.png)\n\nTo remove the connector do the following command. Do this specially if you go to scenario 2 next.\n\n```shell\ncurl -X DELETE http://localhost:8083/connectors/mq-sink\n```\n\n## Deploying MQ Sink Connector to OpenShift\n We could have used MQ broker as part of Cloud Pak for integration or [as a service in IBM Cloud](https://cloud.ibm.com/docs/mqcloud/index.html).\n### Prerequisites\n\nWe are assuming you already have an instance of IBM EventStreams running on IBM Cloud from previous scenarios.  Also, we assume you have a running instance of OpenShift with a project created to run the MQ Sink.  Finally, we assume you're familiar with OpenShift and Kubernetes and know how to work with the configuration files provided below.\n\n### MQ on OpenShift\n\nStrictly speaking you don't need to move the instance of MQ previously used onto OpenShift for the MQ Sink to work however the configuration to do so is provided.  Note that this is not a production configuration and is intended for POC purposes only.\n\nCreate a ConfigMap on OpenShift with the following definition:\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-config\n  namespace: mq-demo\ndata:\n    LICENSE: accept\n    MQ_QMGR_NAME: QM1\n    MQ_APP_PASSWORD: admin\n    MQ_ENABLE_METRICS: \"true\"\n```\n\nThis will make it easier to update the MQ configuration if needed without editing everything in the Pod definition.\n\nNext, create the MQ Pod with the following definition:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ibm-mq\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: ibm-mq\n      image: ibmcom/mq\n      ports:\n        - containerPort: 1414\n          protocol: TCP\n        - containerPort: 9443\n          protocol: TCP\n        - containerPort: 9157\n          protocol: TCP\n      envFrom:\n        - configMapRef:\n            name: mq-config\n```\n\nNext, define a Service to point to the MQ Pod.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mq-service\n  namespace: mq-demo\nspec:\n  selector:\n    app: mq-kafka-sink-demo-app\n  ports:\n    - name: mq-port\n      protocol: TCP\n      port: 1414\n      targetPort: 1414\n    - name: mq-portal\n      protocol: TCP\n      port: 9443\n      targetPort: 9443\n    - name: mq-dunno\n      protocol: TCP\n      port: 9157\n      targetPort: 9157\n```\n\nFinally, define a Route to be able to access the admin UI.\n\n```yaml\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: mq-route\n  namespace: mq-demo\nspec:\n  host: ibmmq.bnpp.apps.openshift.proxmox.lab\n  to:\n    kind: Service\n    name: mq-service\n    weight: 100\n  port:\n    targetPort: mq-portal\n  tls:\n    termination: passthrough\n    insecureEdgeTerminationPolicy: Redirect\n  wildcardPolicy: None\n```\n\nYou will want to connect to the container and run the setup commands as described in a previous scenario.  At this point, IBM MQ should be running and available on OpenShift.\n\nTo run the Kafka Connect MQ Sink on OpenShift or any container platform, you will need to build a container that has Kafka installed as well as the MQ Sink and proper configuration.  Typically we would use the Strimzi containerized Kafka solution to run on OpenShift, but in this case to illustrate all the components we are building a container from scratch.  The following is the Dockerfile:\n\n```Dockerfile\nFROM ubuntu:20.04\n\nADD https://mirrors.koehn.com/apache/kafka/2.5.0/kafka_2.12-2.5.0.tgz /tmp/\n\nRUN apt update                                                                                                     && \\\n    apt install -y curl git maven                                                                                  && \\\n    tar -C /opt -xvf /tmp/kafka_2.12-2.5.0.tgz                                                                     && \\\n    rm -f /tmp/kafka_2.12-2.5.0.tgz                                                                                && \\\n    ln -s /opt/kafka_2.12-2.5.0 /opt/kafka                                                                         && \\\n    mv -f /opt/kafka/config/connect-distributed.properties /opt/kafka/config/connect-distributed.properties.bak    && \\\n    cd /opt                                                                                                        && \\\n    git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git                                           && \\\n    cd /opt/kafka-connect-mq-sink                                                                                  && \\\n    mvn clean package                                                                                              && \\\n    ln -s /opt/kafka-connect-mq-sink/target/kafka-connect-mq-sink-1.3.0-jar-with-dependencies.jar /opt/kafka/libs/ && \\\n    mv -f /opt/kafka-connect-mq-sink/config/mq-sink.json /opt/kafka-connect-mq-sink/config/mq-sink.json.bak\n\nCOPY connect-distributed.properties /opt/kafka/config/connect-distributed.properties\nCOPY mq-sink-connector-config.json /opt/kafka-connect-mq-sink/config/mq-sink.json\nCOPY entrypoint.sh /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\nWe start with a vanilla Linux container, install the binary distribution of Kafka for Linux, clone the MQ Sink repository from Github, build the MQ Sink, and finally copy in some template files.  Finally the container runs a custom entrypoint script as shown below:\n\n```shell\n#!/bin/sh\n\nset -x\n\nsed -i \"s/KAFKA_BOOTSTRAP_SERVERS/${KAFKA_BOOTSTRAP_SERVERS}/g\" /opt/kafka/config/connect-distributed.properties\nsed -i \"s/KAFKA_API_KEY/${KAFKA_API_KEY}/g\"                     /opt/kafka/config/connect-distributed.properties\n\n\nsed -i \"s/KAFKA_TOPICS/${KAFKA_TOPICS}/g\"         /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE_MANAGER/${MQ_QUEUE_MANAGER}/g\" /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_HOST/${MQ_HOST}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PORT/${MQ_PORT}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_USER/${MQ_USER}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PASSWORD/${MQ_PASSWORD}/g\"           /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_CHANNEL/${MQ_CHANNEL}/g\"             /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE/${MQ_QUEUE}/g\"                 /opt/kafka-connect-mq-sink/config/mq-sink.json\n\n/opt/kafka/bin/connect-distributed.sh /opt/kafka/config/connect-distributed.properties &\n\nsleep 60\ncurl -X DELETE -H \"Content-Type: application/json\" http://localhost:8083/connectors/mq-sink-connector\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@/opt/kafka-connect-mq-sink/config/mq-sink.json\"\n\ntail -f /dev/null\n```\n\nAgain this is not a production-ready entrypoint container script; it's intended for POC purposes.  The script updates the template files copied into the container with values from the environment (either `--env` using Docker, or from a `ConfigMap` in OpenShift) and then starts Kafka Connect in distributed mode.  It pauses the script for 1 minute to let Kafka Connect start, then finally activates the MQ Sink by POSTing the MQ Sink configuration using `cURL`.\n\nThe two template configuration files that are copied into the container are shown below.\n\n### connect-distributed.properties:\n\n```shell\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Producer Side\nproducer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,\n# and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\n\n#status.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\n### mq-sink-connector-config.json:\n\n```json\n{\n  \"name\": \"mq-sink-connector\",\n  \"config\":\n  {\n      \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n      \"tasks.max\": \"1\",\n      \"topics\": \"KAFKA_TOPICS\",\n\n      \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n      \"mq.queue.manager\": \"MQ_QUEUE_MANAGER\",\n      \"mq.connection.name.list\": \"MQ_HOST(MQ_PORT)\",\n      \"mq.user.name\": \"MQ_USER\",\n      \"mq.password\": \"MQ_PASSWORD\",\n      \"mq.user.authentication.mqcsp\": true,\n      \"mq.channel.name\": \"MQ_CHANNEL\",\n      \"mq.queue\": \"MQ_QUEUE\",\n      \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n  }\n}\n```\n\nNotice in these two files there are several capitalized variables which are replaced by the entrypoint.sh script at container startup.\n\nTo deploy the container on OpenShift, we create a `ConfigMap` with information about the IBM EventStreams on IBM Cloud instance as well as the local instance of MQ on OpenShift.\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-kafka-sink-demo-config\n  namespace: mq-demo\ndata:\n  KAFKA_API_KEY: bA ... Qp\n  KAFKA_BOOTSTRAP_SERVERS: >-\n    broker-1- ... eventstreams.cloud.ibm.com:9093,broker-0- ... eventstreams.cloud.ibm.com:9093,broker-4- ... eventstreams.cloud.ibm.com:9093,broker-2- ... eventstreams.cloud.ibm.com:9093,broker-5- ... eventstreams.cloud.ibm.com:9093,broker-3- ... eventstreams.cloud.ibm.com:9093\n  KAFKA_TOPICS: inventory\n  MQ_HOST: mq-service\n  MQ_PORT: \"1414\"\n  MQ_USER: admin\n  MQ_QUEUE_MANAGER: QM1\n  MQ_PASSWORD: passw0rd\n  MQ_CHANNEL: KAFKA.CHANNEL\n  MQ_QUEUE: INVENTORY\n```\n\nFinally to deploy the MQ Sink container, we create a Pod definition on OpenShift:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mq-kafka-sink-demo\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: mq-kafka-sink-demo\n      image: registry/mq-kafka-sink-demo:0.0.1\n      envFrom:\n        - configMapRef:\n            name: mq-kafka-sink-demo-config\n```\n\nWith the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured.  You will see signs of success in the container output (via oc logs, or in the UI):\n\n```shell\n+ curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json\n...\n{\"name\":\"mq-sink-connector\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"mq-service(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink-connector\"},\"tasks\":[{\"connector\":\"mq-sink-connector\",\"task\":0}],\"type\":\"sink\"}\n...\n[2020-06-23 04:26:26,054] INFO Creating task mq-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:419)\n...[2020-06-23 04:26:26,449] INFO Connection to MQ established (com.ibm.eventstreams.connect.mqsink.JMSWriter:229)\n[2020-06-23 04:26:26,449] INFO WorkerSinkTask{id=mq-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)\n```\n\nYou should now have the Kafka Connector MQ Sink running on OpenShift.\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-mq/index copy.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}