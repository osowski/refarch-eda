{"componentChunkName":"component---src-pages-use-cases-connect-rabbitmq-index-mdx","path":"/use-cases/connect-rabbitmq/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to RabbitMQ Source Connector","description":"Apache Kafka to RabbitMQ Source Connector usecase"},"relativePagePath":"/use-cases/connect-rabbitmq/index.mdx","titleType":"append","MdxNode":{"id":"c10c2cb0-8485-57da-9499-c8cf408f8e51","children":[],"parent":"d257ca0d-14bb-550b-8afa-1f34b2a6487d","internal":{"content":"---\ntitle: Kafka Connect to RabbitMQ Source Connector\ndescription: Apache Kafka to RabbitMQ Source Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites</AnchorLink>\n<AnchorLink>Start the backend environment</AnchorLink>\n<AnchorLink>Verify the RabbitMQ settings</AnchorLink>\n<AnchorLink>Configure the kafka connector for Rabbitmq source</AnchorLink>\n<AnchorLink>Generate sale messages</AnchorLink>\n<AnchorLink>Verify messages arrived in Kafka items topic</AnchorLink>\n</AnchorLinks>\n\nThis hands-on lab demonstrates how to use IBM RabbitMQ source connector to inject message to Event Streams On Premise. \nWe are using the [IBM messaging github: source connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source). The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe code, configurations and detail readme instructions are in this repository that you should clone:\n\n```shell\ngit clone https://github.com/jbcodeforce/eda-kconnect-lab\n```\n\nTo send message to RabbitMQ, we have implemented a simple simulator, to send item sale messages for one of the stores. The code is under [store-sale-producer](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/store-sale-producer) folder and we have also uploaded the image into dockerhub.\n\nFor quick start and demonstration the [github repository for this lab](https://github.com/jbcodeforce/eda-kconnect-lab) includes a docker compose file [RabbitMQ-Kconnect-compose.yaml](https://github.com/jbcodeforce/eda-kconnect-lab/blob/master/infrastructure/RabbitMQ-Kconnect-compose.yaml) under the `infrastructure` folder to run Rabbit MQ and Kafka Connect together on your local computer. We recommend to follow the instructions in the [store-sale-producer readme](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/store-sale-producer) file.\n\nBut we want to also run and deploy this solution on OpenShift and Event Streams on Premise.\n\n## Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\n If you need to rebuild the image the following commands may be done:\n\n```shell\n./mvnw clean package -Dquarkus.container-image.build=true -Dquarkus.container-image.group=ibmcase -Dquarkus.container-image.tag=1.0.0\n```\n\n## Start the backend environment\n\nIn this section we start RabbitMQ, Kafka Connect and the Store Item Sale generator app. This app exposes a REST api to generate items sale operations that happen in a predefined set of stores.\n\nUnder the infrastructure folder use the command: `docker-compose -f RabbitMQ-Kconnect-compose.yaml up`.\n\nThe trace includes RabbitMQ, storeSaleGenerator_1 and Kafka connect logs. Here is a small extract of important messages:\n\n```shell\nrabbitmq_1            | 2020-06-17 06:12:58.293 [info] <0.9.0> Server startup complete; 4 plugins started.\nrabbitmq_1            |  * rabbitmq_management\nrabbitmq_1            |  * rabbitmq_web_dispatch\nrabbitmq_1            |  * rabbitmq_management_agent\nrabbitmq_1            |  * rabbitmq_amqp1_0\nrabbitmq_1            |  completed with 4 plugins.\n....\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,056 INFO  [io.quarkus] (main) Profile prod activated.\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,057 INFO  [io.quarkus] (main) Installed features: [cdi, mutiny, resteasy, resteasy-jsonb, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-amqp, vertx]\n```\n\n## Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:1567/](http://localhost:1567/) using the guest/guest login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nIf in the Admin tab you do not see **rabbit-user** listed do the following:\n\n* Add a new admin user: **rabbit-user/rabbit-pass** using the Admin tab. Enable the virtual host to be '/'.\n\n![](./images/rabbitmq-user.png)\n\n\nGo to the Queue tab and add `items` queue with default parameters:\n\n![8](./images/rabbitmq-item-queue.png)\n\n\nWith the following result\n\n![9](./images/rabbitmq-item-queue-2.png)\n\n\n## Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n## Generate sale messages\n\nThe Store application has an OpenAPI [http://localhost:8080/swagger-ui/](http://localhost:8080/swagger-ui/) definition to send messages:  `/stores/start/2` api:\n\n![5](./images/store-app-1.png)\n\nOr use `curl -X POST http://localhost:8080/stores/start/2`\n\nIn the trace you should see something like:\n\n```shell\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":0,\"storeName\":\"SF02\",\"itemCode\":\"IT07\",\"quantity\":7,\"price\":46.79320631709398}\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":1,\"storeName\":\"NYC01\",\"itemCode\":\"IT00\",\"quantity\":7,\"price\":0.7764381649099172}\n```\n## Verify messages arrived in Kafka items topic\n\nWe can use the Kafdrop tool to go to the `items` topic as illustrated below. The tool can be started via the `./startKafdrop.sh` command under the `infrastructure` folder after setting a `kakfa.properties` file as:\n\n```properties\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nssl.enabled.protocols=TLSv1.2\nssl.endpoint.identification.algorithm=HTTPS\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"APIKEY\";\n```\n\n![](./images/kafdrop.png)\n","type":"Mdx","contentDigest":"33e96d5125700e2052eae394365bcafa","counter":637,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Connect to RabbitMQ Source Connector\ndescription: Apache Kafka to RabbitMQ Source Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites</AnchorLink>\n<AnchorLink>Start the backend environment</AnchorLink>\n<AnchorLink>Verify the RabbitMQ settings</AnchorLink>\n<AnchorLink>Configure the kafka connector for Rabbitmq source</AnchorLink>\n<AnchorLink>Generate sale messages</AnchorLink>\n<AnchorLink>Verify messages arrived in Kafka items topic</AnchorLink>\n</AnchorLinks>\n\nThis hands-on lab demonstrates how to use IBM RabbitMQ source connector to inject message to Event Streams On Premise. \nWe are using the [IBM messaging github: source connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source). The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe code, configurations and detail readme instructions are in this repository that you should clone:\n\n```shell\ngit clone https://github.com/jbcodeforce/eda-kconnect-lab\n```\n\nTo send message to RabbitMQ, we have implemented a simple simulator, to send item sale messages for one of the stores. The code is under [store-sale-producer](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/store-sale-producer) folder and we have also uploaded the image into dockerhub.\n\nFor quick start and demonstration the [github repository for this lab](https://github.com/jbcodeforce/eda-kconnect-lab) includes a docker compose file [RabbitMQ-Kconnect-compose.yaml](https://github.com/jbcodeforce/eda-kconnect-lab/blob/master/infrastructure/RabbitMQ-Kconnect-compose.yaml) under the `infrastructure` folder to run Rabbit MQ and Kafka Connect together on your local computer. We recommend to follow the instructions in the [store-sale-producer readme](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/store-sale-producer) file.\n\nBut we want to also run and deploy this solution on OpenShift and Event Streams on Premise.\n\n## Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\n If you need to rebuild the image the following commands may be done:\n\n```shell\n./mvnw clean package -Dquarkus.container-image.build=true -Dquarkus.container-image.group=ibmcase -Dquarkus.container-image.tag=1.0.0\n```\n\n## Start the backend environment\n\nIn this section we start RabbitMQ, Kafka Connect and the Store Item Sale generator app. This app exposes a REST api to generate items sale operations that happen in a predefined set of stores.\n\nUnder the infrastructure folder use the command: `docker-compose -f RabbitMQ-Kconnect-compose.yaml up`.\n\nThe trace includes RabbitMQ, storeSaleGenerator_1 and Kafka connect logs. Here is a small extract of important messages:\n\n```shell\nrabbitmq_1            | 2020-06-17 06:12:58.293 [info] <0.9.0> Server startup complete; 4 plugins started.\nrabbitmq_1            |  * rabbitmq_management\nrabbitmq_1            |  * rabbitmq_web_dispatch\nrabbitmq_1            |  * rabbitmq_management_agent\nrabbitmq_1            |  * rabbitmq_amqp1_0\nrabbitmq_1            |  completed with 4 plugins.\n....\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,056 INFO  [io.quarkus] (main) Profile prod activated.\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,057 INFO  [io.quarkus] (main) Installed features: [cdi, mutiny, resteasy, resteasy-jsonb, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-amqp, vertx]\n```\n\n## Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:1567/](http://localhost:1567/) using the guest/guest login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nIf in the Admin tab you do not see **rabbit-user** listed do the following:\n\n* Add a new admin user: **rabbit-user/rabbit-pass** using the Admin tab. Enable the virtual host to be '/'.\n\n![](./images/rabbitmq-user.png)\n\n\nGo to the Queue tab and add `items` queue with default parameters:\n\n![8](./images/rabbitmq-item-queue.png)\n\n\nWith the following result\n\n![9](./images/rabbitmq-item-queue-2.png)\n\n\n## Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n## Generate sale messages\n\nThe Store application has an OpenAPI [http://localhost:8080/swagger-ui/](http://localhost:8080/swagger-ui/) definition to send messages:  `/stores/start/2` api:\n\n![5](./images/store-app-1.png)\n\nOr use `curl -X POST http://localhost:8080/stores/start/2`\n\nIn the trace you should see something like:\n\n```shell\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":0,\"storeName\":\"SF02\",\"itemCode\":\"IT07\",\"quantity\":7,\"price\":46.79320631709398}\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":1,\"storeName\":\"NYC01\",\"itemCode\":\"IT00\",\"quantity\":7,\"price\":0.7764381649099172}\n```\n## Verify messages arrived in Kafka items topic\n\nWe can use the Kafdrop tool to go to the `items` topic as illustrated below. The tool can be started via the `./startKafdrop.sh` command under the `infrastructure` folder after setting a `kakfa.properties` file as:\n\n```properties\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nssl.enabled.protocols=TLSv1.2\nssl.endpoint.identification.algorithm=HTTPS\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"APIKEY\";\n```\n\n![](./images/kafdrop.png)\n","frontmatter":{"title":"Kafka Connect to RabbitMQ Source Connector","description":"Apache Kafka to RabbitMQ Source Connector usecase"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-rabbitmq/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}