{"componentChunkName":"component---src-pages-use-cases-connect-rabbitmq-index-mdx","path":"/use-cases/connect-rabbitmq/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to RabbitMQ Source Connector","description":"Apache Kafka to RabbitMQ Source Connector usecase"},"relativePagePath":"/use-cases/connect-rabbitmq/index.mdx","titleType":"append","MdxNode":{"id":"c10c2cb0-8485-57da-9499-c8cf408f8e51","children":[],"parent":"d257ca0d-14bb-550b-8afa-1f34b2a6487d","internal":{"content":"---\ntitle: Kafka Connect to RabbitMQ Source Connector\ndescription: Apache Kafka to RabbitMQ Source Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 11/10/2020</strong> Lab completed on local computer. Instruction not completed for OpenShift deployment.\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Just run it!</AnchorLink>\n<AnchorLink>Deploy on OpenShift</AnchorLink>\n</AnchorLinks>\n\nThis hands-on lab demonstrates how to use IBM RabbitMQ Kafka source connector to inject message to Event Streams On Premise or any Kafka cluster. \nWe are using the [IBM messaging github: source Kafka connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source) open sourced component. The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe following diagram illustrates the component of this demo / lab:\n\n![0](./images/comp-view.png)\n\nThe configurations used in this use case are in the `refarch-eda-tools` repository that you should clone:\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n```\n\nunder the [labs/rabbitmq-source-lab](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/rabbitmq-source-lab) folder.\n\n## Just run it!\n\nFor demonstration purpose, we have defined a docker compose file and a set of docker images ready to use. So it will be easier to demonstrate the scenario of sending messages to Rabbit MQ `items` queue, have a Kafka connector configured and a simple Kafka consumer which consumes Json doc from the `items` Kafka topic. \n\nTo send message to RabbitMQ, we have implemented a simple store sale simulator, which sends item sale messages for a set of stores. The code is under [eda-store-simulator repository](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) and we have also uploaded the image [into dockerhub ibmcase/eda-store-simulator](https://hub.docker.com/repository/docker/ibmcase/eda-store-simulator).\n\n### Start backend services\n\nTo quickly start a local demonstration environment, the [github repository for this lab](https://github.com/ibm-cloud-architecture/refarch-eda-tools) includes a [docker compose file](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/rabbitmq-source-lab/docker-compose.yaml) under the `labs/rabbitmq-source-lab` folder.\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\ncd refarch-eda-tools/labs/rabbitmq-source-lab\ndocker-compose up -d\ndocker ps \n\nIMAGE                              NAMES                            PORT                  \nibmcase/kconnect:v1.0.0            rabbitmq-source-lab_kconnect_1   0.0.0.0:8083->8083/  \nstrimzi/kafka:latest-kafka-2.6.0   rabbitmq-source-lab_kafka_1      0.0.0.0:9092->9092/\nibmcase/eda-store-simulator        rabbitmq-source-lab_simulator_1  0.0.0.0:8080->8080/\nrabbitmq:3-management              rabbitmq-source-lab_rabbitmq_1   0.0.0.0:5672->5672/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp   \nstrimzi/kafka:latest-kafka-2.6.0   rabbitmq-source-lab_zookeeper_1 0.0.0.0:2181->2181/                                                                                      \n```\n\n### Deploy source connector\n\nOnce the different containers are started, you can deploy the RabbitMQ source connector using the following POST\n\n```shell\n# under the folder use the curl to post configuration\n./sendRMQSrcConfig.sh localhost:8083\n\n# Trace should look like\n{\"name\":\"RabbitMQSourceConnector\",\n\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n\"tasks.max\":\"1\",\n\"kafka.topic\":\"items\",\n\"rabbitmq.host\":\"rabbitmq\",\n\"rabbitmq.port\":\"5672\",\n\"rabbitmq.queue\":\"items\",\n\"rabbitmq.username\":\"rabbit-user\",\n\"rabbitmq.password\":\"rabbit-pass\",\n\"rabbitmq.prefetch.count\":\"500\",\n\"rabbitmq.automatic.recovery.enabled\":\"true\",\n\"rabbitmq.network.recovery.interval.ms\":\"10000\",\n\"rabbitmq.topology.recovery.enabled\":\"true\",\n\"name\":\"RabbitMQSourceConnector\"},\n\"tasks\":[],\n\"type\":\"source\"}\n```\n\n* And then looking at the deployed connectors using [http://localhost:8083/connectors](http://localhost:8083/connectors) or the RabbitMQ connector with [http://localhost:8083/connectors/RabbitMQSourceConnector](http://localhost:8083/connectors/RabbitMQSourceConnector) to get the same response as the above json.\n\n* Create `items` topic into Kafka using the script:\n\n```shell\n./createTopics.sh\n```\n\n* Send some records using the simulator at the [http://localhost:8080/#/simulator](http://localhost:8080/#/simulator) URL by first selecting RabbitMQ button and the number of message to send: \n\n ![1](./images/store-simul-rmq.png)\n\n\n* Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:15672/](http://localhost:15672/) using the rabbit-user/rabbit-pass login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nGo to the Queue tab and select the `items` queue:\n\n![9](./images/rabbitmq-item-queue-2.png)\n\nAnd then get 1 message from the queue:\n\n![10](./images/rabbitmq-item-msgs.png)\n\n* Now verify the Kafka connect-* topics are created successfully:\n\n ```shell\n docker run -ti --network  rabbitmq-source-lab_default strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list\"\n # you should get at least\n __consumer_offsets\n connect-configs\n connect-offsets\n connect-status\n items\n ```\n\n* Verify the message arrived to the `items` in kafka topic:\n\n ```shell\n docker run -ti --network  rabbitmq-source-lab_default  strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic items --from-beginning\"\n\n # You may have a trace like which maps the messages sent \n \"{\\\"id\\\":0,\\\"price\\\":11.0,\\\"quantity\\\":6,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_3\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395325\\\",\\\"type\\\":\\\"SALE\\\"}\"\n\"{\\\"id\\\":1,\\\"price\\\":68.5,\\\"quantity\\\":0,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_1\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395447\\\",\\\"type\\\":\\\"SALE\\\"}\"\n ```\n\nYou validated the solution works end to end. Now we can try to deploy those components to OpenShift.\n\n## Deploy on OpenShift\n\nIn this section, you will deploy the Kafka connector and Rabbit MQ on OpenShift using Event Streams on Premise as Kafka cluster.\n\n### Pre-requisites\n\n* To run the demonstration be sure to have the event store simulator deployed: [see instructions here.]()\n\n_Pull in necessary pre-req from the [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites), like for example the deployment of the store simulator._\n\n### Deploy RabbitMQ using operators\n\nSee [the installation instructions](https://www.rabbitmq.com/kubernetes/operator/install-operator.html) to get a RabbitMQ **operator** installed, or use our manifests from the [refarch-eda-tools](https://github.com/ibm-cloud-architecture/refarch-eda-tools) repository, which will set user 1000 and 999 for OpenShift deployment. The RabbitMQ cluster operator runs as user ID `1000`. The RabbitMQ pod runs the RabbitMQ container as user ID `999` and an init container as user ID `0`.\n\n* Create a dedicated project to run RabbitMQ operator to observe any projects, and install CRD, service account, RBAC role, deployment... \n\n ```shell\n oc new-project rabbitmq-system\n # under the labs/rabbitmq-source-lab folder do\n kubectl apply -f cluster-operator.yml\n\n kubectl get customresourcedefinitions.apiextensions.k8s.io\n ```\n\n* Create a second project for RabbitMQ cluster. It is recommended to have separate project for RabbitMQ cluster.\nModify the namespace to be able to deploy RabbitMQ cluster into it. So we need to get user 999 authorized. For that we specify a security constraint named: `rabbitmq-cluster`\n\n ```shell\n  #Define a Security Context Constraint\n  oc apply -f rmq-scc.yaml\n ```\n\n* Update the name space to get openshift scc userid and groupid range. You can do that in two ways, by directly editing the namespace definition:\n\n ```shell\n  oc edit namespace rabbitmq-cluster\n  # Then add the following annotations:\n  annotations:\n    openshift.io/sa.scc.supplemental-groups: 999/1\n    openshift.io/sa.scc.uid-range: 0-999/1\n ```\n\n Or by using our custom cluster definition: `oc apply -f rmq-cluster-ns.yaml`\n\n*  For every RabbitMQ cluster assign the previously created security context constraint to the cluster's service account: ` oc adm policy add-scc-to-user rabbitmq-cluster -z rmqcluster-rabbitmq-server`\n\n* Finally create a cluster instance: `oc apply -f rmq-cluster.yaml`.\n* Define a route on the client service on port 15672\n* Get the admin user and password as defined in the secrets\n\n ```shell\n  oc describe secret rmqcluster-rabbitmq-default-user\n  # Get user name\n  oc get secret rmqcluster-rabbitmq-default-user  -o jsonpath='{.data.username}' | base64 --decode \n  # Get password\n  oc get secret rmqcluster-rabbitmq-default-user  -o jsonpath='{.data.password}' | base64 --decode \n ```\n\nTo validate the RabbitMQ cluster runs correctly you can use the RabbitMQ console, and add the `items` queue.\n\n## Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.port\": 5672,\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n","type":"Mdx","contentDigest":"1579e667c64b82d920a9142d7fc8e6cc","owner":"gatsby-plugin-mdx","counter":704},"frontmatter":{"title":"Kafka Connect to RabbitMQ Source Connector","description":"Apache Kafka to RabbitMQ Source Connector usecase"},"exports":{},"rawBody":"---\ntitle: Kafka Connect to RabbitMQ Source Connector\ndescription: Apache Kafka to RabbitMQ Source Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 11/10/2020</strong> Lab completed on local computer. Instruction not completed for OpenShift deployment.\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Just run it!</AnchorLink>\n<AnchorLink>Deploy on OpenShift</AnchorLink>\n</AnchorLinks>\n\nThis hands-on lab demonstrates how to use IBM RabbitMQ Kafka source connector to inject message to Event Streams On Premise or any Kafka cluster. \nWe are using the [IBM messaging github: source Kafka connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source) open sourced component. The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe following diagram illustrates the component of this demo / lab:\n\n![0](./images/comp-view.png)\n\nThe configurations used in this use case are in the `refarch-eda-tools` repository that you should clone:\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n```\n\nunder the [labs/rabbitmq-source-lab](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/rabbitmq-source-lab) folder.\n\n## Just run it!\n\nFor demonstration purpose, we have defined a docker compose file and a set of docker images ready to use. So it will be easier to demonstrate the scenario of sending messages to Rabbit MQ `items` queue, have a Kafka connector configured and a simple Kafka consumer which consumes Json doc from the `items` Kafka topic. \n\nTo send message to RabbitMQ, we have implemented a simple store sale simulator, which sends item sale messages for a set of stores. The code is under [eda-store-simulator repository](https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator) and we have also uploaded the image [into dockerhub ibmcase/eda-store-simulator](https://hub.docker.com/repository/docker/ibmcase/eda-store-simulator).\n\n### Start backend services\n\nTo quickly start a local demonstration environment, the [github repository for this lab](https://github.com/ibm-cloud-architecture/refarch-eda-tools) includes a [docker compose file](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/rabbitmq-source-lab/docker-compose.yaml) under the `labs/rabbitmq-source-lab` folder.\n\n```shell\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\ncd refarch-eda-tools/labs/rabbitmq-source-lab\ndocker-compose up -d\ndocker ps \n\nIMAGE                              NAMES                            PORT                  \nibmcase/kconnect:v1.0.0            rabbitmq-source-lab_kconnect_1   0.0.0.0:8083->8083/  \nstrimzi/kafka:latest-kafka-2.6.0   rabbitmq-source-lab_kafka_1      0.0.0.0:9092->9092/\nibmcase/eda-store-simulator        rabbitmq-source-lab_simulator_1  0.0.0.0:8080->8080/\nrabbitmq:3-management              rabbitmq-source-lab_rabbitmq_1   0.0.0.0:5672->5672/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp   \nstrimzi/kafka:latest-kafka-2.6.0   rabbitmq-source-lab_zookeeper_1 0.0.0.0:2181->2181/                                                                                      \n```\n\n### Deploy source connector\n\nOnce the different containers are started, you can deploy the RabbitMQ source connector using the following POST\n\n```shell\n# under the folder use the curl to post configuration\n./sendRMQSrcConfig.sh localhost:8083\n\n# Trace should look like\n{\"name\":\"RabbitMQSourceConnector\",\n\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n\"tasks.max\":\"1\",\n\"kafka.topic\":\"items\",\n\"rabbitmq.host\":\"rabbitmq\",\n\"rabbitmq.port\":\"5672\",\n\"rabbitmq.queue\":\"items\",\n\"rabbitmq.username\":\"rabbit-user\",\n\"rabbitmq.password\":\"rabbit-pass\",\n\"rabbitmq.prefetch.count\":\"500\",\n\"rabbitmq.automatic.recovery.enabled\":\"true\",\n\"rabbitmq.network.recovery.interval.ms\":\"10000\",\n\"rabbitmq.topology.recovery.enabled\":\"true\",\n\"name\":\"RabbitMQSourceConnector\"},\n\"tasks\":[],\n\"type\":\"source\"}\n```\n\n* And then looking at the deployed connectors using [http://localhost:8083/connectors](http://localhost:8083/connectors) or the RabbitMQ connector with [http://localhost:8083/connectors/RabbitMQSourceConnector](http://localhost:8083/connectors/RabbitMQSourceConnector) to get the same response as the above json.\n\n* Create `items` topic into Kafka using the script:\n\n```shell\n./createTopics.sh\n```\n\n* Send some records using the simulator at the [http://localhost:8080/#/simulator](http://localhost:8080/#/simulator) URL by first selecting RabbitMQ button and the number of message to send: \n\n ![1](./images/store-simul-rmq.png)\n\n\n* Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:15672/](http://localhost:15672/) using the rabbit-user/rabbit-pass login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nGo to the Queue tab and select the `items` queue:\n\n![9](./images/rabbitmq-item-queue-2.png)\n\nAnd then get 1 message from the queue:\n\n![10](./images/rabbitmq-item-msgs.png)\n\n* Now verify the Kafka connect-* topics are created successfully:\n\n ```shell\n docker run -ti --network  rabbitmq-source-lab_default strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list\"\n # you should get at least\n __consumer_offsets\n connect-configs\n connect-offsets\n connect-status\n items\n ```\n\n* Verify the message arrived to the `items` in kafka topic:\n\n ```shell\n docker run -ti --network  rabbitmq-source-lab_default  strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic items --from-beginning\"\n\n # You may have a trace like which maps the messages sent \n \"{\\\"id\\\":0,\\\"price\\\":11.0,\\\"quantity\\\":6,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_3\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395325\\\",\\\"type\\\":\\\"SALE\\\"}\"\n\"{\\\"id\\\":1,\\\"price\\\":68.5,\\\"quantity\\\":0,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_1\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395447\\\",\\\"type\\\":\\\"SALE\\\"}\"\n ```\n\nYou validated the solution works end to end. Now we can try to deploy those components to OpenShift.\n\n## Deploy on OpenShift\n\nIn this section, you will deploy the Kafka connector and Rabbit MQ on OpenShift using Event Streams on Premise as Kafka cluster.\n\n### Pre-requisites\n\n* To run the demonstration be sure to have the event store simulator deployed: [see instructions here.]()\n\n_Pull in necessary pre-req from the [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites), like for example the deployment of the store simulator._\n\n### Deploy RabbitMQ using operators\n\nSee [the installation instructions](https://www.rabbitmq.com/kubernetes/operator/install-operator.html) to get a RabbitMQ **operator** installed, or use our manifests from the [refarch-eda-tools](https://github.com/ibm-cloud-architecture/refarch-eda-tools) repository, which will set user 1000 and 999 for OpenShift deployment. The RabbitMQ cluster operator runs as user ID `1000`. The RabbitMQ pod runs the RabbitMQ container as user ID `999` and an init container as user ID `0`.\n\n* Create a dedicated project to run RabbitMQ operator to observe any projects, and install CRD, service account, RBAC role, deployment... \n\n ```shell\n oc new-project rabbitmq-system\n # under the labs/rabbitmq-source-lab folder do\n kubectl apply -f cluster-operator.yml\n\n kubectl get customresourcedefinitions.apiextensions.k8s.io\n ```\n\n* Create a second project for RabbitMQ cluster. It is recommended to have separate project for RabbitMQ cluster.\nModify the namespace to be able to deploy RabbitMQ cluster into it. So we need to get user 999 authorized. For that we specify a security constraint named: `rabbitmq-cluster`\n\n ```shell\n  #Define a Security Context Constraint\n  oc apply -f rmq-scc.yaml\n ```\n\n* Update the name space to get openshift scc userid and groupid range. You can do that in two ways, by directly editing the namespace definition:\n\n ```shell\n  oc edit namespace rabbitmq-cluster\n  # Then add the following annotations:\n  annotations:\n    openshift.io/sa.scc.supplemental-groups: 999/1\n    openshift.io/sa.scc.uid-range: 0-999/1\n ```\n\n Or by using our custom cluster definition: `oc apply -f rmq-cluster-ns.yaml`\n\n*  For every RabbitMQ cluster assign the previously created security context constraint to the cluster's service account: ` oc adm policy add-scc-to-user rabbitmq-cluster -z rmqcluster-rabbitmq-server`\n\n* Finally create a cluster instance: `oc apply -f rmq-cluster.yaml`.\n* Define a route on the client service on port 15672\n* Get the admin user and password as defined in the secrets\n\n ```shell\n  oc describe secret rmqcluster-rabbitmq-default-user\n  # Get user name\n  oc get secret rmqcluster-rabbitmq-default-user  -o jsonpath='{.data.username}' | base64 --decode \n  # Get password\n  oc get secret rmqcluster-rabbitmq-default-user  -o jsonpath='{.data.password}' | base64 --decode \n ```\n\nTo validate the RabbitMQ cluster runs correctly you can use the RabbitMQ console, and add the `items` queue.\n\n## Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.port\": 5672,\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-rabbitmq/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}