{"componentChunkName":"component---src-pages-use-cases-connect-jdbc-index-mdx","path":"/use-cases/connect-jdbc/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to JDBC Sink","description":"Apache Kafka to JDBC-based database Sink Connector usecase"},"relativePagePath":"/use-cases/connect-jdbc/index.mdx","titleType":"append","MdxNode":{"id":"0cb2d243-355a-5720-b435-83c8a7e93f4d","children":[],"parent":"b7e59152-c613-590c-b450-6258076bd0eb","internal":{"content":"---\ntitle: Kafka Connect to JDBC Sink\ndescription: Apache Kafka to JDBC-based database Sink Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 11/03/2020</strong> - Lab works with one small issue needs to be fixed in test or jdbc connector.\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites</AnchorLink>\n<AnchorLink>Run the Kafka Connector in distributed mode</AnchorLink>\n<AnchorLink>Upload the DB2 sink definition</AnchorLink>\n<AnchorLink>Generate some records</AnchorLink>\n<AnchorLink>Verify records are uploaded into the Inventory database</AnchorLink>\n</AnchorLinks>\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from a kafka topic and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n## Pre-requisites\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for CRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/ibm-cloud-architecture/refarch-eda-inventory-app/). At the application starts, stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-inventory-app/master/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-inventory-app/master/src/main/resources/import.sql) from `src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n\n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n## Run the Kafka Connector in distributed mode\n\nIn the [refarch-eda-tools repository](https://github.com/ibm-cloud-architecture/refarch-eda-tools) the `labs/jdbc-sink-lab` folder includes a docker compose file to run the lab with kafka broker, zookeeper, the kafka connector running in distrbuted mode and an inventory app to get records from DB.\n\n```shell\n# \ndocker-compose up -d\n```\n\n## Upload the DB2 sink definition\n\nUpdate the file `db2-sink-config.json` with the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format` with the username.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh <url-kafka-connect>` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhost:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n## Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```shell\nProduce to the topic inventory\n\nending -> {'schema': {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'storeName'}, {'type': 'string', 'optional': False, 'field': 'sku'}, {'type': 'decimal', 'optional': False, 'field': 'id'}, {'type': 'decimal', 'optional': True, 'field': 'quantity'}, {'type': 'decimal', 'optional': True, 'field': 'price'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}], 'optional': False, 'name': 'Inventory'}, 'payload': {'storeName': 'Store_1', 'sku': 'Item_1', 'quantity': 16, 'price': 128, 'id': 0, 'timestamp': '05-Nov-2020 22:31:11'}}\nsending -> {'schema': {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'storeName'}, {'type': 'string', 'optional': False, 'field': 'sku'}, {'type': 'decimal', 'optional': False, 'field': 'id'}, {'type': 'decimal', 'optional': True, 'field': 'quantity'}, {'type': 'decimal', 'optional': True, 'field': 'price'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}], 'optional': False, 'name': 'Inventory'}, 'payload': {'storeName': 'Store_1', 'sku': 'Item_8', 'quantity': 13, 'price': 38, 'id': 1, 'timestamp': '05-Nov-2020 22:31:11'}}\n```\n\n## Verify records are uploaded into the Inventory database\n\nYou can use two approaches to get the database, by using the inventory app or using the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n\nThe swagger is visible at the address [http://localhost:8080/swagger-ui](http://localhost:8080/swagger-ui) and get to the URL [http://localhost:8080/inventory](http://localhost:8080/inventory)\n","type":"Mdx","contentDigest":"69beb006d618bc5cee34115524ac8104","owner":"gatsby-plugin-mdx","counter":722},"frontmatter":{"title":"Kafka Connect to JDBC Sink","description":"Apache Kafka to JDBC-based database Sink Connector usecase"},"exports":{},"rawBody":"---\ntitle: Kafka Connect to JDBC Sink\ndescription: Apache Kafka to JDBC-based database Sink Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>Updated 11/03/2020</strong> - Lab works with one small issue needs to be fixed in test or jdbc connector.\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites</AnchorLink>\n<AnchorLink>Run the Kafka Connector in distributed mode</AnchorLink>\n<AnchorLink>Upload the DB2 sink definition</AnchorLink>\n<AnchorLink>Generate some records</AnchorLink>\n<AnchorLink>Verify records are uploaded into the Inventory database</AnchorLink>\n</AnchorLinks>\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from a kafka topic and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n## Pre-requisites\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for CRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/ibm-cloud-architecture/refarch-eda-inventory-app/). At the application starts, stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-inventory-app/master/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-inventory-app/master/src/main/resources/import.sql) from `src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n\n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n## Run the Kafka Connector in distributed mode\n\nIn the [refarch-eda-tools repository](https://github.com/ibm-cloud-architecture/refarch-eda-tools) the `labs/jdbc-sink-lab` folder includes a docker compose file to run the lab with kafka broker, zookeeper, the kafka connector running in distrbuted mode and an inventory app to get records from DB.\n\n```shell\n# \ndocker-compose up -d\n```\n\n## Upload the DB2 sink definition\n\nUpdate the file `db2-sink-config.json` with the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format` with the username.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh <url-kafka-connect>` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhost:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n## Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```shell\nProduce to the topic inventory\n\nending -> {'schema': {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'storeName'}, {'type': 'string', 'optional': False, 'field': 'sku'}, {'type': 'decimal', 'optional': False, 'field': 'id'}, {'type': 'decimal', 'optional': True, 'field': 'quantity'}, {'type': 'decimal', 'optional': True, 'field': 'price'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}], 'optional': False, 'name': 'Inventory'}, 'payload': {'storeName': 'Store_1', 'sku': 'Item_1', 'quantity': 16, 'price': 128, 'id': 0, 'timestamp': '05-Nov-2020 22:31:11'}}\nsending -> {'schema': {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'storeName'}, {'type': 'string', 'optional': False, 'field': 'sku'}, {'type': 'decimal', 'optional': False, 'field': 'id'}, {'type': 'decimal', 'optional': True, 'field': 'quantity'}, {'type': 'decimal', 'optional': True, 'field': 'price'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}], 'optional': False, 'name': 'Inventory'}, 'payload': {'storeName': 'Store_1', 'sku': 'Item_8', 'quantity': 13, 'price': 38, 'id': 1, 'timestamp': '05-Nov-2020 22:31:11'}}\n```\n\n## Verify records are uploaded into the Inventory database\n\nYou can use two approaches to get the database, by using the inventory app or using the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n\nThe swagger is visible at the address [http://localhost:8080/swagger-ui](http://localhost:8080/swagger-ui) and get to the URL [http://localhost:8080/inventory](http://localhost:8080/inventory)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-jdbc/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}