{"componentChunkName":"component---src-pages-use-cases-connect-jdbc-index-mdx","path":"/use-cases/connect-jdbc/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect to JDBC Sink","description":"Apache Kafka to JDBC-based database Sink Connector usecase"},"relativePagePath":"/use-cases/connect-jdbc/index.mdx","titleType":"append","MdxNode":{"id":"0cb2d243-355a-5720-b435-83c8a7e93f4d","children":[],"parent":"b7e59152-c613-590c-b450-6258076bd0eb","internal":{"content":"---\ntitle: Kafka Connect to JDBC Sink\ndescription: Apache Kafka to JDBC-based database Sink Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites</AnchorLink>\n<AnchorLink>Run the Kafka Connector in distributed mode</AnchorLink>\n<AnchorLink>Upload the DB2 sink definition</AnchorLink>\n<AnchorLink>Generate some records</AnchorLink>\n<AnchorLink>Verify records are uploaded into the Inventory database</AnchorLink>\n</AnchorLinks>\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from the `inventory topic` and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n## Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for cRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) under the `inventory-app` folder of [this repository](https://github.com/jbcodeforce/eda-kconnect-lab). At the application starts stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/import.sql) from `inventory-app/src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n\n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n## Run the Kafka Connector in distributed mode\n\nThe docker image built in the [setup](#scenario-setup) has the configuration for kafka connect distributed cluster, we need in this scenario to start the connector and upload the DB2 Sink connector definition. To start it, run the script `./createOrStartKconnect.sh start` under `kconnect` folder.\n\n## Upload the DB2 sink definition\n\nRename the file `db2-sink-config-TMPL.json` as `db2-sink-config.json` and modify the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format`.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh url-kafka-connect` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhodt:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n## Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```\nProduce to the topic inventory\n[KafkaProducer] - This is the configuration for the producer:\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProducerInventory', 'acks': 0, 'request.timeout.ms': 10000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5'}\nsending -> {'storeName': 'LA02', 'itemCode': 'IT09', 'id': 0, 'timestamp': 1591211295.617515}\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'PT02', 'itemCode': 'IT00', 'id': 1, 'timestamp': 1591211296.7727954}\n[KafkaProducer] - Message delivered to inventory [0]\n\n```\n\n## Verify records are uploaded into the Inventory database\n\nUsing the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n","type":"Mdx","contentDigest":"a8b698ac3b413dc87bddb7da9f676ff9","counter":629,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Connect to JDBC Sink\ndescription: Apache Kafka to JDBC-based database Sink Connector usecase\n---\n\n<InlineNotification kind=\"warning\">\n<strong>TODO</strong> - Work in progress\n</InlineNotification>\n\n<AnchorLinks>\n<AnchorLink>Pre-requisites</AnchorLink>\n<AnchorLink>Run the Kafka Connector in distributed mode</AnchorLink>\n<AnchorLink>Upload the DB2 sink definition</AnchorLink>\n<AnchorLink>Generate some records</AnchorLink>\n<AnchorLink>Verify records are uploaded into the Inventory database</AnchorLink>\n</AnchorLinks>\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from the `inventory topic` and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n## Pre-requisites\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong></InlineNotification>\n\n_Pull in necessary pre-req context from [Realtime Inventory Pre-reqs](/scenarios/realtime-inventory/#general-pre-requisites)._\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for cRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) under the `inventory-app` folder of [this repository](https://github.com/jbcodeforce/eda-kconnect-lab). At the application starts stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/import.sql) from `inventory-app/src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n\n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n## Run the Kafka Connector in distributed mode\n\nThe docker image built in the [setup](#scenario-setup) has the configuration for kafka connect distributed cluster, we need in this scenario to start the connector and upload the DB2 Sink connector definition. To start it, run the script `./createOrStartKconnect.sh start` under `kconnect` folder.\n\n## Upload the DB2 sink definition\n\nRename the file `db2-sink-config-TMPL.json` as `db2-sink-config.json` and modify the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format`.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh url-kafka-connect` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhodt:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n## Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```\nProduce to the topic inventory\n[KafkaProducer] - This is the configuration for the producer:\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProducerInventory', 'acks': 0, 'request.timeout.ms': 10000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5'}\nsending -> {'storeName': 'LA02', 'itemCode': 'IT09', 'id': 0, 'timestamp': 1591211295.617515}\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'PT02', 'itemCode': 'IT00', 'id': 1, 'timestamp': 1591211296.7727954}\n[KafkaProducer] - Message delivered to inventory [0]\n\n```\n\n## Verify records are uploaded into the Inventory database\n\nUsing the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n","frontmatter":{"title":"Kafka Connect to JDBC Sink","description":"Apache Kafka to JDBC-based database Sink Connector usecase"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/connect-jdbc/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}