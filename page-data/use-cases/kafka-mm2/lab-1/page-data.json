{"componentChunkName":"component---src-pages-use-cases-kafka-mm-2-lab-1-index-mdx","path":"/use-cases/kafka-mm2/lab-1/","result":{"pageContext":{"frontmatter":{"title":"Mirror Maker 2 - Event Streams Service to Local Kafka - Lab 1","description":"Using MM2 from Event Streams on Cloud to local Kafka 2.6 cluster."},"relativePagePath":"/use-cases/kafka-mm2/lab-1/index.mdx","titleType":"append","MdxNode":{"id":"554bf244-6fd6-53f9-b365-706c00edf69b","children":[],"parent":"38b00bce-ac82-509b-b163-c3d892026d4b","internal":{"content":"---\ntitle: Mirror Maker 2 - Event Streams Service to Local Kafka - Lab 1\ndescription: Using MM2 from Event Streams on Cloud to local Kafka 2.6 cluster.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Start the local Kafka cluster</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nUpdated 01/08/2021\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on IBM Cloud as managed service, and the target is a local Kafka cluster running with docker compose: it simulates an on-premise deployment. The figure below presents the components used:\n\n![1](../images/mm2-lab1.png)\n\n1. Mirror maker 2 runs in standalone mode on local server\n1. A python producer write to `products` topic defined on Evenstreams on IBM Cloud \n1. A consumer using Kafka console consumer tool to validate the replicated records from the 'products' topic\n\nAs a pre-requisite you need to run your local cluster by using the docker compose as introduced in [this note](/use-cases/kafka-mm2/#pre-requisites).\n\n## Start the local Kafka cluster\n\nIn the `refarch-eda-tools/labs/mirror-maker2/es-ic-to-local` folder there is a [docker compose file](https://raw.githubusercontent.com/refarch-eda-tools/labs/mirror-maker2/es-ic-to-local/master/docker-compose.yml) to start a local three brokers cluster with one Zookeeper node.\n\n1. In one Terminal window, start the local cluster using the command:\n\n```shell\ndocker-compose up -d\n```\n\nThe data are persisted on the local disk within the folder named `kafka-data`.\n\nYour local environment is up and running.\n\n## Start Mirror Maker 2\n\n1. Rename the `.env-tmpl` file to `.env`\n1. From Event Streams on Cloud > Service Credentials, get the brokers address and the APIKEY. If needed read [this note](/technology/event-streams/security/#authentication-with-api-keys).\n1. If not done already, create a `products` topic (with one partition) in the EventStreams on Cloud cluster using the management console. See [this note](/technology/event-streams/es-cloud/#create-topic) if needed, to see how to do it.\n1. Modify this `.env` file to set environment variables for Source Event Streams cluster brokers address and APIKEY.\n\n ```shell\n ES_IC_BROKERS=broker-0-q.....cloud.ibm.com:9093\n ES_IC_USER=token\n ES_IC_PASSWORD=\"<replace with apikey from event streams service credentials>\"\n ES_IC_SASL_MECHANISM=PLAIN\n ES_IC_LOGIN_MODULE=org.apache.kafka.common.security.plain.PlainLoginModule\n ```\n1. To configure Mirror Maker 2 in standalone mode, we need to define a `mm2.properties` file. We have define a template file which will be used by the script that launch Mirror Maker 2. The template looks like the following declaration:\n \n    ```properties\n    clusters=es-ic, target\n    es-ic.bootstrap.servers=KAFKA_SOURCE_BROKERS\n    target.bootstrap.servers=KAFKA_TARGET_BROKERS\n\n    es-ic.security.protocol=SASL_SSL\n    es-ic.ssl.protocol=TLSv1.2\n    es-ic.ssl.endpoint.identification.algorithm=https\n    es-ic.sasl.mechanism=SOURCE_KAFKA_SASL_MECHANISM\n    es-ic.sasl.jaas.config=SOURCE_LOGIN_MODULE required username=KAFKA_SOURCE_USER password=KAFKA_SOURCE_PASSWORD;\n    sync.topic.acls.enabled=false\n    replication.factor=1\n    internal.topic.replication.factor=1\n    es-ic.offset.storage.topic=mm2-cluster-offsets\n    es-ic.configs.storage.topic=mm2-cluster-configs\n    es-ic.status.storage.topic=mm2-cluster-status\n    # enable and configure individual replication flows\n    es-ic->target.enabled=true\n    es-ic->target.topics=products\n    ```\n    \n  A lot of those properties are for the security settings. The `clusters` property defines the alias name for the source to target, and then the `es-ic->target.*` properties define the topic to replicate...\n\n1. Start Mirror Maker2 using the launch script:\n\n    ```shell\n    # In the  es-ic-to-local folder\n    ./launchMM2.sh\n    ```\n\n    This script updates the properties file from the environment variables defined in the `.env` file and starts a Kafka container with a command very similar as:\n\n    ```shell\n     docker run -ti --network es-ic-to-local_default -v $(pwd):/home -v $(pwd)/mirror-maker-2/logs:/opt/kafka/logs strimzi/kafka:latest-kafka-2.6.0 /bin/bash -c \"/opt/kafka/bin/connect-mirror-maker.sh /home/mirror-maker-2/es-to-local/mm2.properties\"\n    ```\n\n    The `mm2.properties` file is mounted in the `/home` within the container. \n    *The network argument is important to get the host names resolved and the connector to connect to Kafka Brokers*\n\n1. Verify the MM2 topics are created:\n\n ```shell\n docker exec -ti kafka1 /bin/bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9091 --list\"\n\n  __consumer_offsets\n  es-ic.checkpoints.internal\n  es-ic.products\n  es-ic.source.heartbeats\n  heartbeats\n  mm2-configs.es-ic.internal\n  mm2-offsets.es-ic.internal\n  mm2-status.es-ic.internal\n ```\n\n### Understanding MirrorMaker 2 trace \n\nA lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues.\n\nIf some messages happen with NO_LEADER for one of the topics, this means MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface.\n\nThen we can observe the following:\n * It creates a producer to the target cluster for the offsets topics: ` [Producer clientId=producer-1] Cluster ID: Bj7Ui3UPQaKtJx7HOkWxPw`\n * It creates consumer for the 25 offset topic partitions: `[Consumer clientId=consumer-mirrormaker2-cluster-1, groupId=mirrormaker2-cluster] Subscribed to partition(s): mirrormaker2-cluster-offsets-0,....`\n * One Kafka connect worker is started: `Worker started ... Starting KafkaBasedLog with topic mirrormaker2-cluster-status`\n * Create a producer and consumers for the ` mirrormaker2-cluster-status` topic for 5 partitions\n * Create another producer and consumer for the `mirrormaker2-cluster-config` topic\n\n * Create WorkerSourceTask{id=es-1->es-ic.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat.\n * WorkerSourceTask{id=es-1->es-ic.MirrorSourceConnector-0} for the topic to replicate\n\n## Start consumer from target cluster\n\nUse Apache Kafka tool like Console consumer to trace the message received on a topic\n\n```shell\ndocker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-ic.products --from-beginning\" \n```\n\n## Start Producer to source cluster\n\nWe are reusing a python environment as defined in the integration tests for the 'kcontainer' solution. [https://hub.docker.com/r/ibmcase/kcontainer-python](https://hub.docker.com/r/ibmcase/kcontainer-python).\n\nThis time the script is producing products data. Here are the steps to send 5 records.\n\n```shell\n# in the es-ic-to-local folder\n./sendProductRecords.sh\n```\n\nThe traces should look like:\n\n```\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '..hidden...'}\n{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n```\n\nValidate the replication is done from the consumer terminal\n\n  ```shell\n\n  { \"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  { \"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6,\"target_humidity_level\": 0.6,\"content_type\": 2}\n  { \"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  ```\nStop producer with Ctrl C.\n\n## Clean up\n\nYou are done with the lab, to stop everything:\n\n```shell\n# stop mirror maker 2\ndocker stop mm2\n# stop local kafka cluster\ndocker-compose down\nrm -r logs\nrm -r kafka-data/\n```\n\nOr run `cleanLab.sh`\n","type":"Mdx","contentDigest":"88d774955b9e938a44f7df923e782bb7","owner":"gatsby-plugin-mdx","counter":768},"frontmatter":{"title":"Mirror Maker 2 - Event Streams Service to Local Kafka - Lab 1","description":"Using MM2 from Event Streams on Cloud to local Kafka 2.6 cluster."},"exports":{},"rawBody":"---\ntitle: Mirror Maker 2 - Event Streams Service to Local Kafka - Lab 1\ndescription: Using MM2 from Event Streams on Cloud to local Kafka 2.6 cluster.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Start the local Kafka cluster</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nUpdated 01/08/2021\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on IBM Cloud as managed service, and the target is a local Kafka cluster running with docker compose: it simulates an on-premise deployment. The figure below presents the components used:\n\n![1](../images/mm2-lab1.png)\n\n1. Mirror maker 2 runs in standalone mode on local server\n1. A python producer write to `products` topic defined on Evenstreams on IBM Cloud \n1. A consumer using Kafka console consumer tool to validate the replicated records from the 'products' topic\n\nAs a pre-requisite you need to run your local cluster by using the docker compose as introduced in [this note](/use-cases/kafka-mm2/#pre-requisites).\n\n## Start the local Kafka cluster\n\nIn the `refarch-eda-tools/labs/mirror-maker2/es-ic-to-local` folder there is a [docker compose file](https://raw.githubusercontent.com/refarch-eda-tools/labs/mirror-maker2/es-ic-to-local/master/docker-compose.yml) to start a local three brokers cluster with one Zookeeper node.\n\n1. In one Terminal window, start the local cluster using the command:\n\n```shell\ndocker-compose up -d\n```\n\nThe data are persisted on the local disk within the folder named `kafka-data`.\n\nYour local environment is up and running.\n\n## Start Mirror Maker 2\n\n1. Rename the `.env-tmpl` file to `.env`\n1. From Event Streams on Cloud > Service Credentials, get the brokers address and the APIKEY. If needed read [this note](/technology/event-streams/security/#authentication-with-api-keys).\n1. If not done already, create a `products` topic (with one partition) in the EventStreams on Cloud cluster using the management console. See [this note](/technology/event-streams/es-cloud/#create-topic) if needed, to see how to do it.\n1. Modify this `.env` file to set environment variables for Source Event Streams cluster brokers address and APIKEY.\n\n ```shell\n ES_IC_BROKERS=broker-0-q.....cloud.ibm.com:9093\n ES_IC_USER=token\n ES_IC_PASSWORD=\"<replace with apikey from event streams service credentials>\"\n ES_IC_SASL_MECHANISM=PLAIN\n ES_IC_LOGIN_MODULE=org.apache.kafka.common.security.plain.PlainLoginModule\n ```\n1. To configure Mirror Maker 2 in standalone mode, we need to define a `mm2.properties` file. We have define a template file which will be used by the script that launch Mirror Maker 2. The template looks like the following declaration:\n \n    ```properties\n    clusters=es-ic, target\n    es-ic.bootstrap.servers=KAFKA_SOURCE_BROKERS\n    target.bootstrap.servers=KAFKA_TARGET_BROKERS\n\n    es-ic.security.protocol=SASL_SSL\n    es-ic.ssl.protocol=TLSv1.2\n    es-ic.ssl.endpoint.identification.algorithm=https\n    es-ic.sasl.mechanism=SOURCE_KAFKA_SASL_MECHANISM\n    es-ic.sasl.jaas.config=SOURCE_LOGIN_MODULE required username=KAFKA_SOURCE_USER password=KAFKA_SOURCE_PASSWORD;\n    sync.topic.acls.enabled=false\n    replication.factor=1\n    internal.topic.replication.factor=1\n    es-ic.offset.storage.topic=mm2-cluster-offsets\n    es-ic.configs.storage.topic=mm2-cluster-configs\n    es-ic.status.storage.topic=mm2-cluster-status\n    # enable and configure individual replication flows\n    es-ic->target.enabled=true\n    es-ic->target.topics=products\n    ```\n    \n  A lot of those properties are for the security settings. The `clusters` property defines the alias name for the source to target, and then the `es-ic->target.*` properties define the topic to replicate...\n\n1. Start Mirror Maker2 using the launch script:\n\n    ```shell\n    # In the  es-ic-to-local folder\n    ./launchMM2.sh\n    ```\n\n    This script updates the properties file from the environment variables defined in the `.env` file and starts a Kafka container with a command very similar as:\n\n    ```shell\n     docker run -ti --network es-ic-to-local_default -v $(pwd):/home -v $(pwd)/mirror-maker-2/logs:/opt/kafka/logs strimzi/kafka:latest-kafka-2.6.0 /bin/bash -c \"/opt/kafka/bin/connect-mirror-maker.sh /home/mirror-maker-2/es-to-local/mm2.properties\"\n    ```\n\n    The `mm2.properties` file is mounted in the `/home` within the container. \n    *The network argument is important to get the host names resolved and the connector to connect to Kafka Brokers*\n\n1. Verify the MM2 topics are created:\n\n ```shell\n docker exec -ti kafka1 /bin/bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9091 --list\"\n\n  __consumer_offsets\n  es-ic.checkpoints.internal\n  es-ic.products\n  es-ic.source.heartbeats\n  heartbeats\n  mm2-configs.es-ic.internal\n  mm2-offsets.es-ic.internal\n  mm2-status.es-ic.internal\n ```\n\n### Understanding MirrorMaker 2 trace \n\nA lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues.\n\nIf some messages happen with NO_LEADER for one of the topics, this means MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface.\n\nThen we can observe the following:\n * It creates a producer to the target cluster for the offsets topics: ` [Producer clientId=producer-1] Cluster ID: Bj7Ui3UPQaKtJx7HOkWxPw`\n * It creates consumer for the 25 offset topic partitions: `[Consumer clientId=consumer-mirrormaker2-cluster-1, groupId=mirrormaker2-cluster] Subscribed to partition(s): mirrormaker2-cluster-offsets-0,....`\n * One Kafka connect worker is started: `Worker started ... Starting KafkaBasedLog with topic mirrormaker2-cluster-status`\n * Create a producer and consumers for the ` mirrormaker2-cluster-status` topic for 5 partitions\n * Create another producer and consumer for the `mirrormaker2-cluster-config` topic\n\n * Create WorkerSourceTask{id=es-1->es-ic.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat.\n * WorkerSourceTask{id=es-1->es-ic.MirrorSourceConnector-0} for the topic to replicate\n\n## Start consumer from target cluster\n\nUse Apache Kafka tool like Console consumer to trace the message received on a topic\n\n```shell\ndocker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-ic.products --from-beginning\" \n```\n\n## Start Producer to source cluster\n\nWe are reusing a python environment as defined in the integration tests for the 'kcontainer' solution. [https://hub.docker.com/r/ibmcase/kcontainer-python](https://hub.docker.com/r/ibmcase/kcontainer-python).\n\nThis time the script is producing products data. Here are the steps to send 5 records.\n\n```shell\n# in the es-ic-to-local folder\n./sendProductRecords.sh\n```\n\nThe traces should look like:\n\n```\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '..hidden...'}\n{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n```\n\nValidate the replication is done from the consumer terminal\n\n  ```shell\n\n  { \"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  { \"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6,\"target_humidity_level\": 0.6,\"content_type\": 2}\n  { \"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  ```\nStop producer with Ctrl C.\n\n## Clean up\n\nYou are done with the lab, to stop everything:\n\n```shell\n# stop mirror maker 2\ndocker stop mm2\n# stop local kafka cluster\ndocker-compose down\nrm -r logs\nrm -r kafka-data/\n```\n\nOr run `cleanLab.sh`\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-mm2/lab-1/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}