{"componentChunkName":"component---src-pages-use-cases-kafka-mm-2-lab-1-index-mdx","path":"/use-cases/kafka-mm2/lab-1/","result":{"pageContext":{"frontmatter":{"title":"Kafka Mirror Maker 2 Lab 1","description":"Using MM2 from Event Streams on Cloud to local Kafka 2.5 cluster."},"relativePagePath":"/use-cases/kafka-mm2/lab-1/index.mdx","titleType":"append","MdxNode":{"id":"554bf244-6fd6-53f9-b365-706c00edf69b","children":[],"parent":"38b00bce-ac82-509b-b163-c3d892026d4b","internal":{"content":"---\ntitle: Kafka Mirror Maker 2 Lab 1\ndescription: Using MM2 from Event Streams on Cloud to local Kafka 2.5 cluster.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Start Kafka Connect</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n\n</AnchorLinks>\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on IBM Cloud cluster instance, and the target is a local Kafka cluster running with docker compose.\n\n![1](../images/mm2-scen1.png)\n\nAs a pre-requisite you need to run your local cluster by using the docker compose as introduced in [this note](/use-cases/kafka-mm2/#pre-requisites).\n\n## Start local kafka cluster\n\n1. Get [Docker](https://docs.docker.com/) desktop and docker compose on your local computer.  and docker compose to run the solution locally.\n\n If for security reason you are not able to install docker on your local workstation, but you have access to a kubernetes cluster, like OpenShift, see the [next section](#running-docker-in-kubernetes-pod).\n\n In this project main folder there is a [docker compose file](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-data-consistency/master/docker-compose.yml) to start a local three brokers cluster with two Zookeeper nodes.\n\n1. In one Terminal window, start the local cluster using `docker-compose up &` command. The data are persisted on the local disk within this folder.\n\nYour environment is up and running.\n\n## Start Kafka Connect\n\n\n1. If this is the first time you start this local Kafka cluster, you need to create the `products` topic. Start a Kafka container to access the Kafka tools with the command:\n\n  ```shell\n  docker run -ti -v $(pwd):/home --network kafkanet strimzi/kafka:latest-kafka-2.5.0 bash -c \"/home/scripts/createProductsTopic.sh\"\n  ```\n1. Verify the topic is created:\n\n ```shell\n docker exec -ti refarch-eda-data-consistency_kafka1_1 /bin/bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list\"\n ```\n\n1. Modify .env file to set environment variables for Source Event Streams cluster brokers address and APIKEY.\n\n ```shell\n export KAFKA_SOURCE_BROKERS=broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n export KAFKA_SOURCE_APIKEY=\"<replace with apikey from event streams service credentials>\"\n ```\n Then use: `source .env`\n\n The configuration file (es-to-local/mm2.properties) is for standalone configuration, and looks like:\n\n    ```properties\n    clusters=source, target\n    source.bootstrap.servers=broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n    source.security.protocol=SASL_SSL\n    source.ssl.protocol=TLSv1.2\n    source.sasl.mechanism=PLAIN\n    source.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\";\n    target.bootstrap.servers=kafka1:9092,kafka2:9093,kafka3:9094\n    # enable and configure individual replication flows\n    source->target.enabled=true\n    source->target.topics=products\n    ```\n\n1. Start Mirror Maker2.0, by using a new container to start another kakfa 2.5 docker container, connected to the  brokers via the `kafkanet` network, and mounting the configuration in the `/home`. We have done a script to modify the properties file from a template file then call mirror maker 2:\n\n    ```shell\n    ./scripts/launchMM2.sh\n    ```\n\n    The command starts the container with a command very similar as:\n\n    ```shell\n     docker run -ti --network kafkanet -v $(pwd):/home -v $(pwd)/mirror-maker-2/logs:/opt/kafka/logs strimzi/kafka:latest-kafka-2.5.0 /bin/bash -c \"/opt/kafka/bin/connect-mirror-maker.sh /home/mirror-maker-2/es-to-local/mm2.properties\"\n    ```\n\n    The `mm2.properties` file defines the source and target clusters and the topics to replicate.\n\n## Start Consumer from target cluster\n\nWe will use a simple and useful tool, called [Kafdrop](https://github.com/obsidiandynamics/kafdrop) to see messages in topics.\n\nWe also have done a simple script to start or stop Kafdrop.\n\n```\n./scripts/startKafdrop.sh\n```\n\n## Start Producer to source cluster\n\nWe are reusing the python environment as defined in the integration tests for the 'kcontainer' solution. [https://hub.docker.com/r/ibmcase/kcontainer-python](https://hub.docker.com/r/ibmcase/kcontainer-python).\n\nThis time the script is producing products data. Here are the steps to send 5 records.\n\n```shell\ncd mirror-maker-2\n# 1- if not done set environment variables to reach the remote kafka cluster, source of the replication\nsource ../.env\n# 2- Execute the script using docker\ndocker run -ti -v $(pwd):/home --rm -e  KAFKA_BROKERS=$KAFKA_SOURCE_BROKERS -e KAFKA_APIKEY=$KAFKA_SOURCE_APIKEY ibmcase/kcontainer-python:itgtests python /home/SendProductToKafka.py --file /home/data/products.json\n```\n\nThe traces should look like:\n\n```\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '..hidden...'}\n{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n```\n\nValidate the replication is done, using Kafdrop connected to the target cluster you can select the `source.products` topic. The last offset should have increased by 5,\n\n ![2](../images/kafdrop-src-topic.png)\n\nGoing to view messages you can see the last 5 messages match the one sent to the topic on the source cluster.\n\n ![3](../images/kafdrop-msg.png)\n\n\nAnother way to validate the target `source.products` topic has records, is to use a Kafka console tool, delivered with Apache kafka open source.\n\n  ```shell\n  docker run -ti --network kafkanet --rm=true strimzi/kafka:latest-kafka-2.5.0  /bin/bash -c \"bin/kafka-console-consumer.sh --bootstrap-server  kafka1:9092 --topic source.products --from-beginning\"\n\n  { \"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  { \"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6,\"target_humidity_level\": 0.6,\"content_type\": 2}\n  { \"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  ```\nStop with Ctrl C.\n","type":"Mdx","contentDigest":"027f695a634e182867d93406f80a5a95","counter":648,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Mirror Maker 2 Lab 1\ndescription: Using MM2 from Event Streams on Cloud to local Kafka 2.5 cluster.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Start Kafka Connect</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n\n</AnchorLinks>\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on IBM Cloud cluster instance, and the target is a local Kafka cluster running with docker compose.\n\n![1](../images/mm2-scen1.png)\n\nAs a pre-requisite you need to run your local cluster by using the docker compose as introduced in [this note](/use-cases/kafka-mm2/#pre-requisites).\n\n## Start local kafka cluster\n\n1. Get [Docker](https://docs.docker.com/) desktop and docker compose on your local computer.  and docker compose to run the solution locally.\n\n If for security reason you are not able to install docker on your local workstation, but you have access to a kubernetes cluster, like OpenShift, see the [next section](#running-docker-in-kubernetes-pod).\n\n In this project main folder there is a [docker compose file](https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-data-consistency/master/docker-compose.yml) to start a local three brokers cluster with two Zookeeper nodes.\n\n1. In one Terminal window, start the local cluster using `docker-compose up &` command. The data are persisted on the local disk within this folder.\n\nYour environment is up and running.\n\n## Start Kafka Connect\n\n\n1. If this is the first time you start this local Kafka cluster, you need to create the `products` topic. Start a Kafka container to access the Kafka tools with the command:\n\n  ```shell\n  docker run -ti -v $(pwd):/home --network kafkanet strimzi/kafka:latest-kafka-2.5.0 bash -c \"/home/scripts/createProductsTopic.sh\"\n  ```\n1. Verify the topic is created:\n\n ```shell\n docker exec -ti refarch-eda-data-consistency_kafka1_1 /bin/bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list\"\n ```\n\n1. Modify .env file to set environment variables for Source Event Streams cluster brokers address and APIKEY.\n\n ```shell\n export KAFKA_SOURCE_BROKERS=broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n export KAFKA_SOURCE_APIKEY=\"<replace with apikey from event streams service credentials>\"\n ```\n Then use: `source .env`\n\n The configuration file (es-to-local/mm2.properties) is for standalone configuration, and looks like:\n\n    ```properties\n    clusters=source, target\n    source.bootstrap.servers=broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n    source.security.protocol=SASL_SSL\n    source.ssl.protocol=TLSv1.2\n    source.sasl.mechanism=PLAIN\n    source.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\";\n    target.bootstrap.servers=kafka1:9092,kafka2:9093,kafka3:9094\n    # enable and configure individual replication flows\n    source->target.enabled=true\n    source->target.topics=products\n    ```\n\n1. Start Mirror Maker2.0, by using a new container to start another kakfa 2.5 docker container, connected to the  brokers via the `kafkanet` network, and mounting the configuration in the `/home`. We have done a script to modify the properties file from a template file then call mirror maker 2:\n\n    ```shell\n    ./scripts/launchMM2.sh\n    ```\n\n    The command starts the container with a command very similar as:\n\n    ```shell\n     docker run -ti --network kafkanet -v $(pwd):/home -v $(pwd)/mirror-maker-2/logs:/opt/kafka/logs strimzi/kafka:latest-kafka-2.5.0 /bin/bash -c \"/opt/kafka/bin/connect-mirror-maker.sh /home/mirror-maker-2/es-to-local/mm2.properties\"\n    ```\n\n    The `mm2.properties` file defines the source and target clusters and the topics to replicate.\n\n## Start Consumer from target cluster\n\nWe will use a simple and useful tool, called [Kafdrop](https://github.com/obsidiandynamics/kafdrop) to see messages in topics.\n\nWe also have done a simple script to start or stop Kafdrop.\n\n```\n./scripts/startKafdrop.sh\n```\n\n## Start Producer to source cluster\n\nWe are reusing the python environment as defined in the integration tests for the 'kcontainer' solution. [https://hub.docker.com/r/ibmcase/kcontainer-python](https://hub.docker.com/r/ibmcase/kcontainer-python).\n\nThis time the script is producing products data. Here are the steps to send 5 records.\n\n```shell\ncd mirror-maker-2\n# 1- if not done set environment variables to reach the remote kafka cluster, source of the replication\nsource ../.env\n# 2- Execute the script using docker\ndocker run -ti -v $(pwd):/home --rm -e  KAFKA_BROKERS=$KAFKA_SOURCE_BROKERS -e KAFKA_APIKEY=$KAFKA_SOURCE_APIKEY ibmcase/kcontainer-python:itgtests python /home/SendProductToKafka.py --file /home/data/products.json\n```\n\nThe traces should look like:\n\n```\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '..hidden...'}\n{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n```\n\nValidate the replication is done, using Kafdrop connected to the target cluster you can select the `source.products` topic. The last offset should have increased by 5,\n\n ![2](../images/kafdrop-src-topic.png)\n\nGoing to view messages you can see the last 5 messages match the one sent to the topic on the source cluster.\n\n ![3](../images/kafdrop-msg.png)\n\n\nAnother way to validate the target `source.products` topic has records, is to use a Kafka console tool, delivered with Apache kafka open source.\n\n  ```shell\n  docker run -ti --network kafkanet --rm=true strimzi/kafka:latest-kafka-2.5.0  /bin/bash -c \"bin/kafka-console-consumer.sh --bootstrap-server  kafka1:9092 --topic source.products --from-beginning\"\n\n  { \"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  { \"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6,\"target_humidity_level\": 0.6,\"content_type\": 2}\n  { \"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n  ```\nStop with Ctrl C.\n","frontmatter":{"title":"Kafka Mirror Maker 2 Lab 1","description":"Using MM2 from Event Streams on Cloud to local Kafka 2.5 cluster."},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-mm2/lab-1/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}