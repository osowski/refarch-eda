{"componentChunkName":"component---src-pages-use-cases-kafka-mm-2-lab-2-index-mdx","path":"/use-cases/kafka-mm2/lab-2/","result":{"pageContext":{"frontmatter":{"title":"Kafka Mirror Maker 2 Lab 2","description":"Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud"},"relativePagePath":"/use-cases/kafka-mm2/lab-2/index.mdx","titleType":"append","MdxNode":{"id":"aa04f9fc-a36c-5149-a94c-dd7c4bd185a2","children":[],"parent":"60209e93-c715-5820-a67d-66fa0ff6e5d9","internal":{"content":"---\ntitle: Kafka Mirror Maker 2 Lab 2\ndescription: Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Pre-requisites</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on premise on OpenShift, and the target cluster is an Event Stream on Cloud. Mirror Maker 2 runs on OpenShift.\n\n ![1](../images/mm2-lab2.png)\n\n## Pre-requisites\n\n* We will use the Cloud shell tool, so connect and create the environment for this shell using the same instructions as in [the schema registry lab](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/schema-registry-cp4i-v10#ibm-cloud-shell).\n\n* Login to the OpenShift cluster using the console and get the API token\n\n ```shell\n oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443\n ```\n\n* Clone the github to get access to the Mirror Maker 2 manifests we are using:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency\n ```\n\n* As Event Streams on the Cloud does not authorize to create topic with the Kafka Admin API, we need to create the following topics with scripts or GUI. The naming convention respects the name of the source cluster as defined in the MirrorMaker2 manifest we will detail later.\n\n  * es-1.checkpoints.internal  use one partition\n  * es-1.heartbeats  use one partition\n  * es-1.products   matches the partition allocated to the source topic\n  * mirrormaker2-cluster-configs one partition\n  * mirrormaker2-cluster-offsets 25 partitions\n  * mirrormaker2-cluster-status 5 partitions.\n\nUse the default replication factor of 3.\n\n## Start Mirror Maker 2\n\nIn this lab, Mirror Maker 2 will run on the same cluster as Event Streams within the same namespace (e.g. integration). \n\n* Create a secret for the API KEY of the Event Streams target cluster:\n\n ```shell\n  oc create secret generic es-oc-api-secret --from-literal=password=<replace-with-event-streams-on-cloud-apikey>\n ```\n\n* Verify the Event Streams on OpenShift service end point URL. This URL will be used to configure Mirror Maker 2. \n\n ```shell\n # Go to the project where Event Streams is installed\n oc project integration\n # Use the bootstrap internal URL\n oc describe svc es-1-kafka-bootstrap  -o=jsonpath='{.status.ingress[0].host}{\"\\n\"}'\n # result:\n    es-1-kafka-bootstrap.integration.svc:9093\n ```\n\n* Verify the CA certificate secrets:\n\n ```shell\n  oc describe secret es-1-cluster-ca -n integration\n ```\n\n* Get a user with write access to topic, transactions. You can use the User Interface to do so with the Create TLS credential on the intern URL panel.\n\n ![4](../images/mm2-cred-1.png)\n\n ![5](../images/mm2-cred-2.png)\n\n This user needs to be able to create topic on target cluster.\n o\n ![6](../images/mm2-cred-3.png)\n\n You can specify a specific set of topics, with Java regexp.\n\n ![7](../images/mm2-cred-4.png)\n\n and all the transactions.\n\n* Define source and target cluster properties in a Mirror Maker 2 `es-ocp-to-es-oc.yml` descriptor file. There is a file for the replication between Event Streams OCP to Event Streams on cloud [es-ocp-to-es-oc.yml](https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency/blob/master/mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml). We strongly recommend to study the schema definition of this [custom resource from this page](https://github.com/strimzi/strimzi-kafka-operator/blob/2d35bfcd99295bef8ee98de9d8b3c86cb33e5842/install/cluster-operator/048-Crd-kafkamirrormaker2.yaml#L648-L663). \n\nHere are some important parameters: The namespace needs to match the newly created project:\n\n```yaml\napiVersion: eventstreams.ibm.com/v1alpha1\nkind: KafkaMirrorMaker2\nmetadata:\n  name: mm2\n  namespace: integration\nspec:\n  template:\n    pod:\n      metadata:\n        annotations:\n          eventstreams.production.type: CloudPakForIntegrationNonProduction\n  version: 2.5.0\n  replicas: 1\n  connectCluster: \"event-streams-wdc\"\n```\n\nThe version matches the Kafka version we use. The number of replicas can be set to 1 to start. The eventstreams.production.type is needed for Event Streams.\n\nThen the yaml defines the connection configuration for each clusters: Event Streams on cloud, so you need to define the bootstrap servers (This could come from a config map too) and the API key coming from the previously defined secret. The user is always 'token'\n\n```yaml\nclusters:\n  - alias: \"event-streams-wdc\"\n    bootstrapServers: broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n    config:\n      config.storage.replication.factor: 3\n      offset.storage.replication.factor: 3\n      status.storage.replication.factor: 3\n    tls: {}\n    authentication:\n      passwordSecret:\n          secretName: es-oc-api-secret  \n          password: password \n      username: token\n      type: plain\n```\n\nFor Event Streams on premise running within OpenShift, the connection uses TLS, certificates and Sram credentials. As we run in a separate namespace the URL is the 'external' one.\n\n```yaml\n- alias: \"es-1-cluster\"\n    bootstrapServers: es-1-kafka-bootstrap.integration.svc:9093\n    config:\n      config.storage.replication.factor: 3\n      offset.storage.replication.factor: 3\n      status.storage.replication.factor: 3\n      ssl.endpoint.identification.algorithm: https\n    tls: \n      trustedCertificates:\n        - secretName: es-1-cluster-ca-cert\n          certificate: ca.crt\n    authentication:\n      type: tls\n      certificateAndKey:\n        certificate: user.crt\n        key: user.key\n        secretName: mm2\n          \n```\n\nFinally the `connectCluster` attribute defines the cluster alias used by MirrorMaker2 to define its hidden topics, it must match the target cluster of the replication in the list at `spec.clusters`.\n    \n \n ```shell\n oc apply -f mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml\n ```\n\n* Verify the chareacteristics of the Mirror Maker 2 instance using the CLI\n\n ```shell\n oc describe kafkamirrormaker2 mm2\n ```\n* As Mirror Maker 2 is set to  be connected to the `event-streams-wdc` which is the event streams on cloud. \n\n## Start Producer to source cluster\n\nAs seen in lab 1, we will use the same python script to create products records. This time the script is producing products data to Event Streams on OpenShift. So we need the URL, pem, and user to access this cluster.\nThe credentials can be accessed and defined using the GUI. [This section in the schema registry](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/schema-registry-cp4i-v10#gui) lab goes into the details on how to get the external URL and Scram password. \n\n* If not done before download the pem file using the following CLI commands:\n\n ```shell\n cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n cloudctl es init\n cloudctl es certificates --format pem\n ```\n* Get userid, password and information of one of the kafka user defined. The namespace used below is the one where event streams is installed.\n\n ```shell\n oc get kafkausers--namespace integration\n oc get kafkauser my-user1 --namespace integration -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'\n ```\n* Set the following environment variables:\n\n ```shell\n export KAFKA_BROKERS=es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n export KAFKA_USER=my-user1\n export KAFKA_PWD=$(oc get secret $KAFKA_USER --namespace integration -o jsonpath='{.data.password}' | base64 --decode)\n export KAFKA_CERT=/home/es-cert.pem\n ```\n\n ```shell\n cd mirror-maker-2\n # Execute the script using docker\n docker run -ti -v $(pwd):/home --rm -e  KAFKA_BROKERS=$KAFKA_SOURCE_BROKERS -e KAFKA_USER=$KAFKA_USER -e KAFKA_PWD=$KAFKA_PWD -e KAFKA_CERT=$KAFKA_CERT ibmcase/kcontainer-python:itgtests python /home/SendProductToKafka.py --file /home/data/products.json\n ```\n\n## Start Consumer from target cluster\n\nSpecifying the target cluster as Event Streams on cloud, we can also use Kafdrop to see the replicated topic.\n\n```\n./scripts/startKafdrop.sh es-cloud.properties\n```\n\nGo to the es-1.products topic to see the replicated messages:\n\n ![9](../images/kafdrop-eso-esic.png)\n\n ## Understanding MirrorMaker 2 trace \n\nA lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues.\n\nIf some message arrive with NO_LEADER for one of the topic, is that MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface.\n\n * WorkerSourceTask{id=es-1->event-streams-wdc.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat.\n * WorkerSourceTask{id=es-1->event-streams-wdc.MirrorSourceConnector-0} for the topic to replicate","type":"Mdx","contentDigest":"9c9afc9a1cba5d64aa64d5e9c8e2361a","counter":590,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Mirror Maker 2 Lab 2\ndescription: Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Pre-requisites</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Start Producer to source cluster</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nFor this scenario the source cluster is an Event Streams on premise on OpenShift, and the target cluster is an Event Stream on Cloud. Mirror Maker 2 runs on OpenShift.\n\n ![1](../images/mm2-lab2.png)\n\n## Pre-requisites\n\n* We will use the Cloud shell tool, so connect and create the environment for this shell using the same instructions as in [the schema registry lab](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/schema-registry-cp4i-v10#ibm-cloud-shell).\n\n* Login to the OpenShift cluster using the console and get the API token\n\n ```shell\n oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443\n ```\n\n* Clone the github to get access to the Mirror Maker 2 manifests we are using:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency\n ```\n\n* As Event Streams on the Cloud does not authorize to create topic with the Kafka Admin API, we need to create the following topics with scripts or GUI. The naming convention respects the name of the source cluster as defined in the MirrorMaker2 manifest we will detail later.\n\n  * es-1.checkpoints.internal  use one partition\n  * es-1.heartbeats  use one partition\n  * es-1.products   matches the partition allocated to the source topic\n  * mirrormaker2-cluster-configs one partition\n  * mirrormaker2-cluster-offsets 25 partitions\n  * mirrormaker2-cluster-status 5 partitions.\n\nUse the default replication factor of 3.\n\n## Start Mirror Maker 2\n\nIn this lab, Mirror Maker 2 will run on the same cluster as Event Streams within the same namespace (e.g. integration). \n\n* Create a secret for the API KEY of the Event Streams target cluster:\n\n ```shell\n  oc create secret generic es-oc-api-secret --from-literal=password=<replace-with-event-streams-on-cloud-apikey>\n ```\n\n* Verify the Event Streams on OpenShift service end point URL. This URL will be used to configure Mirror Maker 2. \n\n ```shell\n # Go to the project where Event Streams is installed\n oc project integration\n # Use the bootstrap internal URL\n oc describe svc es-1-kafka-bootstrap  -o=jsonpath='{.status.ingress[0].host}{\"\\n\"}'\n # result:\n    es-1-kafka-bootstrap.integration.svc:9093\n ```\n\n* Verify the CA certificate secrets:\n\n ```shell\n  oc describe secret es-1-cluster-ca -n integration\n ```\n\n* Get a user with write access to topic, transactions. You can use the User Interface to do so with the Create TLS credential on the intern URL panel.\n\n ![4](../images/mm2-cred-1.png)\n\n ![5](../images/mm2-cred-2.png)\n\n This user needs to be able to create topic on target cluster.\n o\n ![6](../images/mm2-cred-3.png)\n\n You can specify a specific set of topics, with Java regexp.\n\n ![7](../images/mm2-cred-4.png)\n\n and all the transactions.\n\n* Define source and target cluster properties in a Mirror Maker 2 `es-ocp-to-es-oc.yml` descriptor file. There is a file for the replication between Event Streams OCP to Event Streams on cloud [es-ocp-to-es-oc.yml](https://github.com/ibm-cloud-architecture/refarch-eda-data-consistency/blob/master/mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml). We strongly recommend to study the schema definition of this [custom resource from this page](https://github.com/strimzi/strimzi-kafka-operator/blob/2d35bfcd99295bef8ee98de9d8b3c86cb33e5842/install/cluster-operator/048-Crd-kafkamirrormaker2.yaml#L648-L663). \n\nHere are some important parameters: The namespace needs to match the newly created project:\n\n```yaml\napiVersion: eventstreams.ibm.com/v1alpha1\nkind: KafkaMirrorMaker2\nmetadata:\n  name: mm2\n  namespace: integration\nspec:\n  template:\n    pod:\n      metadata:\n        annotations:\n          eventstreams.production.type: CloudPakForIntegrationNonProduction\n  version: 2.5.0\n  replicas: 1\n  connectCluster: \"event-streams-wdc\"\n```\n\nThe version matches the Kafka version we use. The number of replicas can be set to 1 to start. The eventstreams.production.type is needed for Event Streams.\n\nThen the yaml defines the connection configuration for each clusters: Event Streams on cloud, so you need to define the bootstrap servers (This could come from a config map too) and the API key coming from the previously defined secret. The user is always 'token'\n\n```yaml\nclusters:\n  - alias: \"event-streams-wdc\"\n    bootstrapServers: broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n    config:\n      config.storage.replication.factor: 3\n      offset.storage.replication.factor: 3\n      status.storage.replication.factor: 3\n    tls: {}\n    authentication:\n      passwordSecret:\n          secretName: es-oc-api-secret  \n          password: password \n      username: token\n      type: plain\n```\n\nFor Event Streams on premise running within OpenShift, the connection uses TLS, certificates and Sram credentials. As we run in a separate namespace the URL is the 'external' one.\n\n```yaml\n- alias: \"es-1-cluster\"\n    bootstrapServers: es-1-kafka-bootstrap.integration.svc:9093\n    config:\n      config.storage.replication.factor: 3\n      offset.storage.replication.factor: 3\n      status.storage.replication.factor: 3\n      ssl.endpoint.identification.algorithm: https\n    tls: \n      trustedCertificates:\n        - secretName: es-1-cluster-ca-cert\n          certificate: ca.crt\n    authentication:\n      type: tls\n      certificateAndKey:\n        certificate: user.crt\n        key: user.key\n        secretName: mm2\n          \n```\n\nFinally the `connectCluster` attribute defines the cluster alias used by MirrorMaker2 to define its hidden topics, it must match the target cluster of the replication in the list at `spec.clusters`.\n    \n \n ```shell\n oc apply -f mirror-maker-2/es-ocp/es-ocp-to-es-oc.yml\n ```\n\n* Verify the chareacteristics of the Mirror Maker 2 instance using the CLI\n\n ```shell\n oc describe kafkamirrormaker2 mm2\n ```\n* As Mirror Maker 2 is set to  be connected to the `event-streams-wdc` which is the event streams on cloud. \n\n## Start Producer to source cluster\n\nAs seen in lab 1, we will use the same python script to create products records. This time the script is producing products data to Event Streams on OpenShift. So we need the URL, pem, and user to access this cluster.\nThe credentials can be accessed and defined using the GUI. [This section in the schema registry](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/schema-registry-cp4i-v10#gui) lab goes into the details on how to get the external URL and Scram password. \n\n* If not done before download the pem file using the following CLI commands:\n\n ```shell\n cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n cloudctl es init\n cloudctl es certificates --format pem\n ```\n* Get userid, password and information of one of the kafka user defined. The namespace used below is the one where event streams is installed.\n\n ```shell\n oc get kafkausers--namespace integration\n oc get kafkauser my-user1 --namespace integration -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'\n ```\n* Set the following environment variables:\n\n ```shell\n export KAFKA_BROKERS=es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n export KAFKA_USER=my-user1\n export KAFKA_PWD=$(oc get secret $KAFKA_USER --namespace integration -o jsonpath='{.data.password}' | base64 --decode)\n export KAFKA_CERT=/home/es-cert.pem\n ```\n\n ```shell\n cd mirror-maker-2\n # Execute the script using docker\n docker run -ti -v $(pwd):/home --rm -e  KAFKA_BROKERS=$KAFKA_SOURCE_BROKERS -e KAFKA_USER=$KAFKA_USER -e KAFKA_PWD=$KAFKA_PWD -e KAFKA_CERT=$KAFKA_CERT ibmcase/kcontainer-python:itgtests python /home/SendProductToKafka.py --file /home/data/products.json\n ```\n\n## Start Consumer from target cluster\n\nSpecifying the target cluster as Event Streams on cloud, we can also use Kafdrop to see the replicated topic.\n\n```\n./scripts/startKafdrop.sh es-cloud.properties\n```\n\nGo to the es-1.products topic to see the replicated messages:\n\n ![9](../images/kafdrop-eso-esic.png)\n\n ## Understanding MirrorMaker 2 trace \n\nA lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues.\n\nIf some message arrive with NO_LEADER for one of the topic, is that MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface.\n\n * WorkerSourceTask{id=es-1->event-streams-wdc.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat.\n * WorkerSourceTask{id=es-1->event-streams-wdc.MirrorSourceConnector-0} for the topic to replicate","frontmatter":{"title":"Kafka Mirror Maker 2 Lab 2","description":"Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-mm2/lab-2/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}