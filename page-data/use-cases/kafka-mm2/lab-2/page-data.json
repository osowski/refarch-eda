{"componentChunkName":"component---src-pages-use-cases-kafka-mm-2-lab-2-index-mdx","path":"/use-cases/kafka-mm2/lab-2/","result":{"pageContext":{"frontmatter":{"title":"Mirror Maker 2 ES on RHOS to local cluster","description":"Using Mirror Maker 2 from Event Streams on OpenShift to local cluster"},"relativePagePath":"/use-cases/kafka-mm2/lab-2/index.mdx","titleType":"append","MdxNode":{"id":"aa04f9fc-a36c-5149-a94c-dd7c4bd185a2","children":[],"parent":"60209e93-c715-5820-a67d-66fa0ff6e5d9","internal":{"content":"---\ntitle: Mirror Maker 2 ES on RHOS to local cluster\ndescription: Using Mirror Maker 2 from Event Streams on OpenShift to local cluster\n---\n\n<AnchorLinks>\n  <AnchorLink>Scenario Prerequisites</AnchorLink>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Start Strimzi Kafka Cluster</AnchorLink>\n  <AnchorLink>Produce messages to source cluster</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nUpdated 01/22/2021\n\n## Overview\n\nFor this scenario, the source cluster will be an IBM Event Streams instance on OpenShift and the target cluster will be another Kafka cluster (using [Strimzi](https://strimzi.io/)) running locally on your workstation. Mirror Maker 2 will also run locally on your workstation. This lab is similar to the previous [Lab 1](/use-cases/kafka-mm2/lab-1/), but instead it uses IBM Event Streams within the Cloud Pak for Integration as illustrated in the figure below:\n\n ![1](../images/mm2-lab2.png)\n\n 1. Mirror Maker 2 runs locally on your workstation.\n 2. A producer to send records to the `products` topic that also runs locally although it could be deployed on OpenShift as a **job** as well.\n 3. A Kafka cluster running locally on your workstation that will contain the replicated topic and a Kafka console consumer to see the replicated messages.\n\n## Scenario Prerequisites\n\n* An IBM Event Streams instance running on OpenShift. See [here](https://ibm.github.io/event-streams/installing/installing/) for more detail about installing IBM Event Streams.\n* Docker Compose\n* Git CLI\n\nComplete the following steps in order to get ready for executing this lab scenario\n\n1. Create the `products` topic in your IBM Event Streams instance running on OpenShift. **IMPORTANT:** Create the topic with just **1 partition**. To do so, please review the instructions in the Common pre-requisites of this website [here](/use-cases/overview/pre-requisites#create-event-streams-topics). **IMPORTANT:** If you are sharing the IBM Event Streams instance, append a unique identifier to the `products` topic name so that you don't collide with anyone else.\n\n1. If you did not complete [Lab 1](/use-cases/kafka-mm2/lab-1/), clone the following GitHub repository to your local workstation to get the Mirror Maker 2 configuration files for this lab:\n\n  ```shell\n  git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n  ```\n\n1. Change directory into `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local`\n\n  ```shell\n  cd refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local\n  ```\n\n1. Rename the `.env-tmpl` properties file to `.env`\n\n  ```shell\n  mv .env-tmpl .env\n  ```\n\n1. Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. **IMPORTANT:** download the PKCS12 certificate. How to get the certificate in the [Common pre-requisites](/use-cases/overview/pre-requisites/) section. \n\n1. The `.env` properties file will contain the properties needed for Mirror Maker 2 to be able to connect with your IBM Event Streams instance running on Openshift. Therefore, replace the following placeholder in the properties file:\n\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above.\n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n\n## Start Strimzi Kafka Cluster\n\nIn this section, we are going to deploy and start a local [Strimzi Kafka cluster](https://strimzi.io/) which will act as your target cluster for Mirror Maker 2 to mirror the messages getting into the `products` topic in your IBM Event Streams instance to. In order to deploy this local Strimzi Kafka cluster, we are providing a [Docker Compose](https://docs.docker.com/compose/) file that will coordinate the startup of all the components in this Strimzi Kafka cluster.\n\n1. Make sure you are in `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local`.\n\n1. Execute the following command\n\n  ```shell\n  docker-compose up -d\n  ```\n\n1. The above command should start all the components in `detached` mode (`-d`) and you should see the following output:\n\n  ```shell\n  Creating zookeeper1 ... done\n  Creating kafka1     ... done\n  Creating kafka2     ... done\n  Creating kafka3     ... done\n  ```\n\n1. You should see the following Docker containers running on your workstation at the moment\n\n  ```shell\n  docker ps\n  CONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS         PORTS                                              NAMES\n  1981f1913ab6   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9093->9093/tcp, 0.0.0.0:29093->29093/tcp   kafka3\n  5f8fd3e80406   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9091->9091/tcp, 0.0.0.0:29091->29091/tcp   kafka1\n  b19a05bd74dd   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9092->9092/tcp, 0.0.0.0:29092->29092/tcp   kafka2\n  93f500c8517a   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/zookeepe…\"   2 minutes ago   Up 2 minutes   0.0.0.0:2181->2181/tcp                             zookeeper1\n  ```\n\n## Produce messages to source cluster\n\nIn this section, we are going to finally send events to the `products` topic in your IBM Event streams instance, which is your source cluster, and then verify those messages get mirrored by Mirror Maker 2 into your local Strimzi Kafka cluster, which is your target cluster. We are going to use a shell script which, in turn, will run a Python application that will send the messages to the source cluster.\n\n1. Since the application sending the messages to the source cluster is not a Java application, we will first need to download the `PEM` TLS certificate to allow the secure connection from the python application sending the messages to IBM Event Streams. Make sure you are in the `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` directory.  Download the `PEM` TLS certificate there. Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't remember how to download the certificate.\n\n1. Now we are going to send five records. In a new terminal window, make sure you are in the `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` directory and execute the following bash script.\n \n  ```shell\n  ./sendProductRecords.sh\n  ```\n\n1. You should see the following output indicating your messages have been delivered to the source cluster topic\n\n  ```shell\n  --- This is the configuration for the producer: ---\n  [KafkaProducer] - {'bootstrap.servers': 'kafka-bootstrap-integration.apps.net:443', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'test_user', 'sasl.password': '******', 'ssl.ca.location': '/home/es-cp4i-to-local/es-cert.pem'}\n  ---------------------------------------------------\n  {'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n  {'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  ```\n\n1. If you go to the IBM Event Streams console, you should also see those messages in the topic\n\n  ![](../images/es-products-topic.png)\n\n## Start Mirror Maker 2\n\nIn this section, we are going to go through the steps to get Mirror Maker 2 running locally on your workstation and configure it so that it replicates the messages from the `products` topic in your IBM Event Streams instance running on OpenShift to the local Strimzi Kafka cluster you deployed in the previous section as the target cluster for those messages.\n\n1. Make sure you are in `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` and you have done all steps in the [Scenario Prerequisites](#scenario-prerequisites) section.\n\n1. Start your local Mirror Maker 2 instance by executing the following bash script.\n\n  ```shell\n  ./launchMM2.sh\n  ```\n\n1. After quite some long output on your screen, you should see the following messages with the name of your topic. Don't worry if you dont find these as there is a lot of ouput. You will make sure the messages are replicated in the next section.\n\n  ```shell\n  INFO [Consumer clientId=consumer-null-14, groupId=null] Subscribed to partition(s): products-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)\n  INFO Starting with 1 previously uncommitted partitions. (org.apache.kafka.connect.mirror.MirrorSourceTask:94)\n  INFO [Consumer clientId=consumer-null-14, groupId=null] Seeking to offset 0 for partition products-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1596)\n  INFO [Consumer clientId=consumer-null-15, groupId=null] Subscribed to partition(s): heartbeats-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)\n  INFO task-thread-MirrorSourceConnector-0 replicating 1 topic-partitions es-cp4i->target: [products-0]. (org.apache.kafka.connect.mirror.MirrorSourceTask:98)\n  INFO WorkerSourceTask{id=MirrorSourceConnector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)\n  INFO Starting with 1 previously uncommitted partitions. (org.apache.kafka.connect.mirror.MirrorSourceTask:94)\n  INFO [Consumer clientId=consumer-null-15, groupId=null] Seeking to offset 0 for partition heartbeats-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1596)\n  INFO task-thread-MirrorSourceConnector-1 replicating 1 topic-partitions es-cp4i->target: [heartbeats-0]. (org.apache.kafka.connect.mirror.MirrorSourceTask:98)\n  INFO WorkerSourceTask{id=MirrorSourceConnector-1} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)\n  ```\n\n## Start consumer from target cluster\n\nIn this section, we are going to start a consumer to consume messages from the target cluster (your local Strimzi Kafka cluster) to make sure we receive mirrored messages from your source cluster (your IBM Event Streams instance running on OpenShift). We are going to use a couple of Apache Kafka tools comming with the open source Strimzi Kafka Docker image you already have running.\n\n1. Make sure your target mirrored topic has been created executing the following command on a new terminal window.\n\n  ```shell\n  docker exec kafka2 bash -c \"/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server kafka1:9091\" \n\n  __consumer_offsets\n  es-cp4i.checkpoints.internal\n  es-cp4i.heartbeats\n  es-cp4i.products\n  heartbeats\n  mm2-configs.es-cp4i.internal\n  mm2-offsets.es-cp4i.internal\n  mm2-status.es-cp4i.internal\n  ```\n\n  You should see a topic called `es-cp4i.YOUR_TOPIC` where `YOUR_TOPIC` should be the name of the topic you created before in the [Scenario Prerequisites](#scenario-prerequisites) section.\n\n1. Now, execute the following command replacing the `TOPIC_NAME` placeholder with the name of the topic you verified above (`ex-cp4i.YOUR_TOPIC`)\n\n  ```shell\n  docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic TOPIC_NAME --from-beginning\" \n  ```\n\n1. You should see the mirrored messages now in your replicated topic in your target local Strimzi Kafka cluster\n\n  ```shell\n  {\"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1}\n  {\"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6, \"target_humidity_level\": 0.6, \"content_type\": 2}\n  {\"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1}\n  {\"product_id\": \"P04\", \"description\": \"Avocado\", \"target_temperature\": 6, \"target_humidity_level\": 0.4, \"content_type\": 1}\n  {\"product_id\": \"P05\", \"description\": \"Tomato\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 2}\n  ```\n\n## Clean up\n\nYou have now successfully finished the lab. You can stop the consumer and the Mirror Maker 2 console output pressing `ctrl+c` in their respective terminals. You can also stop and remove the Docker containers for both the Mirror Maker 2 and Strimzi Kafka clusters running on your workstation by executing the following script:\n\n```shell\n./cleanLab.sh\n```\n","type":"Mdx","contentDigest":"dfc40997e3c84cf7168bdf24c3bed5c1","owner":"gatsby-plugin-mdx","counter":779},"frontmatter":{"title":"Mirror Maker 2 ES on RHOS to local cluster","description":"Using Mirror Maker 2 from Event Streams on OpenShift to local cluster"},"exports":{},"rawBody":"---\ntitle: Mirror Maker 2 ES on RHOS to local cluster\ndescription: Using Mirror Maker 2 from Event Streams on OpenShift to local cluster\n---\n\n<AnchorLinks>\n  <AnchorLink>Scenario Prerequisites</AnchorLink>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Start Strimzi Kafka Cluster</AnchorLink>\n  <AnchorLink>Produce messages to source cluster</AnchorLink>\n  <AnchorLink>Start Mirror Maker 2</AnchorLink>\n  <AnchorLink>Start Consumer from target cluster</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nUpdated 01/22/2021\n\n## Overview\n\nFor this scenario, the source cluster will be an IBM Event Streams instance on OpenShift and the target cluster will be another Kafka cluster (using [Strimzi](https://strimzi.io/)) running locally on your workstation. Mirror Maker 2 will also run locally on your workstation. This lab is similar to the previous [Lab 1](/use-cases/kafka-mm2/lab-1/), but instead it uses IBM Event Streams within the Cloud Pak for Integration as illustrated in the figure below:\n\n ![1](../images/mm2-lab2.png)\n\n 1. Mirror Maker 2 runs locally on your workstation.\n 2. A producer to send records to the `products` topic that also runs locally although it could be deployed on OpenShift as a **job** as well.\n 3. A Kafka cluster running locally on your workstation that will contain the replicated topic and a Kafka console consumer to see the replicated messages.\n\n## Scenario Prerequisites\n\n* An IBM Event Streams instance running on OpenShift. See [here](https://ibm.github.io/event-streams/installing/installing/) for more detail about installing IBM Event Streams.\n* Docker Compose\n* Git CLI\n\nComplete the following steps in order to get ready for executing this lab scenario\n\n1. Create the `products` topic in your IBM Event Streams instance running on OpenShift. **IMPORTANT:** Create the topic with just **1 partition**. To do so, please review the instructions in the Common pre-requisites of this website [here](/use-cases/overview/pre-requisites#create-event-streams-topics). **IMPORTANT:** If you are sharing the IBM Event Streams instance, append a unique identifier to the `products` topic name so that you don't collide with anyone else.\n\n1. If you did not complete [Lab 1](/use-cases/kafka-mm2/lab-1/), clone the following GitHub repository to your local workstation to get the Mirror Maker 2 configuration files for this lab:\n\n  ```shell\n  git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n  ```\n\n1. Change directory into `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local`\n\n  ```shell\n  cd refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local\n  ```\n\n1. Rename the `.env-tmpl` properties file to `.env`\n\n  ```shell\n  mv .env-tmpl .env\n  ```\n\n1. Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. **IMPORTANT:** download the PKCS12 certificate. How to get the certificate in the [Common pre-requisites](/use-cases/overview/pre-requisites/) section. \n\n1. The `.env` properties file will contain the properties needed for Mirror Maker 2 to be able to connect with your IBM Event Streams instance running on Openshift. Therefore, replace the following placeholder in the properties file:\n\n    * `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n    * `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n    * `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n    * `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n    * `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above.\n  \n  Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't know how to find out any of the config properties above. \n\n\n## Start Strimzi Kafka Cluster\n\nIn this section, we are going to deploy and start a local [Strimzi Kafka cluster](https://strimzi.io/) which will act as your target cluster for Mirror Maker 2 to mirror the messages getting into the `products` topic in your IBM Event Streams instance to. In order to deploy this local Strimzi Kafka cluster, we are providing a [Docker Compose](https://docs.docker.com/compose/) file that will coordinate the startup of all the components in this Strimzi Kafka cluster.\n\n1. Make sure you are in `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local`.\n\n1. Execute the following command\n\n  ```shell\n  docker-compose up -d\n  ```\n\n1. The above command should start all the components in `detached` mode (`-d`) and you should see the following output:\n\n  ```shell\n  Creating zookeeper1 ... done\n  Creating kafka1     ... done\n  Creating kafka2     ... done\n  Creating kafka3     ... done\n  ```\n\n1. You should see the following Docker containers running on your workstation at the moment\n\n  ```shell\n  docker ps\n  CONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS         PORTS                                              NAMES\n  1981f1913ab6   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9093->9093/tcp, 0.0.0.0:29093->29093/tcp   kafka3\n  5f8fd3e80406   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9091->9091/tcp, 0.0.0.0:29091->29091/tcp   kafka1\n  b19a05bd74dd   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9092->9092/tcp, 0.0.0.0:29092->29092/tcp   kafka2\n  93f500c8517a   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/zookeepe…\"   2 minutes ago   Up 2 minutes   0.0.0.0:2181->2181/tcp                             zookeeper1\n  ```\n\n## Produce messages to source cluster\n\nIn this section, we are going to finally send events to the `products` topic in your IBM Event streams instance, which is your source cluster, and then verify those messages get mirrored by Mirror Maker 2 into your local Strimzi Kafka cluster, which is your target cluster. We are going to use a shell script which, in turn, will run a Python application that will send the messages to the source cluster.\n\n1. Since the application sending the messages to the source cluster is not a Java application, we will first need to download the `PEM` TLS certificate to allow the secure connection from the python application sending the messages to IBM Event Streams. Make sure you are in the `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` directory.  Download the `PEM` TLS certificate there. Review the [Common pre-requisites](/use-cases/overview/pre-requisites/) instructions if you don't remember how to download the certificate.\n\n1. Now we are going to send five records. In a new terminal window, make sure you are in the `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` directory and execute the following bash script.\n \n  ```shell\n  ./sendProductRecords.sh\n  ```\n\n1. You should see the following output indicating your messages have been delivered to the source cluster topic\n\n  ```shell\n  --- This is the configuration for the producer: ---\n  [KafkaProducer] - {'bootstrap.servers': 'kafka-bootstrap-integration.apps.net:443', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'test_user', 'sasl.password': '******', 'ssl.ca.location': '/home/es-cp4i-to-local/es-cert.pem'}\n  ---------------------------------------------------\n  {'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n  {'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n  {'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  [KafkaProducer] - Message delivered to products [0]\n  ```\n\n1. If you go to the IBM Event Streams console, you should also see those messages in the topic\n\n  ![](../images/es-products-topic.png)\n\n## Start Mirror Maker 2\n\nIn this section, we are going to go through the steps to get Mirror Maker 2 running locally on your workstation and configure it so that it replicates the messages from the `products` topic in your IBM Event Streams instance running on OpenShift to the local Strimzi Kafka cluster you deployed in the previous section as the target cluster for those messages.\n\n1. Make sure you are in `refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local` and you have done all steps in the [Scenario Prerequisites](#scenario-prerequisites) section.\n\n1. Start your local Mirror Maker 2 instance by executing the following bash script.\n\n  ```shell\n  ./launchMM2.sh\n  ```\n\n1. After quite some long output on your screen, you should see the following messages with the name of your topic. Don't worry if you dont find these as there is a lot of ouput. You will make sure the messages are replicated in the next section.\n\n  ```shell\n  INFO [Consumer clientId=consumer-null-14, groupId=null] Subscribed to partition(s): products-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)\n  INFO Starting with 1 previously uncommitted partitions. (org.apache.kafka.connect.mirror.MirrorSourceTask:94)\n  INFO [Consumer clientId=consumer-null-14, groupId=null] Seeking to offset 0 for partition products-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1596)\n  INFO [Consumer clientId=consumer-null-15, groupId=null] Subscribed to partition(s): heartbeats-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)\n  INFO task-thread-MirrorSourceConnector-0 replicating 1 topic-partitions es-cp4i->target: [products-0]. (org.apache.kafka.connect.mirror.MirrorSourceTask:98)\n  INFO WorkerSourceTask{id=MirrorSourceConnector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)\n  INFO Starting with 1 previously uncommitted partitions. (org.apache.kafka.connect.mirror.MirrorSourceTask:94)\n  INFO [Consumer clientId=consumer-null-15, groupId=null] Seeking to offset 0 for partition heartbeats-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1596)\n  INFO task-thread-MirrorSourceConnector-1 replicating 1 topic-partitions es-cp4i->target: [heartbeats-0]. (org.apache.kafka.connect.mirror.MirrorSourceTask:98)\n  INFO WorkerSourceTask{id=MirrorSourceConnector-1} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)\n  ```\n\n## Start consumer from target cluster\n\nIn this section, we are going to start a consumer to consume messages from the target cluster (your local Strimzi Kafka cluster) to make sure we receive mirrored messages from your source cluster (your IBM Event Streams instance running on OpenShift). We are going to use a couple of Apache Kafka tools comming with the open source Strimzi Kafka Docker image you already have running.\n\n1. Make sure your target mirrored topic has been created executing the following command on a new terminal window.\n\n  ```shell\n  docker exec kafka2 bash -c \"/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server kafka1:9091\" \n\n  __consumer_offsets\n  es-cp4i.checkpoints.internal\n  es-cp4i.heartbeats\n  es-cp4i.products\n  heartbeats\n  mm2-configs.es-cp4i.internal\n  mm2-offsets.es-cp4i.internal\n  mm2-status.es-cp4i.internal\n  ```\n\n  You should see a topic called `es-cp4i.YOUR_TOPIC` where `YOUR_TOPIC` should be the name of the topic you created before in the [Scenario Prerequisites](#scenario-prerequisites) section.\n\n1. Now, execute the following command replacing the `TOPIC_NAME` placeholder with the name of the topic you verified above (`ex-cp4i.YOUR_TOPIC`)\n\n  ```shell\n  docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic TOPIC_NAME --from-beginning\" \n  ```\n\n1. You should see the mirrored messages now in your replicated topic in your target local Strimzi Kafka cluster\n\n  ```shell\n  {\"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1}\n  {\"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6, \"target_humidity_level\": 0.6, \"content_type\": 2}\n  {\"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1}\n  {\"product_id\": \"P04\", \"description\": \"Avocado\", \"target_temperature\": 6, \"target_humidity_level\": 0.4, \"content_type\": 1}\n  {\"product_id\": \"P05\", \"description\": \"Tomato\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 2}\n  ```\n\n## Clean up\n\nYou have now successfully finished the lab. You can stop the consumer and the Mirror Maker 2 console output pressing `ctrl+c` in their respective terminals. You can also stop and remove the Docker containers for both the Mirror Maker 2 and Strimzi Kafka clusters running on your workstation by executing the following script:\n\n```shell\n./cleanLab.sh\n```\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/kafka-mm2/lab-2/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}