{"componentChunkName":"component---src-pages-use-cases-schema-registry-on-ocp-schema-registry-cp-4-i-mdx","path":"/use-cases/schema-registry-on-ocp/schema-registry-cp4i/","result":{"pageContext":{"frontmatter":{"title":"IBM Event Streams Schema Registry from IBM CloudPak for Integration","description":"Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature"},"relativePagePath":"/use-cases/schema-registry-on-ocp/schema-registry-cp4i.mdx","titleType":"append","MdxNode":{"id":"2801edb4-3aa0-5c5c-8869-214691e98172","children":[],"parent":"944de978-c1d0-5b14-b86e-de701d59a3a2","internal":{"content":"---\ntitle: IBM Event Streams Schema Registry from IBM CloudPak for Integration\ndescription: Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature\n---\n\nThis documentation aims to be a introductory hands-on lab on the IBM Event Streams Schema Registry installed throughtthe **IBM Cloud Pak for Integration V2020.1.X** on an Openshift cluster.\n\n## Index\n\n<AnchorLinks>\n  <AnchorLink>Requirements</AnchorLink>\n  <AnchorLink>IBM Event Streams Service Credentials</AnchorLink>\n  <AnchorLink>Python Demo Environment</AnchorLink>\n  <AnchorLink>Schema Registry</AnchorLink>\n  <AnchorLink>Schemas</AnchorLink>\n  <AnchorLink>Python Avro Producer</AnchorLink>\n  <AnchorLink>Python Avro Consumer</AnchorLink>\n  <AnchorLink>Schemas and Messages</AnchorLink>\n  <AnchorLink>Data Evolution</AnchorLink>\n  <AnchorLink>Security</AnchorLink>\n</AnchorLinks>\n\n## Requirements\n\nThis lab requires the following components to work against:\n\n1. An IBM Event Streams instance installed through the IBM CloudPak for Integration.\n\nOn your development workstation you will need:\n\n1. IBM Cloud Pak CLI - <https://www.ibm.com/support/knowledgecenter/SSGT7J_20.1/cloudctl/3.2.3/install_cli.html>\n1. IBM CLoud Pak CLI Event Streams plugin - <https://ibm.github.io/event-streams/installing/post-installation/#installing-the-command-line-interface-cli>\n1. Docker - <https://docs.docker.com/get-docker/>\n1. Our GitHub repository with the material for this lab (<https://github.com/ibm-cloud-architecture/refarch-eda-tools>) cloned on your laptop:\n\t1. Clone the github repository on your workstation on the location of your choice and change directory into `refarch-eda-tools/labs/es-cp4i-schema-lab` where we will be running the rest of the command from throughout this lab:\n\n```shell\n$ git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\nCloning into 'refarch-eda-tools'...\nremote: Enumerating objects: 185, done.\nremote: Counting objects: 100% (185/185), done.\nremote: Compressing objects: 100% (148/148), done.\nremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\nReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n\n$ cd refarch-eda-tools/labs/es-cp4i-schema-lab\n\n$ ls -all\ntotal 240\ndrwxr-xr-x   9 user  staff     288 20 May 19:33 .\ndrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n-rw-r--r--   1 user  staff    1012 20 May 19:33 Dockerfile\n-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\ndrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\ndrwxr-xr-x  27 user  staff     864 20 May 19:33 images\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\n-rw-r--r--   1 user  staff     286 20 May 19:33 kafka.properties\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n```\n\n\n## IBM Event Streams Credentials\n\nFirst thing we need to know is how to access/connect with our IBM Event Streams instance. For doing so, we can either use the GUI or the CLI.\n\n### GUI\n\n1. Go to you IBM Event Streams instance console\n\n\t![1](images/schema-registry-lab-cp4i/1.png)\n\n1. Click on _Connect to this cluster_\n\n\t![2](images/schema-registry-lab-cp4i/2.png)\n\nIn this panel, you will find\n\n1. The **Botstrap server** to connect your applications to in order to send and receive messages from your IBM Event Streams instance.\n1. The **API endpoint** which is also the **Schema Registry url** your applications will need to work with Apache Avro data schemas.\n1. A _Generate API key_ button to generate an **API key** and a _Certificates_ section to download either the **Java truststore** or the **PEM certificate** that your applications will need in order to authenticate and authorize against your IBM Event Streams instance.\n\n\t![3](images/schema-registry-lab-cp4i/3.png)\n\n### CLI\n\n1. Log into your cluster with the IBM CloudPak CLI\n\n\t```shell\n\t$ cloudctl login -a https://icp-console.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud -u admin -p ****** -n eventstreams-cp4i --skip-ssl-validation\n\tAuthenticating...\n\tOK\n\n\tTargeted account mycluster Account\n\n\tTargeted namespace eventstreams-cp4i\n\n\tConfiguring kubectl ...\n\tProperty \"clusters.mycluster\" unset.\n\tProperty \"users.mycluster-user\" unset.\n\tProperty \"contexts.mycluster-context\" unset.\n\tCluster \"mycluster\" set.\n\tUser \"mycluster-user\" set.\n\tContext \"mycluster-context\" created.\n\tSwitched to context \"mycluster-context\".\n\tOK\n\n\tConfiguring helm: /Users/user/.helm\n\tOK\n\t```\n\n1. Initialize the Event Streams CLI plugin\n\n\t```shell\n\t$ cloudctl es init\n\n\tIBM Cloud Platform Common Services endpoint:   https://icp-console.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tNamespace:                                     eventstreams-cp4i\n\tHelm release:                                  es-cp4i\n\tIBM Cloud Pak for Integration UI address:      https://ibm-icp4i-prod-integration.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams API endpoint:                    https://eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams API status:                      OK\n\tEvent Streams SSL client auth endpoint:        https://es-cp4i-ibm-es-clientauth-route-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams UI address:                      https://es-cp4i-ibm-es-ui-route-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams bootstrap address:               es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443\n\tOK\n\t```\n\nWe can see above the Event Streams **bootstrap address** and **API endpoint** (and Schema Registry url) that our applications will need in order to connect to this Event Streams instance\n\nTo be able to authenticate and authorize against your IBM Event Streams instance, you still need the Java truststore, the PEM certificate and an API Key:\n\n1. To download your **Java truststore** certificate, you can use the following command:\n\n\t```shell\n\t$ cloudctl es certificates\n\tCertificate successfully written to /Users/user/Workspace/es-cert.jks.\n\tOK\n\t```\n\n1. To download your **PEM certificate**, you can use the following command:\n\n\t```shell\n\t$ cloudctl es certificates --format pem\n\tCertificate successfully written to /Users/user/Workspace/es-cert.pem.\n\tOK\n\t```\n\n1. To generate your **API key**, you can use the following command:\n\n\t```shell\n\t$ cloudctl es iam-service-id-create --name eventstreams-lab-key --role administrator --all-topics --all-groups --all-txnids --all-schemas\n\tCreated service ID eventstreams-lab-key.\n\tCreated service policy for cluster.\n\tCreated service policy for all topics.\n\tCreated service policy for all consumer groups.\n\tCreated service policy for all Transactional IDs.\n\tCreated service policy for all schemas.\n\n\tPlease preserve the API key! It cannot be retrieved after it's created.\n\n\tService ID UUID   ServiceId-70288651-88df-436b-83f2-2ba7cbedcbeb\n\tAPI key           *****\n\tOK\n\t```\n\n<InlineNotification kind=\"warning\">\n\nWe recommend to carefully set appropriate roles as well as access to topics, groups, transaction IDs and schemas for the API keys that you generate.\n\n</InlineNotification>\n\n### Environment variables\n\nBased on the IBM Event Streams credentials we gathered in the sections above, we are going to set some environment variables that will be used throughout the rest of the lab for easiness.\n\n1. **KAFKA_BROKERS** which should take the value of **bootstrap server**:\n\n\t```shell\n\t$ export KAFKA_BROKERS=es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443\n\t```\n\n1. **KAFKA_APIKEY** which should take the value of the **API key** you have generated:\n\n\t```shell\n\t$ export KAFKA_APIKEY=*****\n\t```\n\n1. **PEM_CERT** which should take the value of the location where the PEM certificate will be located within the docker container we will use for our development environment (explained in later section). For now, just set this to `/tmp/lab/es-cert.pem`:\n\n\t```shell\n\t$ export PEM_CERT=/tmp/lab/es-cert.pem\n\t```\n\n\t(\\*) Don't forget to download both the PEM certificate and the Java truststore to your laptop (either using the GUI or the CLI) but you should have a `es-cert.jks` and `es-cert.pem` file in the same place you cloned this lab's github repository in the [requirements](#requirements) section.\n\n1. **SCHEMA_REGISTRY_URL** which should be a combination of the **API key** and the **API endpoint** (and Schema Registry url) in the form of:\n\n\t`https://token:<API_key>@<API_endpointt>`\n\n\t```shell\n\t$ export SCHEMA_REGISTRY_URL=https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\t```\n\n## Python Demo Environment\n\nGiven that students' workstations may vary quite a lot, not only on their operating system but also on the tools installed on them and the tools we need for our lab might install differently, we have opted to provide a python demo environment in the form of a Docker container where all the libraries and tools needed are already pre-installed.\n\n### Clone\n\n<InlineNotification kind=\"warning\">\n\nYou should have cloned this lab's GitHub repository already on the [requirements](#requirements) section so you should be able to jump to the next build subsection\n\n</InlineNotification>\n\nIn order to build our python demo environment we first need to clone the github repository where the assets live. This github repository is <https://github.com/ibm-cloud-architecture/refarch-eda-tools> and the specific assets we refer to can be found under the `labs/es-cp4i-schema-lab` folder:\n\n1. Clone the github repository on your workstation on the location of your choice:\n\n\t```shell\n\t$ git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\n\tCloning into 'refarch-eda-tools'...\n\tremote: Enumerating objects: 185, done.\n\tremote: Counting objects: 100% (185/185), done.\n\tremote: Compressing objects: 100% (148/148), done.\n\tremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\n\tReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\n\tResolving deltas: 100% (23/23), done.\n\t```\n\n1. Change directory into `refarch-eda-tools/labs/es-cp4i-schema-lab` to find the assets we will we working from now on for the python demo environment and few other scripts/applications:\n\n\t```shell\n\t$ cd refarch-eda-tools/labs/es-cp4i-schema-lab\n\n\t$ ls -all\n\ttotal 240\n\tdrwxr-xr-x   9 user  staff     288 20 May 19:33 .\n\tdrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n\t-rw-r--r--   1 user  staff    1012 20 May 19:33 Dockerfile\n\t-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\n\tdrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\n\tdrwxr-xr-x  27 user  staff     864 20 May 19:33 images\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\n\t-rw-r--r--   1 user  staff     286 20 May 19:33 kafka.properties\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n\t```\n\n### Build\n\nThis Docker container can be built by using the [Dockerfile](Dockerfile) provided within this github repository.\n\nTo build your python demo environment Docker container, execute the following on your workstation:\n\n```shell\n$ docker build -t \"ibmcase/python-schema-registry-lab:latest\" .\n```\n<InlineNotification kind=\"warning\">\n\n**WARNING:**\n\n* Mind the **dot** at the end of the command.\n* Be consistent throughout the lab with the name you give to the Docker container.\n\n</InlineNotification>\n\n### Run\n\nIn order to run the python demo environment Docker container, execute the following on your workstation:\n\n1. Make sure you have declared your `KAFKA_BROKERS`, `KAFKA_APIKEY` and `SCHEMA_REGISTRY_URL` environment variables as explain in the [IBM Event Streams Service Credentials](#ibm-event-streams-service-credentials) section.\n\n1. Run the python demo environment container\n\n\t```shell\n\t$ docker run -e KAFKA_BROKERS=$KAFKA_BROKERS \\\n\t\t\t\t -e KAFKA_APIKEY=$KAFKA_APIKEY \\\n\t\t\t\t -e PEM_CERT=$PEM_CERT \\\n\t\t\t\t -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL \\\n\t\t\t\t -v ${PWD}:/tmp/lab \\\n\t\t\t\t --rm \\\n\t\t\t\t -ti ibmcase/python-schema-registry-lab:latest bash\n\t```\n\n1. Go to `/tmp/lab` to find all the assets you will need to complete this lab.\n\n<InlineNotification kind=\"info\">\n\n**INFO:** we have mounted this working directory into the container so any changes to any of the files apply within the container. This is good as we do not need to restart the python demo environment Docker container if we want to do any changes to the files.\n\n</InlineNotification>\n\n### Exit\n\nOnce you are done with the python demo environment container, just execute `exit` and you will get out of the container and the container will automatically be removed from your system.\n\n## Schema Registry\n\n![diagram](images/schema-registry-lab-cp4i/schema-registry.png)\n\nOne of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro (<https://avro.apache.org/docs/current/>). To learn more about Apache Avro, how to define Apache Avro data schemas and more, we strongly recommend to read through our documentation on Avro and data schemas [here](/technology/avro-schemas/)\n\nIBM Event Streams development team has developed a Schema Registry to work along your Kafka cluster to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. The Schema Registry will also provide the ability for schemas to evolve in time.\n\n### Accessing the Schema Registry\n\nTo access the schema registry, you simply need to click on the Schema Registry button on the main left hand vertical menu bar:\n\n  ![4](images/schema-registry-lab-cp4i/4.png)\n\nYou can also interact with the Schema Registry through the IBM Event Streams CLI:\n\n```bash\n$ cloudctl es --help | grep schema\n   schema                       Display details of a schema.\n   schema-add                   Add a new schema or a new version of a schema to the registry.\n   schema-modify                Modify an entire schema or a specific schema version\n   schema-remove                Remove a schema or a version of a schema from the registry.\n   schema-verify                Verify a schema file.\n   schemas                      List the schemas in the registry.\n   schemas-export               Export the schemas in the registry to a zip file.\n   schemas-import               Import a set of schemas into the registry from a zip file.\n```\n\n## Schemas\n\nIn this section we will finally get our hands dirty with the IBM Event Steams Schema Registry capability by working with Apache Avro schemas and the Schema Registry.\n\n### Create a schema\n\nLet's see how can we create a schema to start playing with.\n\n#### UI\n\nThe IBM EVent Streams user interface allow us to create schemas only from _json_ or Avro schema _avsc_ files.\n\n1. Create an Avro schema file **avsc** with your schema:\n\n\t```bash\n\t$ echo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-ui.avsc\n\t```\n\n1. On the IBM Event Streams Schema Registry User Interface, Click on _Add schema_ button on the top right corner.\n\n1. Click on _Upload definition_ button on the left hand side and select the `demoschema-ui.avsc` file we just created.\n\n1. You should now see you Avro schema loaded in the UI with two tabs, definition and preview to make sure your schema looks as desired:\n\n\t![5](images/schema-registry-lab-cp4i/5.png)\n\n1. Click on _Add schema_ button at the top right corner and you should now see that schema listed among your other schemas.\n\n#### CLI\n\n1. Create another Avro schema **avsc** file with a different schema:\n\n\t```bash\n\t$ echo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_CLI\",\n\t\"namespace\": \"schemas.demo.cli\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-cli.avsc\n\t```\n\n1. Create a schema by executing the following command:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc\n\n\tSchema demoSchema_CLI is active.\n\n\tVersion   Version ID   Schema           State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:30:42 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_CLI to the registry.\n\tOK\n\t```\n\n### List schemas\n\n#### UI\n\nIn order to list the schemas in the UI you just simply need to open up the Schema Registry User Interface and schemas will get listed in there automatically. You also have a search tool bar at the top. You can also see more details about your schema by clicking the drop down arrow on its left:\n\n  ![6](images/schema-registry-lab-cp4i/6.png)\n\n#### CLI\n\n1. Execute the following command to list the schemas in your Schema Registry:\n\n```bash\n$ cloudctl es schemas\n\nSchema                State    Latest version   Latest version ID   Updated\nAircraftAssignment    active   1.0.0            1                   Tue, 09 Jun 2020 18:53:23 UTC\ndefaultValue          active   1.0.0            1                   Sat, 06 Jun 2020 18:14:41 UTC\ndemoSchema_CLI        active   1.0.0            1                   Thu, 25 Jun 2020 11:30:42 UTC\ndemoSchema_UI         active   1.0.0            1                   Thu, 25 Jun 2020 11:25:04 UTC\nFlightTimes           active   1.0.0            1                   Tue, 09 Jun 2020 18:56:15 UTC\npacs008               active   1.0.0            1                   Sun, 07 Jun 2020 14:47:10 UTC\npacs008_cleansed      active   1.0.0            1                   Sun, 07 Jun 2020 14:46:40 UTC\npain013               active   1.0.0            1                   Mon, 08 Jun 2020 12:45:15 UTC\npain013_cleansed      active   1.0.0            1                   Mon, 08 Jun 2020 12:09:39 UTC\npartner_sample_pain   active   1.0.0            1                   Sun, 07 Jun 2020 14:48:07 UTC\npartner_sample2       active   1.0.0            1                   Sun, 07 Jun 2020 14:48:16 UTC\nrtp_summation         active   1.0.0            1                   Sun, 07 Jun 2020 14:48:30 UTC\nSchedules             active   1.0.0            1                   Tue, 09 Jun 2020 18:59:23 UTC\nOK\n```\n\n### Delete schemas\n\n#### UI\n\n1. Click on the schema you want to delete.\n1. Click on the _Manage schema_ tab at the top.\n1. Click on _Remove schema_\n\n\t![7](images/schema-registry-lab-cp4i/7.png)\n\n#### CLI\n\nTo remove a schema using the CLI, simply execute the following command and confirm:\n\n```bash\n$ cloudctl es schema-remove demoSchema_CLI\nRemove schema demoSchema_CLI and all versions? [y/n]> y\nSchema demoSchema_CLI and all versions removed.\nOK\n```\n\n### Create new schema version\n\nTo create a new version of a schema,\n\n1. Let's first create again the previous two schemas:\n\n```bash\n$ cloudctl es schema-add --file demoshema-ui.avsc\n\nSchema demoSchema_UI is active.\n\nVersion   Version ID   Schema          State    Updated                         Comment\n1.0.0     1            demoSchema_UI   active   Thu, 25 Jun 2020 11:47:36 UTC\n\nAdded version 1.0.0 of schema demoSchema_UI to the registry.\nOK\n```\n```bash\n$ cloudctl es schema-add --file demoshema-cli.avsc\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n\nAdded version 1.0.0 of schema demoSchema_CLI to the registry.\nOK\n```\n\n1. Add a new attribute to the schemas by editing their Avro schema avsc files:\n\n```bash\n$ cat demoshema-ui.avsc\n{\n  \"type\":\"record\",\n  \"name\":\"demoSchema_UI\",\n  \"namespace\": \"schemas.demo.ui\",\n  \"fields\":[\n    {\"name\": \"eventKey\",\"type\":\"string\"},\n    {\"name\": \"message\",\"type\":\"string\"},\n    {\"name\": \"attribute1\",\"type\":\"string\"}]\n}\n```\n\n#### UI\n\n1. Click on the schema you want to create a new version for.\n1. Click on the _Add new version_ button on the left hand side.\n1. Click on _Upload definition_ button on the left hand side.\n1. Select the Avro schema avsc file and click ok.\n\n\t![8](images/schema-registry-lab-cp4i/8.png)\n\n<InlineNotification kind=\"error\">\n\n**ERROR:** The error we are seeing on the screen is because the IBM Event Streams Schema Registtry enforces full compatibility: <https://ibm.github.io/event-streams/schemas/creating/#adding-new-schema-versions>\n\n</InlineNotification>\n\n**Full compatibility** for data schemas means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. More on data schema compatibility on the section [Data Evolution](#data-evolution) towards the end of this lab.\n\nAs explained in the error notification above, we need to add a default value for our new attribute in our data schema so that messages serialized with an older version of the data schema which won't contain this new attribute can later be deserialized with the newer version of the data schema that expects such attribute. By providing a default value, we allow deserializers to consume messages that do not contain newer attributes.\n\n1. Add a default value for the new attribute:\n\n\t```bash\n\t$ cat demoshema-ui.avsc\n\t{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"},\n\t\t{\"name\": \"attribute1\",\"type\":\"string\",\"default\": \"whatever\"}]\n\t}\n\t```\n\n1. Repeat the steps for adding a new version of a schema above.\n1. This time you should see that the schema is valid:\n\n\t![9](images/schema-registry-lab-cp4i/9.png)\n\n1. However, it still does not let us add this new version to the data schema until we actually provide a version for it. Click on the _Add +_ link on the right of the version attribute of the schema and give it `2.0.0` for example (hit enter for the version to take the value you type in).\n1. Click on _Add schema_.\n1. You should now see the two versions for your data schema on the left hand side.\n\n\t![10](images/schema-registry-lab-cp4i/10.png)\n\n1. If you go back to the Schema Registry page where all your schemas are listed, you should now see that the latest version for your data schema is `2.0.0` now.\n\n#### CLI\n\n1. If we try to add the new version of the schema from its `demoschema-cli.avsc` Avro schema file, we will get the same error as in the previous UI example:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 400. Avro schema is not compatible with latest schema version: Compatibility type 'MUTUAL_READ' does not hold between 1 schema(s) in the chronology because: Schema[0] has incompatibilities: ['READER_FIELD_MISSING_DEFAULT_VALUE: attribute1' at '/fields/2'].\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI to the registry.\n\t```\n\n1. Add the default value for the new attribute in your Avro schema avsc file and try to add that new version of the schema:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 409. Schema version name already exists\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI to the registry.\n\t```\n\n1. We see that we still have an error because we have not specified a new version value. Specify a new version value when adding this new version of the schema:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc --version 2.0.0\n\n\tSchema demoSchema_CLI is active.\n\n\tVersion   Version ID   Schema           State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n\t2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\n\n\tAdded version 2.0.0 of schema demoSchema_CLI to the registry.\n\tOK\n\t```\n\n### Get latest version of a schema\n\n#### UI\n\nIn order to see the latest version of a data schema using the UI, we just need to go to the Schema Registry web user interface and click on the expand arrow buttton that is on the left:\n\n ![11](images/schema-registry-lab-cp4i/11.png)\n\n#### CLI\n\nIn order to see the latest version of a data schema using the CLI, we simply need to run the following command:\n\n```bash\n$ cloudctl es schema demoSchema_CLI --version 2\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"attribute1\",\n      \"type\": \"string\",\n      \"default\": \"whatever\"\n    }\n  ]\n}\n```\n\n(\\*) The version you specify is actually the version ID (2) rather than the version name we gave to the newer schema version (2.0.0):\n\n```bash\n$ cloudctl es schema demoSchema_CLI\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\nOK\n```\n\n### Get specific version of a schema\n\n#### UI\n\nTo see a specific version of a schema, go to the Schema Registry web user interface and click on the schema you want to see the version for. You will now see how many version of the schema you have and you can click on any of these in order to see more details about it.\n\n  ![12](images/schema-registry-lab-cp4i/12.png)\n\n#### CLI\n\nTo see a specific version of a schema using the CLI, simply run the following command with the version ID you would like to get retrieved:\n\n\n```bash\n$ cloudctl es schema demoSchema_CLI --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n### Listing all versions of a schema\n\n#### UI\n\nTo list all versions of schema in the Schema Registry user interface, you simply need to click on the data schema you want and a new page will display these:\n\n  ![12](images/schema-registry-lab-cp4i/12.png)\n\n#### CLI\n\nIn order to display all versions of a schema, run the following command:\n\n```bash\n$ cloudctl es schema demoSchema_CLI\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\nOK\n```\n\n### Deleting a version of a schema\n\n#### UI\n\nIn order to delete a version of a schema using the Schema Registry user interface,\n\n1. Click on the data schema you want a version of it deleted for.\n1. Select the version you want to delete on the left hand side.\n1. Click on _Manage version_ button that is on the top right corner within the main box in the center of the page.\n1. Click on _Remove version_.\n\n\t![13](images/schema-registry-lab-cp4i/13.png)\n\n#### CLI\n\nIn order to delete a version of a schema through the CLI, execute the following command:\n\n```bash\n$ cloudctl es schema-remove demoSchema_CLI --version 1\n\nRemove version with ID 1 of schema demoSchema_CLI? [y/n]> y\nVersion with ID 1 of schema demoSchema_CLI removed.\nOK\n```\n\nWe can see only version 2 now:\n\n```bash\n$ cloudctl es schema demoSchema_CLI\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\nOK\n\n```\n\n## Python Avro Producer\n\nIn this section we describe the python scripts we will be using in order to be able to produce **avro** messages to a Kafka topic.\n\n### Produce Message\n\nThe python script that we will use to send an avro message to a Kafka topic is [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/src/ProduceAvroMessage.py) where we have the following:\n\n1. A function to parse the arguments:\n\n\t```python\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for this script are: \" , str(sys.argv))\n\t\tif len(sys.argv) == 2:\n\t\t\tTOPIC_NAME = sys.argv[1]\n\t\telse:\n\t\t\tprint(\"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\")\n\t\t\texit(1)\n\t```\n\n1. A function to create the event to be sent:\n\n\t```python\n\tdef createEvent():\n\t\tprint('Creating event...')\n\n\t\tkey = {\"key\": 1}\n\t\tvalue = {\"message\" : \"This is a test message\"}\n\n\t\tprint(\"DONE\")\n\n\t\treturn json.dumps(value), json.dumps(key)\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments\n\t1. Get the Avro schemas for the key and value of the event\n\t1. Create the Event to be sent\n\t1. Print it out for reference\n\t1. Create the Kafka Avro Producer and configure it\n\t1. Send the event\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Get the Kafka topic name\n\t\tparseArguments()\n\t\t# Get the avro schemas for the message's key and value\n\t\tevent_value_schema = getDefaultEventValueSchema(DATA_SCHEMAS)\n\t\tevent_key_schema = getDefaultEventKeySchema(DATA_SCHEMAS)\n\t\t# Create the event\n\t\tevent_value, event_key = createEvent()\n\t\t# Print out the event to be sent\n\t\tprint(\"--- Event to be published: ---\")\n\t\tprint(event_key)\n\t\tprint(event_value)\n\t\tprint(\"----------------------------------------\")\n\t\t# Create the Kafka Avro Producer\n\t\tkafka_producer = KafkaProducer(KAFKA_BROKERS,KAFKA_APIKEY,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the Kafka Avro Producer\n\t\tkafka_producer.prepareProducer(\"ProduceAvroMessagePython\",event_key_schema,event_value_schema)\n\t\t# Publish the event\n\t\tkafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n\t```\n\nAs you can see, this python code depends on a Kafka Avro Producer and an Avro Utils for loading the Avro schemas which are explained next.\n\n### Avro Utils\n\nThis script, called [avroEDAUtils.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/avro_files/utils/avroEDAUtils.py), contains some very simple utility functions to be able to load Avro schemas from their **avsc** files in order to be used by the Kafka Avro Producer.\n\n1. A function to get the key and value Avro schemas for the messages to be sent:\n\n\t```python\n\tdef getDefaultEventValueSchema(schema_files_location):\n\t# Get the default event value data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_value_schema = LoadAvsc(schema_files_location + \"/default_value.avsc\", known_schemas)\n\treturn default_event_value_schema\n\n\tdef getDefaultEventKeySchema(schema_files_location):\n\t# Get the default event key data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_key_schema = LoadAvsc(schema_files_location + \"/default_key.avsc\", known_schemas)\n\treturn default_event_key_schema\n\t```\n\t(\\*) Where `known_schemas` is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this.\n\n1. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary:\n\n\t```python\n\tdef LoadAvsc(file_path, names=None):\n\t# Load avsc file\n\t# file_path: path to schema file\n\t# names(optional): avro.schema.Names object\n\tfile_text = open(file_path).read()\n\tjson_data = json.loads(file_text)\n\tschema = avro.schema.SchemaFromJSONData(json_data, names)\n\treturn schema\n\t```\n\n### Kafka Avro Producer\n\nThis script, called [KcAvroProducer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/kafka/KcAvroProducer.py), will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method:\n\n1. Initialize and prepare the Kafka Producer\n\n\t```python\n\tclass KafkaProducer:\n\n\t\tdef __init__(self,kafka_brokers = \"\",kafka_apikey = \"\",schema_registry_url = \"\"):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.kafka_apikey = kafka_apikey\n\t\t\tself.schema_registry_url = schema_registry_url\n\n\t\tdef prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n\t\t\toptions ={\n\t\t\t\t\t'bootstrap.servers':  self.kafka_brokers,\n\t\t\t\t\t'schema.registry.url': self.schema_registry_url,\n\t\t\t\t\t'group.id': groupID,\n\t\t\t\t\t'security.protocol': 'SASL_SSL',\n\t\t\t\t\t'sasl.mechanisms': 'PLAIN',\n\t\t\t\t\t'sasl.username': 'token',\n\t\t\t\t\t'sasl.password': self.kafka_apikey,\n\t\t\t\t\t'ssl.ca.location': os.environ['PEM_CERT'],\n\t\t\t\t\t'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print out the configuration\n\t\t\tprint(\"--- This is the configuration for the avro producer: ---\")\n\t\t\tprint(options)\n\t\t\tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro Producer\n\t\t\tself.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n\t```\n\n1. Publish method\n\n\t```python\n\tdef publishEvent(self, topicName, value, key):\n\t\t# Produce the Avro message\n\t\t# Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n\t\tself.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n\t\t# Flush\n\t\tself.producer.flush()\n\t```\n\n### Run\n\nWe will see in the following section [Schemas and Messages](#schemas-and-messages) how to send Avro messages according with their schemas to IBM Event Streams.\n\n## Python Avro Consumer\n\nIn this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.\n\n### Consume Message\n\nThe python script that we will use to consume an Avro message from a Kafka topic is [ConsumeAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/src/ConsumeAvroMessage.py) where we have the following:\n\n1. A function to parse arguments:\n\n\t```python\n\t# Parse arguments to get the container ID to poll for\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for the script are: \" , str(sys.argv))\n\t\tif len(sys.argv) != 2:\n\t\t\tprint(\"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\")\n\t\t\texit(1)\n\t\tTOPIC_NAME = sys.argv[1]\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments to get the topic to read from\n\t1. Create the Kafka Consumer and configure it\n\t1. Poll for next avro message\n\t1. Close the Kafka consumer\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Parse arguments\n\t\tparseArguments()\n\t\t# Create the Kafka Avro consumer\n\t\tkafka_consumer = KafkaConsumer(KAFKA_BROKERS,KAFKA_APIKEY,TOPIC_NAME,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the consumer\n\t\tkafka_consumer.prepareConsumer()\n\t\t# Consume next Avro event\n\t\tkafka_consumer.pollNextEvent()\n\t\t# Close the Avro consumer\n\t\tkafka_consumer.close()\n\t```\n\nAs you can see, this python code depends on a Kafka Consumer which is explained next.\n\n### Kafka Avro Consumer\n\nThis script, called [KcAvroConsumer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/kafka/KcAvroConsumer.py), will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method:\n\n1. Initialize and prepare the new Kafka consumer:\n\n\t```python\n\tclass KafkaConsumer:\n\n\t\tdef __init__(self, kafka_brokers = \"\", kafka_apikey = \"\", topic_name = \"\", schema_registry_url = \"\", autocommit = True):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.kafka_apikey = kafka_apikey\n\t\t\tself.topic_name = topic_name\n\t\t\tself.schema_registry_url = schema_registry_url\n\t\t\tself.kafka_auto_commit = autocommit\n\n\t\t# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n\t\tdef prepareConsumer(self, groupID = \"pythonconsumers\"):\n\t\t\toptions ={\n\t\t\t\t\t'bootstrap.servers':  self.kafka_brokers,\n\t\t\t\t\t'group.id': groupID,\n\t\t\t\t\t'auto.offset.reset': 'earliest',\n\t\t\t\t\t'schema.registry.url': self.schema_registry_url,\n\t\t\t\t\t'enable.auto.commit': self.kafka_auto_commit,\n\t\t\t\t\t'security.protocol': 'SASL_SSL',\n\t\t\t\t\t'sasl.mechanisms': 'PLAIN',\n\t\t\t\t\t'sasl.username': 'token',\n\t\t\t\t\t'sasl.password': self.kafka_apikey,\n\t\t\t\t\t'ssl.ca.location': os.environ['PEM_CERT'],\n\t\t\t\t\t'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print the configuration\n\t\t\tprint(\"--- This is the configuration for the Avro consumer: ---\")\n        \tprint(options)\n        \tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro consumer\n\t\t\tself.consumer = AvroConsumer(options)\n\t\t\t# Subscribe to the topic\n\t\t\tself.consumer.subscribe([self.topic_name])\n\t```\n\n1. Poll next event method:\n\n\t```python\n\t# Prints out the message\n\tdef traceResponse(self, msg):\n        print('[Message] - Next message consumed from {} partition: [{}] at offset {} with key {} and value {}'\n                    .format(msg.topic(), msg.partition(), msg.offset(), msg.key(), msg.value() ))\n\n\t# Polls for next event\n\tdef pollNextEvent(self):\n\t\t# Poll for messages\n\t\tmsg = self.consumer.poll(timeout=10.0)\n\t\t# Validate the returned message\n\t\tif msg is None:\n\t\t\tprint(\"[INFO] - No new messages on the topic\")\n\t\telif msg.error():\n\t\t\tif (\"PARTITION_EOF\" in msg.error()):\n\t\t\t\tprint(\"[INFO] - End of partition\")\n\t\t\telse:\n\t\t\t\tprint(\"[ERROR] - Consumer error: {}\".format(msg.error()))\n\t\telse:\n\t\t\t# Print the message\n\t\t\tmsgStr = self.traceResponse(msg)\n\t```\n\n### Run\n\nWe will see in the following section [Schemas and Messages](#schemas-and-messages) how to consume Avro messages.\n\n## Schemas and Messages\n\nIn this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python scripts presented above in the [Python Avro Producer](#python-avro-producer) and [Python Avro Consumer](#python-avro-consumer).\n\nOnce again, we are going to run these scripts in the python demo environment we presented earlier in this lab in [this section](#python-demo-environment). Please, review that section in order to understand how to run the environment in your local workstation.\n\n1. Make sure you have a newly created topic for this exercise that you can create either in the IBM Event Streams UI on the topics section or by using the following IBM Event Streams CLI command:\n\n\t```shell\n\t$ cloudctl es topic-create test-schema --partitions 1 --replication-factor 1\n\n\tCreated topic test-schema\n\tOK\n\t```\n\n1. Start your python environment with:\n\n\t```shell\n\t$ docker run -e KAFKA_BROKERS=$KAFKA_BROKERS \\\n\t\t\t\t -e KAFKA_APIKEY=$KAFKA_APIKEY \\\n\t\t\t\t -e PEM_CERT=$PEM_CERT \\\n\t\t\t\t -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL \\\n\t\t\t\t -v ${PWD}:/tmp/lab \\\n\t\t\t\t --rm \\\n\t\t\t\t -ti ibmcase/python-schema-registry-lab:latest bash\n\t```\n\n### Create a message\n\nIn order to create a message, we execute the `ProduceAvroMessage.py` within the `/tmp/lab/src` folder in our python demo environment. This script, as you could see in the [Python Avro Producer](#python-avro-producer) section, it is sending the event with key `{'key': '1'}` and value `{'message': 'This is a test message'}` according to the schemas defined in [default_key.avsc](avro_files/default_key.avsc) and [default_value.avsc](avro_files/default_value.avsc) for the key and value of the event respectively.\n\n```shell\npython ProduceAvroMessage.py test-schema\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema [0]\n```\n\nWe can see our new message delivered in the `test-schema` topic by\n\n1. Go into the topics page in the IBM Event Streams UI\n\n\t![14](images/schema-registry-lab-cp4i/14.png)\n\n1. Click on the topic and then on the _Messages_ tab at the top. Finally, click on a message to see it displayed in a hovering card on the right hand side of the screen\n\n\t![15](images/schema-registry-lab-cp4i/15.png)\n\n<InlineNotification kind=\"info\">\n\n**INFO:** Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas.\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\n**WARNING:** Most of the Avro producer clients, whether it is in Java, Python or many other languages, give users the ability to **auto-register** a schema automatically with the specified schema registry in its configuration.\n\n</InlineNotification>\n\nIf we look know at the schemas our schema registry has:\n\n```shell\n$ cloudctl es schemas\n\nSchema                     State    Latest version   Latest version ID   Updated\ndemoSchema_CLI             active   2.0.0            2                   Thu, 25 Jun 2020 14:59:11 UTC\ndemoSchema_UI              active   2.0.0            2                   Thu, 25 Jun 2020 14:21:09 UTC\ntest-schema-key-7jffxi     active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\ntest-schema-value-a7paxm   active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\nOK\n```\n\nwe see two schemas, `test-schema-key-7jffxi` and `test-schema-value-a7paxm`, which in fact correspond to the Avro data schema used for the `key` ([default_key.avsc](avro_files/default_key.avsc)) and the `value` ([default_value.avsc](avro_files/default_value.avsc)) of events sent to the `test-schema` topic in the [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/src/ProduceAvroMessage.py) as explained before sending the message.\n\nTo make sure of what we are saying, we can inspect those schemas:\n\n```shell\n$ cloudctl es schema test-schema-key-7jffxi --version 1\n{\n  \"type\": \"record\",\n  \"name\": \"defaultKey\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"int\",\n      \"name\": \"key\",\n      \"doc\": \"We expect any int as the event key\"\n    }\n  ],\n  \"doc\": \"Default Message's key Avro data schema\"\n}\n```\n\n```shell\n$ cloudctl es schema test-schema-value-a7paxm --version 1\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nIf I now decided that my events should contain another attribute, I would modify the event value schema ([default_value.avsc](avro_files/default_value.avsc)) to reflect that as well as `ProduceAvroMessage.py` to send that new attribute in the event it sends:\n\n```shell\npython ProduceAvroMessage.py test-schema\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\", \"anotherAttribute\": \"Just another test string\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema [0]\n```\n\nI can see that an event with a new attribute has been sent:\n\n ![16](images/schema-registry-lab-cp4i/16.png)\n\nAnd I can also see that the new shcema has got registered as well:\n\n```shell\n$ cloudctl es schemas\nSchema                     State    Latest version   Latest version ID   Updated\ndemoSchema_CLI             active   2.0.0            2                   Thu, 25 Jun 2020 14:59:11 UTC\ndemoSchema_UI              active   2.0.0            2                   Thu, 25 Jun 2020 14:21:09 UTC\ntest-schema-key-7jffxi     active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\ntest-schema-value-a7paxm   active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\ntest-schema-value-nc2q7    active   1                1                   Mon, 29 Jun 2020 16:38:16 UTC\nOK\n```\n\nIf I inspect that new schema, I see my new attribute in it:\n\n```shell\n$ cloudctl es schema test-schema-value-nc2q7 --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\n<InlineNotification kind=\"info\">\n\nThe schema evolution above (test-schema-value-nc2q7) should have got registered as a new version of the already existing schema (test-schema-value-a7paxm). IBM Event Streams allows schemas to auto-register themselves when when these are sent along with a message from a producer application. However, the Schema Registry does not pick \"new\" schemas up as a new version of a previous schema and simply creates a new schema. Anyway, when reading messages off the topic, Schema Registry handles well what schema to return back to the receiver application so messages can get properly deserialized. Will see that in the next section.\n\n</InlineNotification>\n\n<InlineNotification kind=\"error\">\n\n**SECURITY:** As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is **NOT** a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the [Security](#security) section how we can control schema registration and evolution based on roles at the schema level also.\n\n</InlineNotification>\n\n### Create a non-compliant message\n\n\nLet's see what happens if we send a message that does not comply with its Avro data schema. Let's say that I send the following message:\n\n```shell\nkey = {\"key\": 1}\nvalue = {\"message\" : 12345}\n```\n\nand this is the output of that attempt:\n\n```shell\npython ProduceAvroMessage.py test-schema\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": 12345}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\nTraceback (most recent call last):\n  File \"ProduceAvroMessage.py\", line 74, in <module>\n    kafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n  File \"/tmp/lab/kafka/KcAvroProducer.py\", line 42, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 99, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 118, in encode_record_with_schema\n    return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 152, in encode_record_with_schema_id\n    writer(record, outf)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 86, in <lambda>\n    return lambda record, fp: writer.write(record, avro.io.BinaryEncoder(fp))\n  File \"/root/.local/lib/python3.7/site-packages/avro/io.py\", line 771, in write\n    raise AvroTypeException(self.writer_schema, datum)\navro.io.AvroTypeException: The datum {'message': 12345} is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nAs we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply (the message value attribute we are sending is an integer rather than a string).\n\nTherefore, using Avro schemas with IBM Event Streams give us the ability to build our system with **robustness** protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.\n\n### Consume a message\n\nIn order to consume a message, we execute the `ConsumeAvroMessage.py` within the `/tmp/lab/src` folder in our python demo environment:\n\n```shell\npython ConsumeAvroMessage.py test-schema\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\npython ConsumeAvroMessage.py test-schema\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\nAs you can see, our script was able to read the Avro messages from the `test-schema` topic and map that back to their original structure thanks to the Avro schemas:\n\n```shell\n[Message] - Next message consumed from test-schema partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n[Message] - Next message consumed from test-schema partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\n## Data Evolution\n\nSo far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich CLI IBM Event Streams provides to interact with.\n\nHowever, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying [Event Storming](../../methodology/event-storming/) or [Domain Driven Design](../../methodology/domain-driven-design/) for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.\n\nBut it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to [hundreds of years](https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/)) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the **compatibility** of old and new data schemas and, in fact, old and new data at the end of the day.\n\n**The IBM Event Streams Schema Registry enforces full compatibility when creating a new version of a schema**. Full compatibility means that **old data can be read with the new data schema, and new data can also be read with the last data schema**.\n\nIn data formats like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. Let's see then how this affects when you want your data to evolve in a way that it needs to add a new attribute or delete an existing attribute.\n\nBut let's see what that means in terms of adding and removing attributes from your data schema.\n\n### Adding a new attribute\n\nAlthough we have already seen this in the adding a new version of a schema section, let's try to add a new version of our `test-schema-value` schema where we have a new attribute. Remember, our `default_schema.avsc` already contains a new attribute than the original one but that it got registered as a new schema rather than as a new version of the original one. Let's reuse that Avro schema file to register it as a new version.\n\nWhen doing so from the UI, we see the following error:\n\n  ![17](images/schema-registry-lab-cp4i/17.png)\n\nThe reason, as alread explained in the add a new version of a schema section, is because full compatibility dictates that you can only add new attributes to a schema if these have a default value. Reason being that a receiver should be able to deserialize messages produced with an older schema using the newer schema. Because old messages were written with an older schema that did not contain our new attribute, those messages won't have that attribute so we need to provide a default value for it in our never version of the schema so that the receiver is able to deserialize those older messages with the newer schema.\n\nIf we add the default value for the new attribute, we see that our newer version is now compatible:\n\n  ![18](images/schema-registry-lab-cp4i/18.png)\n\nand that it gets registered fine:\n\n  ![19](images/schema-registry-lab-cp4i/19.png)\n\n### Removing an existing attribute\n\nWhat if we now wanted to remove the original `message` attribute from our schema. Let's remove it from the `default_value.avsc` file and try to register that new version:\n\n  ![20](images/schema-registry-lab-cp4i/20.png)\n\nWe, again, get the same error. And the reason is because receivers must be able to read and deserialize messages produced with the newer schema (that is, without the `message` attribute) but with the older schema (that is, with the schema version that enforces the existence of the `message` attribute).\n\nIn order to work this around, what we need to do is to register first an intermediate schema that defines a default value for the `message` attribute:\n\n  ![21](images/schema-registry-lab-cp4i/21.png)\n\nOnce we have a default value for the `message` attribute, we can register a new version of the schema that finally removes that attribute:\n\n  ![22](images/schema-registry-lab-cp4i/22.png)\n\n## Security\n\nAs we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams), etc since we dont want everyone and everything to be, for instance, creating or deleting topics, schemas, etc.\n\nYou can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in policy definitions:\n\n- Cluster (cluster): you can control which users and applications can connect to the cluster.\n- Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.\n- Consumer groups (group): you can control an application’s ability to join a consumer group.\n- Transactional IDs (txnid): you can control the ability to use the transaction capability in Kafka.\n\nIn the context of the Schema Registry, this is something you need to bear in mind when creating the API key that your applications will use to produce messages since they could pontetially be creating new topics, modifying data schemas, etc.\n\nYou can find more information about how to secure your IBM Event Streams resources in the official documentation at: <https://ibm.github.io/event-streams/2019.4/security/managing-access/>\n","type":"Mdx","contentDigest":"4f4c6c8a17ab6635e759ab564c02a85f","owner":"gatsby-plugin-mdx","counter":729},"frontmatter":{"title":"IBM Event Streams Schema Registry from IBM CloudPak for Integration","description":"Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature"},"exports":{},"rawBody":"---\ntitle: IBM Event Streams Schema Registry from IBM CloudPak for Integration\ndescription: Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature\n---\n\nThis documentation aims to be a introductory hands-on lab on the IBM Event Streams Schema Registry installed throughtthe **IBM Cloud Pak for Integration V2020.1.X** on an Openshift cluster.\n\n## Index\n\n<AnchorLinks>\n  <AnchorLink>Requirements</AnchorLink>\n  <AnchorLink>IBM Event Streams Service Credentials</AnchorLink>\n  <AnchorLink>Python Demo Environment</AnchorLink>\n  <AnchorLink>Schema Registry</AnchorLink>\n  <AnchorLink>Schemas</AnchorLink>\n  <AnchorLink>Python Avro Producer</AnchorLink>\n  <AnchorLink>Python Avro Consumer</AnchorLink>\n  <AnchorLink>Schemas and Messages</AnchorLink>\n  <AnchorLink>Data Evolution</AnchorLink>\n  <AnchorLink>Security</AnchorLink>\n</AnchorLinks>\n\n## Requirements\n\nThis lab requires the following components to work against:\n\n1. An IBM Event Streams instance installed through the IBM CloudPak for Integration.\n\nOn your development workstation you will need:\n\n1. IBM Cloud Pak CLI - <https://www.ibm.com/support/knowledgecenter/SSGT7J_20.1/cloudctl/3.2.3/install_cli.html>\n1. IBM CLoud Pak CLI Event Streams plugin - <https://ibm.github.io/event-streams/installing/post-installation/#installing-the-command-line-interface-cli>\n1. Docker - <https://docs.docker.com/get-docker/>\n1. Our GitHub repository with the material for this lab (<https://github.com/ibm-cloud-architecture/refarch-eda-tools>) cloned on your laptop:\n\t1. Clone the github repository on your workstation on the location of your choice and change directory into `refarch-eda-tools/labs/es-cp4i-schema-lab` where we will be running the rest of the command from throughout this lab:\n\n```shell\n$ git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\nCloning into 'refarch-eda-tools'...\nremote: Enumerating objects: 185, done.\nremote: Counting objects: 100% (185/185), done.\nremote: Compressing objects: 100% (148/148), done.\nremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\nReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n\n$ cd refarch-eda-tools/labs/es-cp4i-schema-lab\n\n$ ls -all\ntotal 240\ndrwxr-xr-x   9 user  staff     288 20 May 19:33 .\ndrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n-rw-r--r--   1 user  staff    1012 20 May 19:33 Dockerfile\n-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\ndrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\ndrwxr-xr-x  27 user  staff     864 20 May 19:33 images\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\n-rw-r--r--   1 user  staff     286 20 May 19:33 kafka.properties\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n```\n\n\n## IBM Event Streams Credentials\n\nFirst thing we need to know is how to access/connect with our IBM Event Streams instance. For doing so, we can either use the GUI or the CLI.\n\n### GUI\n\n1. Go to you IBM Event Streams instance console\n\n\t![1](images/schema-registry-lab-cp4i/1.png)\n\n1. Click on _Connect to this cluster_\n\n\t![2](images/schema-registry-lab-cp4i/2.png)\n\nIn this panel, you will find\n\n1. The **Botstrap server** to connect your applications to in order to send and receive messages from your IBM Event Streams instance.\n1. The **API endpoint** which is also the **Schema Registry url** your applications will need to work with Apache Avro data schemas.\n1. A _Generate API key_ button to generate an **API key** and a _Certificates_ section to download either the **Java truststore** or the **PEM certificate** that your applications will need in order to authenticate and authorize against your IBM Event Streams instance.\n\n\t![3](images/schema-registry-lab-cp4i/3.png)\n\n### CLI\n\n1. Log into your cluster with the IBM CloudPak CLI\n\n\t```shell\n\t$ cloudctl login -a https://icp-console.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud -u admin -p ****** -n eventstreams-cp4i --skip-ssl-validation\n\tAuthenticating...\n\tOK\n\n\tTargeted account mycluster Account\n\n\tTargeted namespace eventstreams-cp4i\n\n\tConfiguring kubectl ...\n\tProperty \"clusters.mycluster\" unset.\n\tProperty \"users.mycluster-user\" unset.\n\tProperty \"contexts.mycluster-context\" unset.\n\tCluster \"mycluster\" set.\n\tUser \"mycluster-user\" set.\n\tContext \"mycluster-context\" created.\n\tSwitched to context \"mycluster-context\".\n\tOK\n\n\tConfiguring helm: /Users/user/.helm\n\tOK\n\t```\n\n1. Initialize the Event Streams CLI plugin\n\n\t```shell\n\t$ cloudctl es init\n\n\tIBM Cloud Platform Common Services endpoint:   https://icp-console.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tNamespace:                                     eventstreams-cp4i\n\tHelm release:                                  es-cp4i\n\tIBM Cloud Pak for Integration UI address:      https://ibm-icp4i-prod-integration.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams API endpoint:                    https://eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams API status:                      OK\n\tEvent Streams SSL client auth endpoint:        https://es-cp4i-ibm-es-clientauth-route-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams UI address:                      https://es-cp4i-ibm-es-ui-route-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\tEvent Streams bootstrap address:               es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443\n\tOK\n\t```\n\nWe can see above the Event Streams **bootstrap address** and **API endpoint** (and Schema Registry url) that our applications will need in order to connect to this Event Streams instance\n\nTo be able to authenticate and authorize against your IBM Event Streams instance, you still need the Java truststore, the PEM certificate and an API Key:\n\n1. To download your **Java truststore** certificate, you can use the following command:\n\n\t```shell\n\t$ cloudctl es certificates\n\tCertificate successfully written to /Users/user/Workspace/es-cert.jks.\n\tOK\n\t```\n\n1. To download your **PEM certificate**, you can use the following command:\n\n\t```shell\n\t$ cloudctl es certificates --format pem\n\tCertificate successfully written to /Users/user/Workspace/es-cert.pem.\n\tOK\n\t```\n\n1. To generate your **API key**, you can use the following command:\n\n\t```shell\n\t$ cloudctl es iam-service-id-create --name eventstreams-lab-key --role administrator --all-topics --all-groups --all-txnids --all-schemas\n\tCreated service ID eventstreams-lab-key.\n\tCreated service policy for cluster.\n\tCreated service policy for all topics.\n\tCreated service policy for all consumer groups.\n\tCreated service policy for all Transactional IDs.\n\tCreated service policy for all schemas.\n\n\tPlease preserve the API key! It cannot be retrieved after it's created.\n\n\tService ID UUID   ServiceId-70288651-88df-436b-83f2-2ba7cbedcbeb\n\tAPI key           *****\n\tOK\n\t```\n\n<InlineNotification kind=\"warning\">\n\nWe recommend to carefully set appropriate roles as well as access to topics, groups, transaction IDs and schemas for the API keys that you generate.\n\n</InlineNotification>\n\n### Environment variables\n\nBased on the IBM Event Streams credentials we gathered in the sections above, we are going to set some environment variables that will be used throughout the rest of the lab for easiness.\n\n1. **KAFKA_BROKERS** which should take the value of **bootstrap server**:\n\n\t```shell\n\t$ export KAFKA_BROKERS=es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443\n\t```\n\n1. **KAFKA_APIKEY** which should take the value of the **API key** you have generated:\n\n\t```shell\n\t$ export KAFKA_APIKEY=*****\n\t```\n\n1. **PEM_CERT** which should take the value of the location where the PEM certificate will be located within the docker container we will use for our development environment (explained in later section). For now, just set this to `/tmp/lab/es-cert.pem`:\n\n\t```shell\n\t$ export PEM_CERT=/tmp/lab/es-cert.pem\n\t```\n\n\t(\\*) Don't forget to download both the PEM certificate and the Java truststore to your laptop (either using the GUI or the CLI) but you should have a `es-cert.jks` and `es-cert.pem` file in the same place you cloned this lab's github repository in the [requirements](#requirements) section.\n\n1. **SCHEMA_REGISTRY_URL** which should be a combination of the **API key** and the **API endpoint** (and Schema Registry url) in the form of:\n\n\t`https://token:<API_key>@<API_endpointt>`\n\n\t```shell\n\t$ export SCHEMA_REGISTRY_URL=https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud\n\t```\n\n## Python Demo Environment\n\nGiven that students' workstations may vary quite a lot, not only on their operating system but also on the tools installed on them and the tools we need for our lab might install differently, we have opted to provide a python demo environment in the form of a Docker container where all the libraries and tools needed are already pre-installed.\n\n### Clone\n\n<InlineNotification kind=\"warning\">\n\nYou should have cloned this lab's GitHub repository already on the [requirements](#requirements) section so you should be able to jump to the next build subsection\n\n</InlineNotification>\n\nIn order to build our python demo environment we first need to clone the github repository where the assets live. This github repository is <https://github.com/ibm-cloud-architecture/refarch-eda-tools> and the specific assets we refer to can be found under the `labs/es-cp4i-schema-lab` folder:\n\n1. Clone the github repository on your workstation on the location of your choice:\n\n\t```shell\n\t$ git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\n\tCloning into 'refarch-eda-tools'...\n\tremote: Enumerating objects: 185, done.\n\tremote: Counting objects: 100% (185/185), done.\n\tremote: Compressing objects: 100% (148/148), done.\n\tremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\n\tReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\n\tResolving deltas: 100% (23/23), done.\n\t```\n\n1. Change directory into `refarch-eda-tools/labs/es-cp4i-schema-lab` to find the assets we will we working from now on for the python demo environment and few other scripts/applications:\n\n\t```shell\n\t$ cd refarch-eda-tools/labs/es-cp4i-schema-lab\n\n\t$ ls -all\n\ttotal 240\n\tdrwxr-xr-x   9 user  staff     288 20 May 19:33 .\n\tdrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n\t-rw-r--r--   1 user  staff    1012 20 May 19:33 Dockerfile\n\t-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\n\tdrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\n\tdrwxr-xr-x  27 user  staff     864 20 May 19:33 images\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\n\t-rw-r--r--   1 user  staff     286 20 May 19:33 kafka.properties\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n\t```\n\n### Build\n\nThis Docker container can be built by using the [Dockerfile](Dockerfile) provided within this github repository.\n\nTo build your python demo environment Docker container, execute the following on your workstation:\n\n```shell\n$ docker build -t \"ibmcase/python-schema-registry-lab:latest\" .\n```\n<InlineNotification kind=\"warning\">\n\n**WARNING:**\n\n* Mind the **dot** at the end of the command.\n* Be consistent throughout the lab with the name you give to the Docker container.\n\n</InlineNotification>\n\n### Run\n\nIn order to run the python demo environment Docker container, execute the following on your workstation:\n\n1. Make sure you have declared your `KAFKA_BROKERS`, `KAFKA_APIKEY` and `SCHEMA_REGISTRY_URL` environment variables as explain in the [IBM Event Streams Service Credentials](#ibm-event-streams-service-credentials) section.\n\n1. Run the python demo environment container\n\n\t```shell\n\t$ docker run -e KAFKA_BROKERS=$KAFKA_BROKERS \\\n\t\t\t\t -e KAFKA_APIKEY=$KAFKA_APIKEY \\\n\t\t\t\t -e PEM_CERT=$PEM_CERT \\\n\t\t\t\t -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL \\\n\t\t\t\t -v ${PWD}:/tmp/lab \\\n\t\t\t\t --rm \\\n\t\t\t\t -ti ibmcase/python-schema-registry-lab:latest bash\n\t```\n\n1. Go to `/tmp/lab` to find all the assets you will need to complete this lab.\n\n<InlineNotification kind=\"info\">\n\n**INFO:** we have mounted this working directory into the container so any changes to any of the files apply within the container. This is good as we do not need to restart the python demo environment Docker container if we want to do any changes to the files.\n\n</InlineNotification>\n\n### Exit\n\nOnce you are done with the python demo environment container, just execute `exit` and you will get out of the container and the container will automatically be removed from your system.\n\n## Schema Registry\n\n![diagram](images/schema-registry-lab-cp4i/schema-registry.png)\n\nOne of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro (<https://avro.apache.org/docs/current/>). To learn more about Apache Avro, how to define Apache Avro data schemas and more, we strongly recommend to read through our documentation on Avro and data schemas [here](/technology/avro-schemas/)\n\nIBM Event Streams development team has developed a Schema Registry to work along your Kafka cluster to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. The Schema Registry will also provide the ability for schemas to evolve in time.\n\n### Accessing the Schema Registry\n\nTo access the schema registry, you simply need to click on the Schema Registry button on the main left hand vertical menu bar:\n\n  ![4](images/schema-registry-lab-cp4i/4.png)\n\nYou can also interact with the Schema Registry through the IBM Event Streams CLI:\n\n```bash\n$ cloudctl es --help | grep schema\n   schema                       Display details of a schema.\n   schema-add                   Add a new schema or a new version of a schema to the registry.\n   schema-modify                Modify an entire schema or a specific schema version\n   schema-remove                Remove a schema or a version of a schema from the registry.\n   schema-verify                Verify a schema file.\n   schemas                      List the schemas in the registry.\n   schemas-export               Export the schemas in the registry to a zip file.\n   schemas-import               Import a set of schemas into the registry from a zip file.\n```\n\n## Schemas\n\nIn this section we will finally get our hands dirty with the IBM Event Steams Schema Registry capability by working with Apache Avro schemas and the Schema Registry.\n\n### Create a schema\n\nLet's see how can we create a schema to start playing with.\n\n#### UI\n\nThe IBM EVent Streams user interface allow us to create schemas only from _json_ or Avro schema _avsc_ files.\n\n1. Create an Avro schema file **avsc** with your schema:\n\n\t```bash\n\t$ echo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-ui.avsc\n\t```\n\n1. On the IBM Event Streams Schema Registry User Interface, Click on _Add schema_ button on the top right corner.\n\n1. Click on _Upload definition_ button on the left hand side and select the `demoschema-ui.avsc` file we just created.\n\n1. You should now see you Avro schema loaded in the UI with two tabs, definition and preview to make sure your schema looks as desired:\n\n\t![5](images/schema-registry-lab-cp4i/5.png)\n\n1. Click on _Add schema_ button at the top right corner and you should now see that schema listed among your other schemas.\n\n#### CLI\n\n1. Create another Avro schema **avsc** file with a different schema:\n\n\t```bash\n\t$ echo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_CLI\",\n\t\"namespace\": \"schemas.demo.cli\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-cli.avsc\n\t```\n\n1. Create a schema by executing the following command:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc\n\n\tSchema demoSchema_CLI is active.\n\n\tVersion   Version ID   Schema           State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:30:42 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_CLI to the registry.\n\tOK\n\t```\n\n### List schemas\n\n#### UI\n\nIn order to list the schemas in the UI you just simply need to open up the Schema Registry User Interface and schemas will get listed in there automatically. You also have a search tool bar at the top. You can also see more details about your schema by clicking the drop down arrow on its left:\n\n  ![6](images/schema-registry-lab-cp4i/6.png)\n\n#### CLI\n\n1. Execute the following command to list the schemas in your Schema Registry:\n\n```bash\n$ cloudctl es schemas\n\nSchema                State    Latest version   Latest version ID   Updated\nAircraftAssignment    active   1.0.0            1                   Tue, 09 Jun 2020 18:53:23 UTC\ndefaultValue          active   1.0.0            1                   Sat, 06 Jun 2020 18:14:41 UTC\ndemoSchema_CLI        active   1.0.0            1                   Thu, 25 Jun 2020 11:30:42 UTC\ndemoSchema_UI         active   1.0.0            1                   Thu, 25 Jun 2020 11:25:04 UTC\nFlightTimes           active   1.0.0            1                   Tue, 09 Jun 2020 18:56:15 UTC\npacs008               active   1.0.0            1                   Sun, 07 Jun 2020 14:47:10 UTC\npacs008_cleansed      active   1.0.0            1                   Sun, 07 Jun 2020 14:46:40 UTC\npain013               active   1.0.0            1                   Mon, 08 Jun 2020 12:45:15 UTC\npain013_cleansed      active   1.0.0            1                   Mon, 08 Jun 2020 12:09:39 UTC\npartner_sample_pain   active   1.0.0            1                   Sun, 07 Jun 2020 14:48:07 UTC\npartner_sample2       active   1.0.0            1                   Sun, 07 Jun 2020 14:48:16 UTC\nrtp_summation         active   1.0.0            1                   Sun, 07 Jun 2020 14:48:30 UTC\nSchedules             active   1.0.0            1                   Tue, 09 Jun 2020 18:59:23 UTC\nOK\n```\n\n### Delete schemas\n\n#### UI\n\n1. Click on the schema you want to delete.\n1. Click on the _Manage schema_ tab at the top.\n1. Click on _Remove schema_\n\n\t![7](images/schema-registry-lab-cp4i/7.png)\n\n#### CLI\n\nTo remove a schema using the CLI, simply execute the following command and confirm:\n\n```bash\n$ cloudctl es schema-remove demoSchema_CLI\nRemove schema demoSchema_CLI and all versions? [y/n]> y\nSchema demoSchema_CLI and all versions removed.\nOK\n```\n\n### Create new schema version\n\nTo create a new version of a schema,\n\n1. Let's first create again the previous two schemas:\n\n```bash\n$ cloudctl es schema-add --file demoshema-ui.avsc\n\nSchema demoSchema_UI is active.\n\nVersion   Version ID   Schema          State    Updated                         Comment\n1.0.0     1            demoSchema_UI   active   Thu, 25 Jun 2020 11:47:36 UTC\n\nAdded version 1.0.0 of schema demoSchema_UI to the registry.\nOK\n```\n```bash\n$ cloudctl es schema-add --file demoshema-cli.avsc\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n\nAdded version 1.0.0 of schema demoSchema_CLI to the registry.\nOK\n```\n\n1. Add a new attribute to the schemas by editing their Avro schema avsc files:\n\n```bash\n$ cat demoshema-ui.avsc\n{\n  \"type\":\"record\",\n  \"name\":\"demoSchema_UI\",\n  \"namespace\": \"schemas.demo.ui\",\n  \"fields\":[\n    {\"name\": \"eventKey\",\"type\":\"string\"},\n    {\"name\": \"message\",\"type\":\"string\"},\n    {\"name\": \"attribute1\",\"type\":\"string\"}]\n}\n```\n\n#### UI\n\n1. Click on the schema you want to create a new version for.\n1. Click on the _Add new version_ button on the left hand side.\n1. Click on _Upload definition_ button on the left hand side.\n1. Select the Avro schema avsc file and click ok.\n\n\t![8](images/schema-registry-lab-cp4i/8.png)\n\n<InlineNotification kind=\"error\">\n\n**ERROR:** The error we are seeing on the screen is because the IBM Event Streams Schema Registtry enforces full compatibility: <https://ibm.github.io/event-streams/schemas/creating/#adding-new-schema-versions>\n\n</InlineNotification>\n\n**Full compatibility** for data schemas means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. More on data schema compatibility on the section [Data Evolution](#data-evolution) towards the end of this lab.\n\nAs explained in the error notification above, we need to add a default value for our new attribute in our data schema so that messages serialized with an older version of the data schema which won't contain this new attribute can later be deserialized with the newer version of the data schema that expects such attribute. By providing a default value, we allow deserializers to consume messages that do not contain newer attributes.\n\n1. Add a default value for the new attribute:\n\n\t```bash\n\t$ cat demoshema-ui.avsc\n\t{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"},\n\t\t{\"name\": \"attribute1\",\"type\":\"string\",\"default\": \"whatever\"}]\n\t}\n\t```\n\n1. Repeat the steps for adding a new version of a schema above.\n1. This time you should see that the schema is valid:\n\n\t![9](images/schema-registry-lab-cp4i/9.png)\n\n1. However, it still does not let us add this new version to the data schema until we actually provide a version for it. Click on the _Add +_ link on the right of the version attribute of the schema and give it `2.0.0` for example (hit enter for the version to take the value you type in).\n1. Click on _Add schema_.\n1. You should now see the two versions for your data schema on the left hand side.\n\n\t![10](images/schema-registry-lab-cp4i/10.png)\n\n1. If you go back to the Schema Registry page where all your schemas are listed, you should now see that the latest version for your data schema is `2.0.0` now.\n\n#### CLI\n\n1. If we try to add the new version of the schema from its `demoschema-cli.avsc` Avro schema file, we will get the same error as in the previous UI example:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 400. Avro schema is not compatible with latest schema version: Compatibility type 'MUTUAL_READ' does not hold between 1 schema(s) in the chronology because: Schema[0] has incompatibilities: ['READER_FIELD_MISSING_DEFAULT_VALUE: attribute1' at '/fields/2'].\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI to the registry.\n\t```\n\n1. Add the default value for the new attribute in your Avro schema avsc file and try to add that new version of the schema:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 409. Schema version name already exists\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI to the registry.\n\t```\n\n1. We see that we still have an error because we have not specified a new version value. Specify a new version value when adding this new version of the schema:\n\n\t```bash\n\t$ cloudctl es schema-add --file demoshema-cli.avsc --version 2.0.0\n\n\tSchema demoSchema_CLI is active.\n\n\tVersion   Version ID   Schema           State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n\t2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\n\n\tAdded version 2.0.0 of schema demoSchema_CLI to the registry.\n\tOK\n\t```\n\n### Get latest version of a schema\n\n#### UI\n\nIn order to see the latest version of a data schema using the UI, we just need to go to the Schema Registry web user interface and click on the expand arrow buttton that is on the left:\n\n ![11](images/schema-registry-lab-cp4i/11.png)\n\n#### CLI\n\nIn order to see the latest version of a data schema using the CLI, we simply need to run the following command:\n\n```bash\n$ cloudctl es schema demoSchema_CLI --version 2\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"attribute1\",\n      \"type\": \"string\",\n      \"default\": \"whatever\"\n    }\n  ]\n}\n```\n\n(\\*) The version you specify is actually the version ID (2) rather than the version name we gave to the newer schema version (2.0.0):\n\n```bash\n$ cloudctl es schema demoSchema_CLI\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\nOK\n```\n\n### Get specific version of a schema\n\n#### UI\n\nTo see a specific version of a schema, go to the Schema Registry web user interface and click on the schema you want to see the version for. You will now see how many version of the schema you have and you can click on any of these in order to see more details about it.\n\n  ![12](images/schema-registry-lab-cp4i/12.png)\n\n#### CLI\n\nTo see a specific version of a schema using the CLI, simply run the following command with the version ID you would like to get retrieved:\n\n\n```bash\n$ cloudctl es schema demoSchema_CLI --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n### Listing all versions of a schema\n\n#### UI\n\nTo list all versions of schema in the Schema Registry user interface, you simply need to click on the data schema you want and a new page will display these:\n\n  ![12](images/schema-registry-lab-cp4i/12.png)\n\n#### CLI\n\nIn order to display all versions of a schema, run the following command:\n\n```bash\n$ cloudctl es schema demoSchema_CLI\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n1.0.0     1            demoSchema_CLI   active   Thu, 25 Jun 2020 11:47:43 UTC\n2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\nOK\n```\n\n### Deleting a version of a schema\n\n#### UI\n\nIn order to delete a version of a schema using the Schema Registry user interface,\n\n1. Click on the data schema you want a version of it deleted for.\n1. Select the version you want to delete on the left hand side.\n1. Click on _Manage version_ button that is on the top right corner within the main box in the center of the page.\n1. Click on _Remove version_.\n\n\t![13](images/schema-registry-lab-cp4i/13.png)\n\n#### CLI\n\nIn order to delete a version of a schema through the CLI, execute the following command:\n\n```bash\n$ cloudctl es schema-remove demoSchema_CLI --version 1\n\nRemove version with ID 1 of schema demoSchema_CLI? [y/n]> y\nVersion with ID 1 of schema demoSchema_CLI removed.\nOK\n```\n\nWe can see only version 2 now:\n\n```bash\n$ cloudctl es schema demoSchema_CLI\n\nSchema demoSchema_CLI is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n2.0.0     2            demoSchema_CLI   active   Thu, 25 Jun 2020 14:59:11 UTC\nOK\n\n```\n\n## Python Avro Producer\n\nIn this section we describe the python scripts we will be using in order to be able to produce **avro** messages to a Kafka topic.\n\n### Produce Message\n\nThe python script that we will use to send an avro message to a Kafka topic is [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/src/ProduceAvroMessage.py) where we have the following:\n\n1. A function to parse the arguments:\n\n\t```python\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for this script are: \" , str(sys.argv))\n\t\tif len(sys.argv) == 2:\n\t\t\tTOPIC_NAME = sys.argv[1]\n\t\telse:\n\t\t\tprint(\"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\")\n\t\t\texit(1)\n\t```\n\n1. A function to create the event to be sent:\n\n\t```python\n\tdef createEvent():\n\t\tprint('Creating event...')\n\n\t\tkey = {\"key\": 1}\n\t\tvalue = {\"message\" : \"This is a test message\"}\n\n\t\tprint(\"DONE\")\n\n\t\treturn json.dumps(value), json.dumps(key)\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments\n\t1. Get the Avro schemas for the key and value of the event\n\t1. Create the Event to be sent\n\t1. Print it out for reference\n\t1. Create the Kafka Avro Producer and configure it\n\t1. Send the event\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Get the Kafka topic name\n\t\tparseArguments()\n\t\t# Get the avro schemas for the message's key and value\n\t\tevent_value_schema = getDefaultEventValueSchema(DATA_SCHEMAS)\n\t\tevent_key_schema = getDefaultEventKeySchema(DATA_SCHEMAS)\n\t\t# Create the event\n\t\tevent_value, event_key = createEvent()\n\t\t# Print out the event to be sent\n\t\tprint(\"--- Event to be published: ---\")\n\t\tprint(event_key)\n\t\tprint(event_value)\n\t\tprint(\"----------------------------------------\")\n\t\t# Create the Kafka Avro Producer\n\t\tkafka_producer = KafkaProducer(KAFKA_BROKERS,KAFKA_APIKEY,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the Kafka Avro Producer\n\t\tkafka_producer.prepareProducer(\"ProduceAvroMessagePython\",event_key_schema,event_value_schema)\n\t\t# Publish the event\n\t\tkafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n\t```\n\nAs you can see, this python code depends on a Kafka Avro Producer and an Avro Utils for loading the Avro schemas which are explained next.\n\n### Avro Utils\n\nThis script, called [avroEDAUtils.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/avro_files/utils/avroEDAUtils.py), contains some very simple utility functions to be able to load Avro schemas from their **avsc** files in order to be used by the Kafka Avro Producer.\n\n1. A function to get the key and value Avro schemas for the messages to be sent:\n\n\t```python\n\tdef getDefaultEventValueSchema(schema_files_location):\n\t# Get the default event value data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_value_schema = LoadAvsc(schema_files_location + \"/default_value.avsc\", known_schemas)\n\treturn default_event_value_schema\n\n\tdef getDefaultEventKeySchema(schema_files_location):\n\t# Get the default event key data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_key_schema = LoadAvsc(schema_files_location + \"/default_key.avsc\", known_schemas)\n\treturn default_event_key_schema\n\t```\n\t(\\*) Where `known_schemas` is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this.\n\n1. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary:\n\n\t```python\n\tdef LoadAvsc(file_path, names=None):\n\t# Load avsc file\n\t# file_path: path to schema file\n\t# names(optional): avro.schema.Names object\n\tfile_text = open(file_path).read()\n\tjson_data = json.loads(file_text)\n\tschema = avro.schema.SchemaFromJSONData(json_data, names)\n\treturn schema\n\t```\n\n### Kafka Avro Producer\n\nThis script, called [KcAvroProducer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/kafka/KcAvroProducer.py), will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method:\n\n1. Initialize and prepare the Kafka Producer\n\n\t```python\n\tclass KafkaProducer:\n\n\t\tdef __init__(self,kafka_brokers = \"\",kafka_apikey = \"\",schema_registry_url = \"\"):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.kafka_apikey = kafka_apikey\n\t\t\tself.schema_registry_url = schema_registry_url\n\n\t\tdef prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n\t\t\toptions ={\n\t\t\t\t\t'bootstrap.servers':  self.kafka_brokers,\n\t\t\t\t\t'schema.registry.url': self.schema_registry_url,\n\t\t\t\t\t'group.id': groupID,\n\t\t\t\t\t'security.protocol': 'SASL_SSL',\n\t\t\t\t\t'sasl.mechanisms': 'PLAIN',\n\t\t\t\t\t'sasl.username': 'token',\n\t\t\t\t\t'sasl.password': self.kafka_apikey,\n\t\t\t\t\t'ssl.ca.location': os.environ['PEM_CERT'],\n\t\t\t\t\t'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print out the configuration\n\t\t\tprint(\"--- This is the configuration for the avro producer: ---\")\n\t\t\tprint(options)\n\t\t\tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro Producer\n\t\t\tself.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n\t```\n\n1. Publish method\n\n\t```python\n\tdef publishEvent(self, topicName, value, key):\n\t\t# Produce the Avro message\n\t\t# Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n\t\tself.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n\t\t# Flush\n\t\tself.producer.flush()\n\t```\n\n### Run\n\nWe will see in the following section [Schemas and Messages](#schemas-and-messages) how to send Avro messages according with their schemas to IBM Event Streams.\n\n## Python Avro Consumer\n\nIn this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.\n\n### Consume Message\n\nThe python script that we will use to consume an Avro message from a Kafka topic is [ConsumeAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/src/ConsumeAvroMessage.py) where we have the following:\n\n1. A function to parse arguments:\n\n\t```python\n\t# Parse arguments to get the container ID to poll for\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for the script are: \" , str(sys.argv))\n\t\tif len(sys.argv) != 2:\n\t\t\tprint(\"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\")\n\t\t\texit(1)\n\t\tTOPIC_NAME = sys.argv[1]\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments to get the topic to read from\n\t1. Create the Kafka Consumer and configure it\n\t1. Poll for next avro message\n\t1. Close the Kafka consumer\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Parse arguments\n\t\tparseArguments()\n\t\t# Create the Kafka Avro consumer\n\t\tkafka_consumer = KafkaConsumer(KAFKA_BROKERS,KAFKA_APIKEY,TOPIC_NAME,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the consumer\n\t\tkafka_consumer.prepareConsumer()\n\t\t# Consume next Avro event\n\t\tkafka_consumer.pollNextEvent()\n\t\t# Close the Avro consumer\n\t\tkafka_consumer.close()\n\t```\n\nAs you can see, this python code depends on a Kafka Consumer which is explained next.\n\n### Kafka Avro Consumer\n\nThis script, called [KcAvroConsumer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/kafka/KcAvroConsumer.py), will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method:\n\n1. Initialize and prepare the new Kafka consumer:\n\n\t```python\n\tclass KafkaConsumer:\n\n\t\tdef __init__(self, kafka_brokers = \"\", kafka_apikey = \"\", topic_name = \"\", schema_registry_url = \"\", autocommit = True):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.kafka_apikey = kafka_apikey\n\t\t\tself.topic_name = topic_name\n\t\t\tself.schema_registry_url = schema_registry_url\n\t\t\tself.kafka_auto_commit = autocommit\n\n\t\t# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n\t\tdef prepareConsumer(self, groupID = \"pythonconsumers\"):\n\t\t\toptions ={\n\t\t\t\t\t'bootstrap.servers':  self.kafka_brokers,\n\t\t\t\t\t'group.id': groupID,\n\t\t\t\t\t'auto.offset.reset': 'earliest',\n\t\t\t\t\t'schema.registry.url': self.schema_registry_url,\n\t\t\t\t\t'enable.auto.commit': self.kafka_auto_commit,\n\t\t\t\t\t'security.protocol': 'SASL_SSL',\n\t\t\t\t\t'sasl.mechanisms': 'PLAIN',\n\t\t\t\t\t'sasl.username': 'token',\n\t\t\t\t\t'sasl.password': self.kafka_apikey,\n\t\t\t\t\t'ssl.ca.location': os.environ['PEM_CERT'],\n\t\t\t\t\t'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print the configuration\n\t\t\tprint(\"--- This is the configuration for the Avro consumer: ---\")\n        \tprint(options)\n        \tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro consumer\n\t\t\tself.consumer = AvroConsumer(options)\n\t\t\t# Subscribe to the topic\n\t\t\tself.consumer.subscribe([self.topic_name])\n\t```\n\n1. Poll next event method:\n\n\t```python\n\t# Prints out the message\n\tdef traceResponse(self, msg):\n        print('[Message] - Next message consumed from {} partition: [{}] at offset {} with key {} and value {}'\n                    .format(msg.topic(), msg.partition(), msg.offset(), msg.key(), msg.value() ))\n\n\t# Polls for next event\n\tdef pollNextEvent(self):\n\t\t# Poll for messages\n\t\tmsg = self.consumer.poll(timeout=10.0)\n\t\t# Validate the returned message\n\t\tif msg is None:\n\t\t\tprint(\"[INFO] - No new messages on the topic\")\n\t\telif msg.error():\n\t\t\tif (\"PARTITION_EOF\" in msg.error()):\n\t\t\t\tprint(\"[INFO] - End of partition\")\n\t\t\telse:\n\t\t\t\tprint(\"[ERROR] - Consumer error: {}\".format(msg.error()))\n\t\telse:\n\t\t\t# Print the message\n\t\t\tmsgStr = self.traceResponse(msg)\n\t```\n\n### Run\n\nWe will see in the following section [Schemas and Messages](#schemas-and-messages) how to consume Avro messages.\n\n## Schemas and Messages\n\nIn this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python scripts presented above in the [Python Avro Producer](#python-avro-producer) and [Python Avro Consumer](#python-avro-consumer).\n\nOnce again, we are going to run these scripts in the python demo environment we presented earlier in this lab in [this section](#python-demo-environment). Please, review that section in order to understand how to run the environment in your local workstation.\n\n1. Make sure you have a newly created topic for this exercise that you can create either in the IBM Event Streams UI on the topics section or by using the following IBM Event Streams CLI command:\n\n\t```shell\n\t$ cloudctl es topic-create test-schema --partitions 1 --replication-factor 1\n\n\tCreated topic test-schema\n\tOK\n\t```\n\n1. Start your python environment with:\n\n\t```shell\n\t$ docker run -e KAFKA_BROKERS=$KAFKA_BROKERS \\\n\t\t\t\t -e KAFKA_APIKEY=$KAFKA_APIKEY \\\n\t\t\t\t -e PEM_CERT=$PEM_CERT \\\n\t\t\t\t -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL \\\n\t\t\t\t -v ${PWD}:/tmp/lab \\\n\t\t\t\t --rm \\\n\t\t\t\t -ti ibmcase/python-schema-registry-lab:latest bash\n\t```\n\n### Create a message\n\nIn order to create a message, we execute the `ProduceAvroMessage.py` within the `/tmp/lab/src` folder in our python demo environment. This script, as you could see in the [Python Avro Producer](#python-avro-producer) section, it is sending the event with key `{'key': '1'}` and value `{'message': 'This is a test message'}` according to the schemas defined in [default_key.avsc](avro_files/default_key.avsc) and [default_value.avsc](avro_files/default_value.avsc) for the key and value of the event respectively.\n\n```shell\npython ProduceAvroMessage.py test-schema\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema [0]\n```\n\nWe can see our new message delivered in the `test-schema` topic by\n\n1. Go into the topics page in the IBM Event Streams UI\n\n\t![14](images/schema-registry-lab-cp4i/14.png)\n\n1. Click on the topic and then on the _Messages_ tab at the top. Finally, click on a message to see it displayed in a hovering card on the right hand side of the screen\n\n\t![15](images/schema-registry-lab-cp4i/15.png)\n\n<InlineNotification kind=\"info\">\n\n**INFO:** Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas.\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\n**WARNING:** Most of the Avro producer clients, whether it is in Java, Python or many other languages, give users the ability to **auto-register** a schema automatically with the specified schema registry in its configuration.\n\n</InlineNotification>\n\nIf we look know at the schemas our schema registry has:\n\n```shell\n$ cloudctl es schemas\n\nSchema                     State    Latest version   Latest version ID   Updated\ndemoSchema_CLI             active   2.0.0            2                   Thu, 25 Jun 2020 14:59:11 UTC\ndemoSchema_UI              active   2.0.0            2                   Thu, 25 Jun 2020 14:21:09 UTC\ntest-schema-key-7jffxi     active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\ntest-schema-value-a7paxm   active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\nOK\n```\n\nwe see two schemas, `test-schema-key-7jffxi` and `test-schema-value-a7paxm`, which in fact correspond to the Avro data schema used for the `key` ([default_key.avsc](avro_files/default_key.avsc)) and the `value` ([default_value.avsc](avro_files/default_value.avsc)) of events sent to the `test-schema` topic in the [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/src/ProduceAvroMessage.py) as explained before sending the message.\n\nTo make sure of what we are saying, we can inspect those schemas:\n\n```shell\n$ cloudctl es schema test-schema-key-7jffxi --version 1\n{\n  \"type\": \"record\",\n  \"name\": \"defaultKey\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"int\",\n      \"name\": \"key\",\n      \"doc\": \"We expect any int as the event key\"\n    }\n  ],\n  \"doc\": \"Default Message's key Avro data schema\"\n}\n```\n\n```shell\n$ cloudctl es schema test-schema-value-a7paxm --version 1\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nIf I now decided that my events should contain another attribute, I would modify the event value schema ([default_value.avsc](avro_files/default_value.avsc)) to reflect that as well as `ProduceAvroMessage.py` to send that new attribute in the event it sends:\n\n```shell\npython ProduceAvroMessage.py test-schema\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\", \"anotherAttribute\": \"Just another test string\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema [0]\n```\n\nI can see that an event with a new attribute has been sent:\n\n ![16](images/schema-registry-lab-cp4i/16.png)\n\nAnd I can also see that the new shcema has got registered as well:\n\n```shell\n$ cloudctl es schemas\nSchema                     State    Latest version   Latest version ID   Updated\ndemoSchema_CLI             active   2.0.0            2                   Thu, 25 Jun 2020 14:59:11 UTC\ndemoSchema_UI              active   2.0.0            2                   Thu, 25 Jun 2020 14:21:09 UTC\ntest-schema-key-7jffxi     active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\ntest-schema-value-a7paxm   active   1                1                   Fri, 26 Jun 2020 14:25:07 UTC\ntest-schema-value-nc2q7    active   1                1                   Mon, 29 Jun 2020 16:38:16 UTC\nOK\n```\n\nIf I inspect that new schema, I see my new attribute in it:\n\n```shell\n$ cloudctl es schema test-schema-value-nc2q7 --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\n<InlineNotification kind=\"info\">\n\nThe schema evolution above (test-schema-value-nc2q7) should have got registered as a new version of the already existing schema (test-schema-value-a7paxm). IBM Event Streams allows schemas to auto-register themselves when when these are sent along with a message from a producer application. However, the Schema Registry does not pick \"new\" schemas up as a new version of a previous schema and simply creates a new schema. Anyway, when reading messages off the topic, Schema Registry handles well what schema to return back to the receiver application so messages can get properly deserialized. Will see that in the next section.\n\n</InlineNotification>\n\n<InlineNotification kind=\"error\">\n\n**SECURITY:** As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is **NOT** a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the [Security](#security) section how we can control schema registration and evolution based on roles at the schema level also.\n\n</InlineNotification>\n\n### Create a non-compliant message\n\n\nLet's see what happens if we send a message that does not comply with its Avro data schema. Let's say that I send the following message:\n\n```shell\nkey = {\"key\": 1}\nvalue = {\"message\" : 12345}\n```\n\nand this is the output of that attempt:\n\n```shell\npython ProduceAvroMessage.py test-schema\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": 12345}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\nTraceback (most recent call last):\n  File \"ProduceAvroMessage.py\", line 74, in <module>\n    kafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n  File \"/tmp/lab/kafka/KcAvroProducer.py\", line 42, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 99, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 118, in encode_record_with_schema\n    return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 152, in encode_record_with_schema_id\n    writer(record, outf)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 86, in <lambda>\n    return lambda record, fp: writer.write(record, avro.io.BinaryEncoder(fp))\n  File \"/root/.local/lib/python3.7/site-packages/avro/io.py\", line 771, in write\n    raise AvroTypeException(self.writer_schema, datum)\navro.io.AvroTypeException: The datum {'message': 12345} is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nAs we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply (the message value attribute we are sending is an integer rather than a string).\n\nTherefore, using Avro schemas with IBM Event Streams give us the ability to build our system with **robustness** protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.\n\n### Consume a message\n\nIn order to consume a message, we execute the `ConsumeAvroMessage.py` within the `/tmp/lab/src` folder in our python demo environment:\n\n```shell\npython ConsumeAvroMessage.py test-schema\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\npython ConsumeAvroMessage.py test-schema\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-cp4i-ibm-es-proxy-route-bootstrap-eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://token:*****@eventstreams-cp4i.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****', 'ssl.ca.location': '/tmp/lab/es-cert.pem', 'schema.registry.ssl.ca.location': '/tmp/lab/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\nAs you can see, our script was able to read the Avro messages from the `test-schema` topic and map that back to their original structure thanks to the Avro schemas:\n\n```shell\n[Message] - Next message consumed from test-schema partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n[Message] - Next message consumed from test-schema partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\n## Data Evolution\n\nSo far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich CLI IBM Event Streams provides to interact with.\n\nHowever, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying [Event Storming](../../methodology/event-storming/) or [Domain Driven Design](../../methodology/domain-driven-design/) for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.\n\nBut it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to [hundreds of years](https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/)) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the **compatibility** of old and new data schemas and, in fact, old and new data at the end of the day.\n\n**The IBM Event Streams Schema Registry enforces full compatibility when creating a new version of a schema**. Full compatibility means that **old data can be read with the new data schema, and new data can also be read with the last data schema**.\n\nIn data formats like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. Let's see then how this affects when you want your data to evolve in a way that it needs to add a new attribute or delete an existing attribute.\n\nBut let's see what that means in terms of adding and removing attributes from your data schema.\n\n### Adding a new attribute\n\nAlthough we have already seen this in the adding a new version of a schema section, let's try to add a new version of our `test-schema-value` schema where we have a new attribute. Remember, our `default_schema.avsc` already contains a new attribute than the original one but that it got registered as a new schema rather than as a new version of the original one. Let's reuse that Avro schema file to register it as a new version.\n\nWhen doing so from the UI, we see the following error:\n\n  ![17](images/schema-registry-lab-cp4i/17.png)\n\nThe reason, as alread explained in the add a new version of a schema section, is because full compatibility dictates that you can only add new attributes to a schema if these have a default value. Reason being that a receiver should be able to deserialize messages produced with an older schema using the newer schema. Because old messages were written with an older schema that did not contain our new attribute, those messages won't have that attribute so we need to provide a default value for it in our never version of the schema so that the receiver is able to deserialize those older messages with the newer schema.\n\nIf we add the default value for the new attribute, we see that our newer version is now compatible:\n\n  ![18](images/schema-registry-lab-cp4i/18.png)\n\nand that it gets registered fine:\n\n  ![19](images/schema-registry-lab-cp4i/19.png)\n\n### Removing an existing attribute\n\nWhat if we now wanted to remove the original `message` attribute from our schema. Let's remove it from the `default_value.avsc` file and try to register that new version:\n\n  ![20](images/schema-registry-lab-cp4i/20.png)\n\nWe, again, get the same error. And the reason is because receivers must be able to read and deserialize messages produced with the newer schema (that is, without the `message` attribute) but with the older schema (that is, with the schema version that enforces the existence of the `message` attribute).\n\nIn order to work this around, what we need to do is to register first an intermediate schema that defines a default value for the `message` attribute:\n\n  ![21](images/schema-registry-lab-cp4i/21.png)\n\nOnce we have a default value for the `message` attribute, we can register a new version of the schema that finally removes that attribute:\n\n  ![22](images/schema-registry-lab-cp4i/22.png)\n\n## Security\n\nAs we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams), etc since we dont want everyone and everything to be, for instance, creating or deleting topics, schemas, etc.\n\nYou can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in policy definitions:\n\n- Cluster (cluster): you can control which users and applications can connect to the cluster.\n- Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.\n- Consumer groups (group): you can control an application’s ability to join a consumer group.\n- Transactional IDs (txnid): you can control the ability to use the transaction capability in Kafka.\n\nIn the context of the Schema Registry, this is something you need to bear in mind when creating the API key that your applications will use to produce messages since they could pontetially be creating new topics, modifying data schemas, etc.\n\nYou can find more information about how to secure your IBM Event Streams resources in the official documentation at: <https://ibm.github.io/event-streams/2019.4/security/managing-access/>\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/schema-registry-on-ocp/schema-registry-cp4i.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}