{"componentChunkName":"component---src-pages-use-cases-schema-registry-on-ocp-index-mdx","path":"/use-cases/schema-registry-on-ocp/","result":{"pageContext":{"frontmatter":{"title":"IBM Event Streams Schema Registry from IBM CloudPak for Integration","description":"Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature"},"relativePagePath":"/use-cases/schema-registry-on-ocp/index.mdx","titleType":"append","MdxNode":{"id":"7caff923-91dd-5fc5-905b-e335cdb3aa12","children":[],"parent":"7de89959-dd5d-5ec7-913b-32c79a3e3203","internal":{"content":"---\ntitle: IBM Event Streams Schema Registry from IBM CloudPak for Integration\ndescription: Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature\n---\n\n<!-- Originally available via \"/technology/event-streams/schema-registry-cp4i-v10/\" -->\n\nThis documentation aims to be a introductory hands-on lab on the IBM Event Streams Schema Registry installed throught the **IBM Cloud Pak for Integration V2020.2.X+** on an Openshift cluster.\n\n## Index\n\n<AnchorLinks>\n  <AnchorLink>Requirements</AnchorLink>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Schema Registry</AnchorLink>\n  <AnchorLink>Schemas</AnchorLink>\n  <AnchorLink>IBM Event Streams Credentials</AnchorLink>\n  <AnchorLink>Python Application</AnchorLink>\n  <AnchorLink>Python Avro Producer</AnchorLink>\n  <AnchorLink>Python Avro Consumer</AnchorLink>\n  <AnchorLink>Schemas and Messages</AnchorLink>\n  <AnchorLink>Data Evolution</AnchorLink>\n  <AnchorLink>Security</AnchorLink>\n</AnchorLinks>\n\n## Requirements\n\nThis lab requires the following components to work against:\n\n1. An IBM Event Streams V10 instance installed through the IBM CloudPak for Integration V2020.2.X or greater.\n2. An IBM Cloud Shell - <https://www.ibm.com/cloud/cloud-shell>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Schema Registry\n\n![diagram](./images/schema-registry.png)\n\nOne of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro (<https://avro.apache.org/docs/current/>). To learn more about Apache Avro, how to define Apache Avro data schemas and more, we strongly recommend to read through our documentation on Avro and data schemas [here](/technology/avro-schemas/)\n\nIBM Event Streams development team has developed a Schema Registry to work along your Kafka cluster to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. The Schema Registry will also provide the ability for schemas to evolve in time.\n\n### Accessing the Schema Registry\n\n#### UI\n\nTo access the schema registry, we first need to log into IBM Event Streams.\n\n1. Point your browser to your IBM Event Streams instace's user interface url and introduce your credentials\n\n\t![login](./images/login-v10.png)\n\n1. Once you are logged into your IBM Event Streams instance, you simply need to click on the Schema Registry button on the main left hand vertical menu bar:\n\n  ![4](./images/4-v10.png)\n\n\n#### CLI\n\nWe can also interact with the Schema Registry through the IBM Event Streams CLI. In order to do so, we first need to log in with the IBM Cloud Pak CLI:\n\n1. Log into your cluster with the IBM CloudPak CLI\n\n\t<InlineNotification kind=\"warning\">\n\n\tMake sure to use the appropriate credentials and select the namespace where your IBM Event Streams instance is installed\n\n\t</InlineNotification>\n\n\t```shell\n\tcloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n\n\tUsername> admin\n\n\tPassword>\n\tAuthenticating...\n\tOK\n\n\tTargeted account mycluster Account\n\n\tEnter a namespace > integration\n\tTargeted namespace integration\n\n\tConfiguring kubectl ...\n\tProperty \"clusters.mycluster\" unset.\n\tProperty \"users.mycluster-user\" unset.\n\tProperty \"contexts.mycluster-context\" unset.\n\tCluster \"mycluster\" set.\n\tUser \"mycluster-user\" set.\n\tContext \"mycluster-context\" created.\n\tSwitched to context \"mycluster-context\".\n\tOK\n\n\tConfiguring helm: /Users/user/.helm\n\tOK\n\t```\n\n1. Initialize the Event Streams CLI plugin\n\n\t```shell\n\tcloudctl es init\n\n\tIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-solutions.gse-ocp.net\n\tNamespace:                                     integration\n\tName:                                          es-1\n\tIBM Cloud Pak for Integration UI address:      No instance of Cloud Pak for Integration has been found. Please check that you have access to it.\n\tEvent Streams API endpoint:                    https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams API status:                      OK\n\tEvent Streams UI address:                      https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams Schema Registry endpoint:        https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams bootstrap address:               es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n\tOK\n\t```\n\n\t(\\*)The above information will later be used in the [IBM Event Streams Credentials](#ibm-event-streams-credentials) section as these are neeeded by the Python application we will work with.\n\n1. Make sure you can access the IBM Event Streams Schema Registry:\n\n\t```bash\n\tcloudctl es schemas\n\n\tNo schemas were found.\n\tOK\n\t```\n\n## Schemas\n\nIn this section we will finally get our hands dirty with the IBM Event Steams Schema Registry capability by working with Apache Avro schemas and the Schema Registry.\n\n<InlineNotification kind=\"info\">\n\nWe recommend to complete most of the UI steps from your **local workstation** since these will require you to upload the files your create/modify to IBM Event Streams and that requires having your files available locally on your workstation rather than on the IBM Cloud Shell\n\n</InlineNotification>\n\n### Create a schema\n\nLet's see how can we create a schema to start playing with.\n\n#### UI\n\nThe IBM EVent Streams user interface allow us to create schemas only from _json_ or Avro schema _avsc_ files.\n\n1. Create an Avro schema file **avsc** with your schema:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\techo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI_USER1\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-ui.avsc\n\t```\n\n1. On the IBM Event Streams Schema Registry User Interface, Click on _Add schema_ button on the top right corner.\n\n1. Click on _Upload definition_ button on the left hand side and select the `demoschema-ui.avsc` file we just created.\n\n1. You should now see you Avro schema loaded in the UI with two tabs, definition and preview to make sure your schema looks as desired:\n\n\t![5](./images/5-v10.png)\n\n1. Click on _Add schema_ button at the top right corner and you should now see that schema listed among your other schemas.\n\n#### CLI\n\n1. Create another Avro schema **avsc** file with a different schema:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\techo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_CLI_USER1\",\n\t\"namespace\": \"schemas.demo.cli\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-cli.avsc\n\t```\n\n1. Create a schema by executing the following command:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tSchema demoSchema_CLI_USER1 is active.\n\n\tVersion   Version ID   Schema           State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI_USER1   active   Thu, 25 Jun 2020 11:30:42 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\tOK\n\t```\n\n### List schemas\n\n#### UI\n\nIn order to list the schemas in the UI you just simply need to open up the Schema Registry User Interface and schemas will get listed in there automatically. You also have a search tool bar at the top. You can also see more details about your schema by clicking the drop down arrow on its left:\n\n  ![6](./images/6-v10.png)\n\n#### CLI\n\n1. Execute the following command to list the schemas in your Schema Registry:\n\n\t```bash\n\tcloudctl es schemas\n\n\tSchema                 State    Latest version   Latest version ID   Updated\n\tdemoSchema_CLI_USER1   active   1.0.0            1                   Fri, 24 Jul 2020 13:55:49 UTC\n\tdemoSchema_UI_USER1    active   1.0.0            1                   Fri, 24 Jul 2020 13:55:51 UTC\n\tOK\n\n\t```\n\n### Delete schemas\n\n#### UI\n\n1. Click on the schema you want to delete.\n1. Click on the _Manage schema_ tab at the top.\n1. Click on _Remove schema_\n\n\t![7](./images/7-v10.png)\n\n#### CLI\n\nTo remove a schema using the CLI, simply execute the following command and confirm:\n\n<InlineNotification kind=\"warning\">\n\nChange USER1\n\n</InlineNotification>\n\n```bash\ncloudctl es schema-remove demoSchema_CLI_USER1\n\nRemove schema demoSchema_CLI_USER1 and all versions? [y/n]> y\nSchema demoSchema_CLI_USER1 and all versions removed.\nOK\n```\n\n### Create new schema version\n\nTo create a new version of a schema,\n\n1. Let's first create again the previous two schemas:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-ui.avsc\n\n\tSchema demoSchema_UI_USER1 is active.\n\n\tVersion   Version ID   Schema                State    Updated                         Comment\n\t1.0.0     1            demoSchema_UI_USER1   active   Fri, 24 Jul 2020 13:59:55 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_UI_USER1 to the registry.\n\tOK\n\t```\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tSchema demoSchema_CLI_USER1 is active.\n\n\tVersion   Version ID   Schema                 State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\tOK\n\t```\n\n1. Add a new attribute to the schemas by editing their Avro schema avsc files:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\tcat demoshema-ui.avsc\n\n\t{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI_USER1\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"},\n\t\t{\"name\": \"attribute1\",\"type\":\"string\"}]\n\t}\n\t```\n\n#### UI\n\n1. Click on the schema you want to create a new version for.\n1. Click on the _Add new version_ button on the left hand side.\n1. Click on _Upload definition_ button on the left hand side.\n1. Select the Avro schema avsc file and click ok.\n\n\t![8](./images/8-v10.png)\n\n<InlineNotification kind=\"error\">\n\n**ERROR:** The error we are seeing on the screen is because the IBM Event Streams Schema Registtry enforces full compatibility: <https://ibm.github.io/event-streams/schemas/creating/#adding-new-schema-versions>\n\n</InlineNotification>\n\n**Full compatibility** for data schemas means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. More on data schema compatibility on the section [Data Evolution](#data-evolution) towards the end of this lab.\n\nAs explained in the error notification above, we need to add a default value for our new attribute in our data schema so that messages serialized with an older version of the data schema which won't contain this new attribute can later be deserialized with the newer version of the data schema that expects such attribute. By providing a default value, we allow deserializers to consume messages that do not contain newer attributes.\n\n1. Add a default value for the new attribute:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\tcat demoshema-ui.avsc\n\n\t{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI_USER1\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"},\n\t\t{\"name\": \"attribute1\",\"type\":\"string\",\"default\": \"whatever\"}]\n\t}\n\t```\n\n1. Repeat the steps for adding a new version of a schema above.\n1. This time you should see that the schema is valid:\n\n\t![9](./images/9-v10.png)\n\n1. However, it still does not let us add this new version to the data schema until we actually provide a version for it. Click on the _Add +_ link on the right of the version attribute of the schema and give it `2.0.0` for example (hit enter for the version to take the value you type in).\n1. Click on _Add schema_.\n1. You should now see the two versions for your data schema on the left hand side.\n\n\t![10](./images/10-v10.png)\n\n1. If you go back to the Schema Registry page where all your schemas are listed, you should now see that the latest version for your data schema is `2.0.0` now.\n\n#### CLI\n\n1. If we try to add the new version of the schema from its `demoschema-cli.avsc` Avro schema file, we will get the same error as in the previous UI example:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 400. Avro schema is not compatible with latest schema version: Compatibility type 'MUTUAL_READ' does not hold between 1 schema(s) in the chronology because: Schema[0] has incompatibilities: ['READER_FIELD_MISSING_DEFAULT_VALUE: attribute1' at '/fields/2'].\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\t```\n\n1. Add the default value for the new attribute in your Avro schema avsc file and try to add that new version of the schema:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 409. Schema version name already exists\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\t```\n\n1. We see that we still have an error because we have not specified a new version value. Specify a new version value when adding this new version of the schema:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc --version 2.0.0\n\n\tSchema demoSchema_CLI_USER1 is active.\n\n\tVersion   Version ID   Schema                 State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n\t2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\n\n\tAdded version 2.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\tOK\n\t```\n\n### Get latest version of a schema\n\n#### UI\n\nIn order to see the latest version of a data schema using the UI, we just need to go to the Schema Registry web user interface and click on the expand arrow buttton that is on the left:\n\n ![11](./images/11.png)\n\n#### CLI\n\nIn order to see the latest version of a data schema using the CLI, we simply need to run the following command:\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1 --version 2\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI_USER1\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"attribute1\",\n      \"type\": \"string\",\n      \"default\": \"whatever\"\n    }\n  ]\n}\n```\n\n(\\*) The version you specify is actually the version ID (2) rather than the version name we gave to the newer schema version (2.0.0):\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n```\n\n### Get specific version of a schema\n\n#### UI\n\nTo see a specific version of a schema, go to the Schema Registry web user interface and click on the schema you want to see the version for. You will now see how many version of the schema you have and you can click on any of these in order to see more details about it.\n\n  ![12](./images/12-v10.png)\n\n#### CLI\n\nTo see a specific version of a schema using the CLI, simply run the following command with the version ID you would like to get retrieved:\n\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1 --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI_USER1\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n### Listing all versions of a schema\n\n#### UI\n\nTo list all versions of schema in the Schema Registry user interface, you simply need to click on the data schema you want and a new page will display these:\n\n  ![12](./images/12-v10.png)\n\n#### CLI\n\nIn order to display all versions of a schema, run the following command:\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n```\n\n### Deleting a version of a schema\n\n#### UI\n\nIn order to delete a version of a schema using the Schema Registry user interface,\n\n1. Click on the data schema you want a version of it deleted for.\n1. Select the version you want to delete on the left hand side.\n1. Click on _Manage version_ button that is on the top right corner within the main box in the center of the page.\n1. Click on _Remove version_.\n\n\t![13](./images/13-v10.png)\n\n#### CLI\n\nIn order to delete a version of a schema through the CLI, execute the following command:\n\n```bash\ncloudctl es schema-remove demoSchema_CLI_USER1 --version 1\n\nRemove version with ID 1 of schema demoSchema_CLI_USER1? [y/n]> y\nVersion with ID 1 of schema demoSchema_CLI_USER1 removed.\nOK\n```\n\nWe can see only version 2 now:\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n```\n\n## IBM Event Streams Credentials\n\nWe have seen how to interact with the IBM Event Streams Schema Registry in order to create, delete, update, etc schemas that our applications will theoretically used for data correctness and application robusteness. However, the first thing that we need to set up in our IBM Event Streams instance are these applications' **service credentials** to be able to interact with IBM Event Streams and its Schema Registry. For doing so, we can either use either the GUI or the CLI.\n\n### GUI\n\n1. Go to you IBM Event Streams instance console\n\n\t![1](./images/1.png)\n\n1. Click on **Connect to this cluster**\n\n\t![2](./images/2.png)\n\nIn this panel, you will find\n\n1. The **Botstrap server** to connect your applications to in order to send and receive messages from your IBM Event Streams instance. We can see we have one **external listener** (whith SCRAM-SHA authentication) and one internal listener (Depending your IBM Event Streams installation you might have different listeners and authentications for these).\n1. The **Schema Registry url** your applications will need to work with Apache Avro data schemas.\n1. A _Generate SCRAM credentials_ button to generate the **SCRAM credentials** your applications will need to authenticate with.\n1. A _Certificates_ section to download either the **Java PKCS12** or the **PEM** certificates (or both) that your applications will need in order to be able to establish the communitaction with your IBM Event Streams instance.\n\n\t![3](./images/3-v10.png)\n\nTo generate the SCRAM credentials needed by your application to get authenticated against IBM Event Streams to be able to produce and consume messages as well as to create, delete, etc topics and schemas, we need to create a **KafkaUser** (this happens behind the scenes) which we will set some permissions and get the corresponding SCRAM usernname and password for to be used in our applications kafka clients configuration:\n\n1. Click on _Generate SCRAM credentials_\n1. Enter a user name for your credentials and click next (leave the last option selected: _Produce messages, consume messages and create topics and schemas_ so that we give full access to our user for simplicity)\n\n\t![scram1](./images/scram-1-v10.png)\n\n1. Select all topics and click next\n\n\t![scram2](./images/scram-2-v10.png)\n\n1. Select all consumer groups and click next\n\n\t![scram3](./images/scram-3-v10.png)\n\n1. Select all transactional IDs and click next\n\n\t![scram4](./images/scram-4-v10.png)\n\nOnce you have created your new KafkaUser, you get the SCRAM credentials displayed on the screen:\n\n![scram5](./images/scram-5-v10.png)\n\n<InlineNotification kind=\"info\">\n\nYou can download the PEM certificate from the UI and then use the IBM Cloud Shell upload file option on the top bar or you can download it from within your IBM Cloud Shell by using the CLI (see below)\n\n</InlineNotification>\n\n### CLI\n\n<InlineNotification kind=\"info\">\n\nThe following two steps should have been completed already in the previous [Accessing the Schema Registry](#accessing-the-schema-registry) section\n\n</InlineNotification>\n\n1. Log into your cluster with the IBM CloudPak CLI\n\n\t```shell\n\tcloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n\n\tUsername> admin\n\n\tPassword>\n\tAuthenticating...\n\tOK\n\n\tTargeted account mycluster Account\n\n\tEnter a namespace > integration\n\tTargeted namespace integration\n\n\tConfiguring kubectl ...\n\tProperty \"clusters.mycluster\" unset.\n\tProperty \"users.mycluster-user\" unset.\n\tProperty \"contexts.mycluster-context\" unset.\n\tCluster \"mycluster\" set.\n\tUser \"mycluster-user\" set.\n\tContext \"mycluster-context\" created.\n\tSwitched to context \"mycluster-context\".\n\tOK\n\n\tConfiguring helm: /Users/jesusalmaraz/.helm\n\tOK\n\t```\n\n1. Initialize the Event Streams CLI plugin\n\n\t```shell\n\tcloudctl es init\n\n\tIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-solutions.gse-ocp.net\n\tNamespace:                                     integration\n\tName:                                          es-1\n\tIBM Cloud Pak for Integration UI address:      No instance of Cloud Pak for Integration has been found. Please check that you have access to it.\n\tEvent Streams API endpoint:                    https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams API status:                      OK\n\tEvent Streams UI address:                      https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams Schema Registry endpoint:        https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams bootstrap address:               es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n\tOK\n\t```\n\nWe can see above the Event Streams **bootstrap address** and **Schema Registry url** that our applications will need in order to connect to this Event Streams instance\n\nTo be able to establish communication and authenticate against your IBM Event Streams instance, you will need the PEM certificate and an the SCRAM credentials:\n\n1. To download your **PEM certificate**, you can use the following command:\n\n\t```shell\n\tcloudctl es certificates --format pem\n\n\tCertificate successfully written to /home/ALMARAZJ/es-cert.pem.\n\tOK\n\t```\n\n1. To generate your SCRAM credentials, we first need to create a **KafkaUser**, you can use the following command:\n\n\t```shell\n\tcloudctl es kafka-user-create --name my-user1 --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n\n\tKafkaUser name   Authentication   Authorization   Username                                                Secret\n\tmy-user1         scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret\n\n\tResource type     Name        Pattern type   Host   Operation\n\ttopic             *           literal        *      Read\n\ttopic             __schema_   prefix         *      Read\n\ttopic             *           literal        *      Write\n\ttopic             *           literal        *      Create\n\ttopic             __schema_   prefix         *      Alter\n\tgroup             *           literal        *      Read\n\ttransactionalId   *           literal        *      Write\n\n\tCreated KafkaUser my-user1.\n\tOK\n\t```\n\n<InlineNotification kind=\"warning\">\n\nWe recommend to carefully set appropriate roles as well as access to topics, groups, transaction IDs and schemas for the API keys that you generate.\n\n</InlineNotification>\n\nWhen a KafkaUser custom resource is created, the Entity Operator within Event Streams will create the principal in ZooKeeper with appropriate ACL entries. It will also create a Kubernetes Secret that contains the Base64-encoded SCRAM password for the scram-sha-512 authentication type, or the Base64-encoded certificates and keys for the tls authentication type.\n\n1. Retrieve the username and the secret name containing the password of your SCRAM credentials for your KafkaUser:\n\n\t```shell\n\toc get kafkauser my-user1 --namespace integration -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'\n\n\tusername: my-user1\n\tsecret-name: my-user1\n\t```\n1. Retrieve the password of your SCRAM credentials from the secret above:\n\n\t```shell\n\toc get secret my-user1 --namespace integration -o jsonpath='{.data.password}' | base64 --decode\n\n\t*****\n\t```\n\n### Environment variables\n\nNow that we have generated the appropriate IBM Event Streams credentials for applications to be able to establish communication and authenticate against our IBM Event Streams instance, we are going to set some environment variables that will be used by our Python application:\n\n1. **KAFKA_BROKERS** which should take the value of **bootstrap server**:\n\n\t```shell\n\texport KAFKA_BROKERS=es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n\t```\n\n1. **SCRAM_USERNAME** which should take the value of the **SCRAM username** you have generated:\n\n\t```shell\n\texport SCRAM_USERNAME=my-user1\n\t```\n\n1. **SCRAM_PASSWORD** which should take the value of the **SCRAM password** you have generated:\n\n\t```shell\n\texport SCRAM_PASSWORD=*****\n\t```\n\n1. **PEM_CERT** which should take the value of the location where the PEM certificate is within your IBM Cloud Shell:\n\n\t<InlineNotification kind=\"warning\">\n\n\tSet the path appropriately to your IBM Cloud Shell\n\n\t</InlineNotification>\n\n\t```shell\n\texport PEM_CERT=~/es-cert.pem\n\t```\n\n\t(\\*) Don't forget to download both the PEM certificate to your IBM Cloud Shell through the CLI or upload it to your IBM Cloud Shell from your laptop if you used the UI to get the certificate. Review previous section if needed.\n\n1. **SCHEMA_REGISTRY_URL** which should be a combination of the **SCRAM username**, the **SCRAM password** and the **Schema Registry url** in the form of:\n\n\t`https://<SCRAM_username>:<SCRAM_password>@<Schema_Registry_url>`\n\n\t```shell\n\texport SCHEMA_REGISTRY_URL=https://${SCRAM_USERNAME}:${SCRAM_PASSWORD}@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\n\t```\n\n## Python Application\n\nThe Python application we have built to see how to produce and consume messages (either plain messages or Avro encoded messages based on Avro Data Schemas) to and from an IBM Event Streams instance installed through the IBM Cloud Pak for Integration is public at the following GitHub repo: <https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cp4i-schema-lab-v10>\n\n### Clone\n\nIn order to use and work with this Python application, the first thing we need to do is to clone the GitHub repository where it is published.\n\n1. Clone the github repository on your workstation on the location of your choice:\n\n\t```shell\n\tgit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\n\n\tCloning into 'refarch-eda-tools'...\n\tremote: Enumerating objects: 185, done.\n\tremote: Counting objects: 100% (185/185), done.\n\tremote: Compressing objects: 100% (148/148), done.\n\tremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\n\tReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\n\tResolving deltas: 100% (23/23), done.\n\t```\n\n1. Change directory into `refarch-eda-tools/labs/es-cp4i-schema-lab-v10` to find the assets we will we working from now on for the python demo environment and few other scripts/applications:\n\n\t```shell\n\tcd refarch-eda-tools/labs/es-cp4i-schema-lab-v10\n\n\t$ ls -all\n\ttotal 240\n\tdrwxr-xr-x   9 user  staff     288 20 May 19:33 .\n\tdrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n\t-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\n\tdrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n\t```\n\nIn the next sections, we are going to briefly explain the implementation of this Python application so that you understand what is being done behind the scenes and more importantly, if you are a developer, how to do so.\n\n## Python Avro Producer\n\nIn this section we describe the Python scripts we will be using in order to be able to produce **avro** messages to a Kafka topic.\n\n### Produce Message\n\nThe python script that we will use to send an avro message to a Kafka topic is [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/src/ProduceAvroMessage.py) where we have the following:\n\n1. A function to parse the arguments:\n\n\t```python\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for this script are: \" , str(sys.argv))\n\t\tif len(sys.argv) == 2:\n\t\t\tTOPIC_NAME = sys.argv[1]\n\t\telse:\n\t\t\tprint(\"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\")\n\t\t\texit(1)\n\t```\n\n1. A function to create the event to be sent:\n\n\t```python\n\tdef createEvent():\n\t\tprint('Creating event...')\n\n\t\tkey = {\"key\": 1}\n\t\tvalue = {\"message\" : \"This is a test message\"}\n\n\t\tprint(\"DONE\")\n\n\t\treturn json.dumps(value), json.dumps(key)\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments\n\t1. Get the Avro schemas for the key and value of the event\n\t1. Create the Event to be sent\n\t1. Print it out for reference\n\t1. Create the Kafka Avro Producer and configure it\n\t1. Send the event\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Get the Kafka topic name\n\t\tparseArguments()\n\t\t# Get the avro schemas for the message's key and value\n\t\tevent_value_schema = getDefaultEventValueSchema(DATA_SCHEMAS)\n\t\tevent_key_schema = getDefaultEventKeySchema(DATA_SCHEMAS)\n\t\t# Create the event\n\t\tevent_value, event_key = createEvent()\n\t\t# Print out the event to be sent\n\t\tprint(\"--- Event to be published: ---\")\n\t\tprint(event_key)\n\t\tprint(event_value)\n\t\tprint(\"----------------------------------------\")\n\t\t# Create the Kafka Avro Producer\n\t\tkafka_producer = KafkaProducer(KAFKA_BROKERS,SCRAM_USERNAME,SCRAM_PASSWORD,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the Kafka Avro Producer\n\t\tkafka_producer.prepareProducer(\"ProduceAvroMessagePython\",event_key_schema,event_value_schema)\n\t\t# Publish the event\n\t\tkafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n\t```\n\nAs you can see, this python code depends on an **Avro Utils** for loading the Avro schemas and a **Kafka Avro Producer** to send the messages. These are explained next.\n\n### Avro Utils\n\nThis script, called [avroEDAUtils.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/avro_files/utils/avroEDAUtils.py), contains some very simple utility functions to be able to load Avro schemas from their **avsc** files in order to be used by the Kafka Avro Producer.\n\n1. A function to get the key and value Avro schemas for the messages to be sent:\n\n\t```python\n\tdef getDefaultEventValueSchema(schema_files_location):\n\t# Get the default event value data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_value_schema = LoadAvsc(schema_files_location + \"/default_value.avsc\", known_schemas)\n\treturn default_event_value_schema\n\n\tdef getDefaultEventKeySchema(schema_files_location):\n\t# Get the default event key data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_key_schema = LoadAvsc(schema_files_location + \"/default_key.avsc\", known_schemas)\n\treturn default_event_key_schema\n\t```\n\t(\\*) Where `known_schemas` is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this.\n\n1. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary:\n\n\t```python\n\tdef LoadAvsc(file_path, names=None):\n\t# Load avsc file\n\t# file_path: path to schema file\n\t# names(optional): avro.schema.Names object\n\tfile_text = open(file_path).read()\n\tjson_data = json.loads(file_text)\n\tschema = avro.schema.SchemaFromJSONData(json_data, names)\n\treturn schema\n\t```\n\n### Kafka Avro Producer\n\nThis script, called [KcAvroProducer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/kafka/KcAvroProducer.py), will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method:\n\n1. Initialize and prepare the Kafka Producer\n\n\t```python\n\tclass KafkaProducer:\n\n\t\tdef __init__(self,kafka_brokers = \"\",scram_username = \"\",scram_password = \"\",schema_registry_url = \"\"):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.scram_username = scram_username\n\t\t\tself.scram_password = scram_password\n\t\t\tself.schema_registry_url = schema_registry_url\n\n\t\tdef prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n\t\t\toptions ={\n\t\t\t\t\t'bootstrap.servers':  self.kafka_brokers,\n\t\t\t\t\t'schema.registry.url': self.schema_registry_url,\n\t\t\t\t\t'group.id': groupID,\n\t\t\t\t\t'security.protocol': 'SASL_SSL',\n\t\t\t\t\t'sasl.mechanisms': 'SCRAM-SHA-512',\n\t\t\t\t\t'sasl.username': self.scram_username,\n\t\t\t\t\t'sasl.password': self.scram_password,\n\t\t\t\t\t'ssl.ca.location': os.environ['PEM_CERT'],\n\t\t\t\t\t'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print out the configuration\n\t\t\tprint(\"--- This is the configuration for the avro producer: ---\")\n\t\t\tprint(options)\n\t\t\tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro Producer\n\t\t\tself.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n\t```\n\n1. Publish method\n\n\t```python\n\tdef publishEvent(self, topicName, value, key):\n\t\t# Produce the Avro message\n\t\t# Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n\t\tself.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n\t\t# Flush\n\t\tself.producer.flush()\n\t```\n\n## Python Avro Consumer\n\nIn this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.\n\n### Consume Message\n\nThe python script that we will use to consume an Avro message from a Kafka topic is [ConsumeAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/src/ConsumeAvroMessage.py) where we have the following:\n\n1. A function to parse arguments:\n\n\t```python\n\t# Parse arguments to get the container ID to poll for\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for the script are: \" , str(sys.argv))\n\t\tif len(sys.argv) != 2:\n\t\t\tprint(\"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\")\n\t\t\texit(1)\n\t\tTOPIC_NAME = sys.argv[1]\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments to get the topic to read from\n\t1. Create the Kafka Consumer and configure it\n\t1. Poll for next avro message\n\t1. Close the Kafka consumer\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Parse arguments\n\t\tparseArguments()\n\t\t# Create the Kafka Avro consumer\n\t\tkafka_consumer = KafkaConsumer(KAFKA_BROKERS,SCRAM_USERNAME,SCRAM_PASSWORD,TOPIC_NAME,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the consumer\n\t\tkafka_consumer.prepareConsumer()\n\t\t# Consume next Avro event\n\t\tkafka_consumer.pollNextEvent()\n\t\t# Close the Avro consumer\n\t\tkafka_consumer.close()\n\t```\n\nAs you can see, this python code depends on a **Kafka Avro Consumer** to consume messages. This is explained next.\n\n### Kafka Avro Consumer\n\nThis script, called [KcAvroConsumer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/kafka/KcAvroConsumer.py), will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method:\n\n1. Initialize and prepare the new Kafka consumer:\n\n\t```python\n\tclass KafkaConsumer:\n\n\t\tdef __init__(self, kafka_brokers = \"\", scram_username = \"\",scram_password = \"\", topic_name = \"\", schema_registry_url = \"\", autocommit = True):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.scram_username = scram_username\n\t\t\tself.scram_password = scram_password\n\t\t\tself.topic_name = topic_name\n\t\t\tself.schema_registry_url = schema_registry_url\n\t\t\tself.kafka_auto_commit = autocommit\n\n\t\t# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n\t\tdef prepareConsumer(self, groupID = \"pythonconsumers\"):\n\t\t\toptions ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'group.id': groupID,\n                'auto.offset.reset': 'earliest',\n                'schema.registry.url': self.schema_registry_url,\n                'enable.auto.commit': self.kafka_auto_commit,\n                'security.protocol': 'SASL_SSL',\n                'sasl.mechanisms': 'SCRAM-SHA-512',\n                'sasl.username': self.scram_username,\n                'sasl.password': self.scram_password,\n                'ssl.ca.location': os.environ['PEM_CERT'],\n                'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print the configuration\n\t\t\tprint(\"--- This is the configuration for the Avro consumer: ---\")\n        \tprint(options)\n        \tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro consumer\n\t\t\tself.consumer = AvroConsumer(options)\n\t\t\t# Subscribe to the topic\n\t\t\tself.consumer.subscribe([self.topic_name])\n\t```\n\n1. Poll next event method:\n\n\t```python\n\t# Prints out the message\n\tdef traceResponse(self, msg):\n        print('[Message] - Next message consumed from {} partition: [{}] at offset {} with key {} and value {}'\n                    .format(msg.topic(), msg.partition(), msg.offset(), msg.key(), msg.value() ))\n\n\t# Polls for next event\n\tdef pollNextEvent(self):\n\t\t# Poll for messages\n\t\tmsg = self.consumer.poll(timeout=10.0)\n\t\t# Validate the returned message\n\t\tif msg is None:\n\t\t\tprint(\"[INFO] - No new messages on the topic\")\n\t\telif msg.error():\n\t\t\tif (\"PARTITION_EOF\" in msg.error()):\n\t\t\t\tprint(\"[INFO] - End of partition\")\n\t\t\telse:\n\t\t\t\tprint(\"[ERROR] - Consumer error: {}\".format(msg.error()))\n\t\telse:\n\t\t\t# Print the message\n\t\t\tmsgStr = self.traceResponse(msg)\n\t```\n\n## Schemas and Messages\n\nIn this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python application presented above in the [Python Avro Producer](#python-avro-producer) and [Python Avro Consumer](#python-avro-consumer) sections.\n\n**IMPORTANT:** Before start using our Python application we must set the **PYTHONPATH** environment variable to point to where we have all the Python scripts that make up our application in order for Python to find these at execution time.\n\n1. Set the PYTHONPATH variable to the location where you cloned the GitHub repository containing the Python application we are going to be working with\n\n```shell\nexport PYTHONPATH=~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10\n```\n\n### Produce a message\n\nIn order to produce a message, we execute the `ProduceAvroMessage.py`. This script, as you could see in the [Python Avro Producer](#python-avro-producer) section, is sending the event with key `{'key': '1'}` and value `{'message': 'This is a test message'}` according to the schemas defined in [default_key.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cp4i-schema-lab-v10/avro_files/default_key.avsc) and [default_value.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cp4i-schema-lab-v10/avro_files/default_value.avsc) for the key and value of the event respectively.\n\n<InlineNotification kind=\"warning\">\n\nMake sure you are on the right path where the python scripts live: **~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10/src**\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema-user1 [0]\n```\n\nWe can see our new message delivered in the `test-schema-user1` topic by\n\n1. Go into the topics page in the IBM Event Streams UI\n\n\t![14](./images/14-v10.png)\n\n1. Click on the topic and then on the _Messages_ tab at the top. Finally, click on a message to see it displayed on the right hand side of the screen\n\n\t![15](./images/15-v10.png)\n\n<InlineNotification kind=\"info\">\n\n**INFO:** Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas.\n\n</InlineNotification>\n\n\n**IMPORTANT:** As you can see, we got the `test-schema-user1` topic auto-created when we produced the message. The reason for this is that\n\n1. Kafka is set out of the box to let applications to auto-create topics.\n1. We created the SCRAM credentials for our application to allow the application to create topics.\n\nOn a production-like environment, you don't want developers creating applications that auto-create topics in your IBM Event Streams instance without any control. For that, we would configure Kafka to forbid topic auto-creation (<https://kafka.apache.org/documentation/#auto.create.topics.enable>) as well as thoroughly created the SCRAM credentials with the most strict but appropriate permissions that our application needs.\n\n**IMPORTANT:** Similar to the auto-creation of topics, we can see below that our application got the Avro data schemas for both the key and value of the message produced auto-registered. This is because many client libraries come with a SerDes property to allow them to auto-register the Avro data schemas (<https://docs.confluent.io/current/clients/confluent-kafka-python/#avroserializer>). However, on a production-like environment we don't want applications to auto-register schemas without any control but yet we can not leave it to the developers to set the auto-registration property off on their libraries. Instead, we would create the SCRAM credentials with the most strict but appropriate permissions that our application needs.\n\nIf we look now at the schemas our schema registry has:\n\n```shell\ncloudctl es schemas\n\nSchema                           State    Latest version   Latest version ID   Updated\ndemoSchema_CLI_USER1             active   2.0.0            2                   Fri, 24 Jul 2020 14:09:37 UTC\ndemoSchema_UI_USER1              active   2.0.0            2                   Fri, 24 Jul 2020 14:06:27 UTC\ntest-schema-user1-key-d89uk      active   1                1                   Fri, 24 Jul 2020 15:41:46 UTC\ntest-schema-user1-value-tv5efr   active   1                1                   Fri, 24 Jul 2020 15:41:45 UTC\nOK\n```\n\nwe see two schemas, `test-schema-user1-key-d89uk` and `test-schema-user1-value-tv5efr`, which in fact correspond to the Avro data schema used for the `key` ([default_key.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cloud-schema-lab/avro_files/default_key.avsc)) and the `value` ([default_value.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cloud-schema-lab/avro_files/default_value.avsc)) of events sent to the `test-schema-user1` topic in the [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/src/ProduceAvroMessage.py) as explained before sending the message.\n\nTo make sure of what we are saying, we can inspect those schemas:\n\n```shell\ncloudctl es schema test-schema-user1-key-d89uk --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultKey\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"int\",\n      \"name\": \"key\",\n      \"doc\": \"We expect any int as the event key\"\n    }\n  ],\n  \"doc\": \"Default Message's key Avro data schema\"\n}\n```\n\n```shell\ncloudctl es schema test-schema-user1-value-tv5efr --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nIf I now decided that my events should contain another attribute, I would modify the event value schema ([default_value.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cloud-schema-lab/avro_files/default_value.avsc)) to reflect that as well as `ProduceAvroMessage.py` to send that new attribute in the event it sends:\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\", \"anotherAttribute\": \"Just another test string\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema-user1 [0]\n```\n\nI can see that an event with a new attribute has been sent:\n\n ![16](./images/16-v10.png)\n\nAnd I can also see that the new shcema has got registered as well:\n\n```shell\ncloudctl es schemas\n\nSchema                           State    Latest version   Latest version ID   Updated\ndemoSchema_CLI_USER1             active   2.0.0            2                   Fri, 24 Jul 2020 14:09:37 UTC\ndemoSchema_UI_USER1              active   2.0.0            2                   Fri, 24 Jul 2020 14:06:27 UTC\ntest-schema-user1-key-d89uk      active   1                1                   Fri, 24 Jul 2020 15:41:46 UTC\ntest-schema-user1-value-a5bbaa   active   1                1                   Fri, 24 Jul 2020 15:54:37 UTC\ntest-schema-user1-value-tv5efr   active   1                1                   Fri, 24 Jul 2020 15:41:45 UTC\nOK\n```\n\nIf I inspect that new schema, I see my new attribute in it:\n\n```shell\ncloudctl es schema test-schema-user1-value-a5bbaa --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any other string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\n<InlineNotification kind=\"info\">\n\nThe schema evolution above (test-schema-user1-value-a5bbaa) should have got registered as a new version of the already existing schema (test-schema-user1-value-tv5efr). IBM Event Streams allows schemas to auto-register themselves when these are sent along with a message from a producer application. However, the Schema Registry does not pick \"new\" schemas up as a new version of a previous schema and simply creates a new schema. Anyway, when reading messages off the topic, Schema Registry handles well what schema to return back to the receiver application so messages can get properly deserialized. Will see that in the next section.\n\n</InlineNotification>\n\n<InlineNotification kind=\"error\">\n\n**SECURITY:** As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is **NOT** a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the [Security](#security) section how we can control schema registration and evolution based on roles at the schema level also.\n\n</InlineNotification>\n\n### Create a non-compliant message\n\n\nLet's see what happens if we send a message that does not comply with its Avro data schema. Let's say that I send the following message:\n\n```shell\nkey = {\"key\": 1}\nvalue = {\"message\" : 12345}\n```\n\nand this is the output of that attempt:\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": 12345}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nTraceback (most recent call last):\n  File \"ProduceAvroMessage.py\", line 81, in <module>\n    kafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n  File \"/tmp/lab/kafka/KcAvroProducer.py\", line 43, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 99, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 118, in encode_record_with_schema\n    return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 152, in encode_record_with_schema_id\n    writer(record, outf)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 86, in <lambda>\n    return lambda record, fp: writer.write(record, avro.io.BinaryEncoder(fp))\n  File \"/root/.local/lib/python3.7/site-packages/avro/io.py\", line 771, in write\n    raise AvroTypeException(self.writer_schema, datum)\navro.io.AvroTypeException: The datum {'message': 12345} is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any other string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nAs we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply (the message value attribute we are sending is an integer rather than a string and we are missing the second attribute).\n\nTherefore, using Avro schemas with IBM Event Streams give us the ability to build our system with **robustness** protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.\n\n### Consume a message\n\nIn order to consume a message, we execute the `ConsumeAvroMessage.py` within the `/tmp/lab/src` folder in our python demo environment:\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ConsumeAvroMessage.py test-schema-user1\n\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema-user1']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema-user1 partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n\npython3 ConsumeAvroMessage.py test-schema-user1\n\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema-user1']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema-user1 partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\nAs you can see, our script was able to read the Avro messages from the `test-schema-user1` topic and map that back to their original structure thanks to the Avro schemas:\n\n```shell\n[Message] - Next message consumed from test-schema partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n[Message] - Next message consumed from test-schema partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\n## Data Evolution\n\nSo far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich CLI IBM Event Streams provides to interact with.\n\nHowever, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying [Event Storming](/methodology/event-storming/) or [Domain Driven Design](/methodology/domain-driven-design/) for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.\n\nBut it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to [hundreds of years](https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/)) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the **compatibility** of old and new data schemas and, in fact, old and new data at the end of the day.\n\n**The IBM Event Streams Schema Registry enforces full compatibility when creating a new version of a schema**. Full compatibility means that **old data can be read with the new data schema, and new data can also be read with the last data schema**.\n\nIn data formats like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. Let's see then how this affects when you want your data to evolve in a way that it needs to add a new attribute or delete an existing attribute.\n\nBut let's see what that means in terms of adding and removing attributes from your data schema.\n\n### Adding a new attribute\n\nAlthough we have already seen this in the adding a new version of a schema section, let's try to add a new version of our `test-schema-value` schema where we have a new attribute. Remember, our `default_schema.avsc` already contains a new attribute than the original one but that it got registered as a new schema rather than as a new version of the original one. Let's reuse that Avro schema file to register it as a new version (**INFO:** you might need to copy/download that file to your local workstation in order to be able to then upload it to the IBM Event Streams through its UI)\n\nWhen doing so from the UI, we see the following error:\n\n  ![17](./images/17-v10.png)\n\nThe reason, as alread explained in the add a new version of a schema section, is because full compatibility dictates that you can only add new attributes to a schema if these have a default value. Reason being that a receiver should be able to deserialize messages produced with an older schema using the newer schema. Because old messages were written with an older schema that did not contain our new attribute, those messages won't have that attribute so we need to provide a default value for it in our never version of the schema so that the receiver is able to deserialize those older messages with the newer schema.\n\nIf we add the default value for the new attribute, we see that our newer version is now compatible:\n\n  ![18](./images/18-v10.png)\n\nand that it gets registered fine:\n\n  ![19](./images/19-v10.png)\n\n### Removing an existing attribute\n\nWhat if we now wanted to remove the original `message` attribute from our schema. Let's remove it from the `default_value.avsc` file and try to register that new version:\n\n  ![20](./images/20-v10.png)\n\nWe, again, get the same error. And the reason is because receivers must be able to read and deserialize messages produced with the newer schema (that is, without the `message` attribute) but with the older schema (that is, with the schema version that enforces the existence of the `message` attribute).\n\nIn order to work this around, what we need to do is to register first an intermediate schema that defines a default value for the `message` attribute:\n\n  ![21](./images/21-v10.png)\n\nOnce we have a default value for the `message` attribute, we can register a new version of the schema that finally removes that attribute:\n\n  ![22](./images/22-v10.png)\n\n## Security\n\nAs we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams), etc since we don't want everyone and everything to be, for instance, creating or deleting topics, schemas, etc.\n\nYou can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in Access Control List (ACL) rules:\n\n* **Topics (topic):** you can control the ability of users and applications to create, delete, read, and write to a topic.\n* **Consumer groups (group):** you can control an application’s ability to join a consumer group.\n* **Transactional IDs (transactionalId):** you can control the ability to use the transaction capability in Kafka.\n\nNote: Schemas in the Event Streams Schema Registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas.\n\nYou can find more information about how to secure your IBM Event Streams resources in the official documentation at: <https://ibm.github.io/event-streams/security/managing-access/>\n","type":"Mdx","contentDigest":"5843aa0d18ab2f6312e29d6fb6cf5224","counter":637,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: IBM Event Streams Schema Registry from IBM CloudPak for Integration\ndescription: Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature\n---\n\n<!-- Originally available via \"/technology/event-streams/schema-registry-cp4i-v10/\" -->\n\nThis documentation aims to be a introductory hands-on lab on the IBM Event Streams Schema Registry installed throught the **IBM Cloud Pak for Integration V2020.2.X+** on an Openshift cluster.\n\n## Index\n\n<AnchorLinks>\n  <AnchorLink>Requirements</AnchorLink>\n  <AnchorLink>IBM Cloud Shell</AnchorLink>\n  <AnchorLink>Schema Registry</AnchorLink>\n  <AnchorLink>Schemas</AnchorLink>\n  <AnchorLink>IBM Event Streams Credentials</AnchorLink>\n  <AnchorLink>Python Application</AnchorLink>\n  <AnchorLink>Python Avro Producer</AnchorLink>\n  <AnchorLink>Python Avro Consumer</AnchorLink>\n  <AnchorLink>Schemas and Messages</AnchorLink>\n  <AnchorLink>Data Evolution</AnchorLink>\n  <AnchorLink>Security</AnchorLink>\n</AnchorLinks>\n\n## Requirements\n\nThis lab requires the following components to work against:\n\n1. An IBM Event Streams V10 instance installed through the IBM CloudPak for Integration V2020.2.X or greater.\n2. An IBM Cloud Shell - <https://www.ibm.com/cloud/cloud-shell>\n\n## IBM Cloud Shell\n\nHere we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.\n\nStart your IBM Cloud Shell by pointing your browser to <https://cloud.ibm.com/shell>\n\n![shell](./images/shell-v10.png)\n\n#### IBM Cloud Pak CLI\n\nCloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the IBM Cloud Pak CLI - `curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz`\n1. Untar it - `tar -xvf cloudctl-linux-amd64.tar.gz`\n1. Rename it for ease of use - `mv cloudctl-linux-amd64 cloudctl`\n1. Include it to the PATH environment variable - `export PATH=$PATH:$PWD`\n1. Make sure your IBM Cloud Pak CLI is in the path- `which cloudctl`\n1. Make sure your IBM Cloud Pak CLI works - `cloudctl help`\n\n![shell2](./images/shell2-v10.png)\n\n#### Event Streams plugin for IBM Cloud Pak CLI\n\nThis plugin will allow us to manage IBM Event Streams.\n\nIn order to install it, execute the following commands in your IBM Cloud Shell:\n\n1. Download the Event Streams plugin for IBM Cloud Pak CLI - `curl -L http://ibm.biz/es-cli-linux -o es-plugin`\n1. Install it - `cloudctl plugin install es-plugin`\n1. Make sure it works - `cloudctl es help`\n\n![shell3](./images/shell3-v10.png)\n\n#### Git\n\nIBM Cloud Shell comes with Git already installed out of the box.\n\n#### Vi\n\nIBM Cloud Shell comes with Vi already installed out of the box.\n\n#### Python 3\n\nIBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are `confluent_kafka` and `avro-python3`\n\nIn order to install these modules, execute the following command in your IBM Cloud Shell:\n\n1. Install the modules - `python3 -mpip install avro-python3 confluent_kafka`\n\n![shell4](./images/shell4-v10.png)\n\n\n**Congrats!** you have now your IBM Cloud Shell ready to start working.\n\n## Schema Registry\n\n![diagram](./images/schema-registry.png)\n\nOne of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro (<https://avro.apache.org/docs/current/>). To learn more about Apache Avro, how to define Apache Avro data schemas and more, we strongly recommend to read through our documentation on Avro and data schemas [here](/technology/avro-schemas/)\n\nIBM Event Streams development team has developed a Schema Registry to work along your Kafka cluster to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. The Schema Registry will also provide the ability for schemas to evolve in time.\n\n### Accessing the Schema Registry\n\n#### UI\n\nTo access the schema registry, we first need to log into IBM Event Streams.\n\n1. Point your browser to your IBM Event Streams instace's user interface url and introduce your credentials\n\n\t![login](./images/login-v10.png)\n\n1. Once you are logged into your IBM Event Streams instance, you simply need to click on the Schema Registry button on the main left hand vertical menu bar:\n\n  ![4](./images/4-v10.png)\n\n\n#### CLI\n\nWe can also interact with the Schema Registry through the IBM Event Streams CLI. In order to do so, we first need to log in with the IBM Cloud Pak CLI:\n\n1. Log into your cluster with the IBM CloudPak CLI\n\n\t<InlineNotification kind=\"warning\">\n\n\tMake sure to use the appropriate credentials and select the namespace where your IBM Event Streams instance is installed\n\n\t</InlineNotification>\n\n\t```shell\n\tcloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n\n\tUsername> admin\n\n\tPassword>\n\tAuthenticating...\n\tOK\n\n\tTargeted account mycluster Account\n\n\tEnter a namespace > integration\n\tTargeted namespace integration\n\n\tConfiguring kubectl ...\n\tProperty \"clusters.mycluster\" unset.\n\tProperty \"users.mycluster-user\" unset.\n\tProperty \"contexts.mycluster-context\" unset.\n\tCluster \"mycluster\" set.\n\tUser \"mycluster-user\" set.\n\tContext \"mycluster-context\" created.\n\tSwitched to context \"mycluster-context\".\n\tOK\n\n\tConfiguring helm: /Users/user/.helm\n\tOK\n\t```\n\n1. Initialize the Event Streams CLI plugin\n\n\t```shell\n\tcloudctl es init\n\n\tIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-solutions.gse-ocp.net\n\tNamespace:                                     integration\n\tName:                                          es-1\n\tIBM Cloud Pak for Integration UI address:      No instance of Cloud Pak for Integration has been found. Please check that you have access to it.\n\tEvent Streams API endpoint:                    https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams API status:                      OK\n\tEvent Streams UI address:                      https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams Schema Registry endpoint:        https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams bootstrap address:               es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n\tOK\n\t```\n\n\t(\\*)The above information will later be used in the [IBM Event Streams Credentials](#ibm-event-streams-credentials) section as these are neeeded by the Python application we will work with.\n\n1. Make sure you can access the IBM Event Streams Schema Registry:\n\n\t```bash\n\tcloudctl es schemas\n\n\tNo schemas were found.\n\tOK\n\t```\n\n## Schemas\n\nIn this section we will finally get our hands dirty with the IBM Event Steams Schema Registry capability by working with Apache Avro schemas and the Schema Registry.\n\n<InlineNotification kind=\"info\">\n\nWe recommend to complete most of the UI steps from your **local workstation** since these will require you to upload the files your create/modify to IBM Event Streams and that requires having your files available locally on your workstation rather than on the IBM Cloud Shell\n\n</InlineNotification>\n\n### Create a schema\n\nLet's see how can we create a schema to start playing with.\n\n#### UI\n\nThe IBM EVent Streams user interface allow us to create schemas only from _json_ or Avro schema _avsc_ files.\n\n1. Create an Avro schema file **avsc** with your schema:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\techo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI_USER1\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-ui.avsc\n\t```\n\n1. On the IBM Event Streams Schema Registry User Interface, Click on _Add schema_ button on the top right corner.\n\n1. Click on _Upload definition_ button on the left hand side and select the `demoschema-ui.avsc` file we just created.\n\n1. You should now see you Avro schema loaded in the UI with two tabs, definition and preview to make sure your schema looks as desired:\n\n\t![5](./images/5-v10.png)\n\n1. Click on _Add schema_ button at the top right corner and you should now see that schema listed among your other schemas.\n\n#### CLI\n\n1. Create another Avro schema **avsc** file with a different schema:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\techo '{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_CLI_USER1\",\n\t\"namespace\": \"schemas.demo.cli\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"}]\n\t}' > demoshema-cli.avsc\n\t```\n\n1. Create a schema by executing the following command:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tSchema demoSchema_CLI_USER1 is active.\n\n\tVersion   Version ID   Schema           State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI_USER1   active   Thu, 25 Jun 2020 11:30:42 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\tOK\n\t```\n\n### List schemas\n\n#### UI\n\nIn order to list the schemas in the UI you just simply need to open up the Schema Registry User Interface and schemas will get listed in there automatically. You also have a search tool bar at the top. You can also see more details about your schema by clicking the drop down arrow on its left:\n\n  ![6](./images/6-v10.png)\n\n#### CLI\n\n1. Execute the following command to list the schemas in your Schema Registry:\n\n\t```bash\n\tcloudctl es schemas\n\n\tSchema                 State    Latest version   Latest version ID   Updated\n\tdemoSchema_CLI_USER1   active   1.0.0            1                   Fri, 24 Jul 2020 13:55:49 UTC\n\tdemoSchema_UI_USER1    active   1.0.0            1                   Fri, 24 Jul 2020 13:55:51 UTC\n\tOK\n\n\t```\n\n### Delete schemas\n\n#### UI\n\n1. Click on the schema you want to delete.\n1. Click on the _Manage schema_ tab at the top.\n1. Click on _Remove schema_\n\n\t![7](./images/7-v10.png)\n\n#### CLI\n\nTo remove a schema using the CLI, simply execute the following command and confirm:\n\n<InlineNotification kind=\"warning\">\n\nChange USER1\n\n</InlineNotification>\n\n```bash\ncloudctl es schema-remove demoSchema_CLI_USER1\n\nRemove schema demoSchema_CLI_USER1 and all versions? [y/n]> y\nSchema demoSchema_CLI_USER1 and all versions removed.\nOK\n```\n\n### Create new schema version\n\nTo create a new version of a schema,\n\n1. Let's first create again the previous two schemas:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-ui.avsc\n\n\tSchema demoSchema_UI_USER1 is active.\n\n\tVersion   Version ID   Schema                State    Updated                         Comment\n\t1.0.0     1            demoSchema_UI_USER1   active   Fri, 24 Jul 2020 13:59:55 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_UI_USER1 to the registry.\n\tOK\n\t```\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tSchema demoSchema_CLI_USER1 is active.\n\n\tVersion   Version ID   Schema                 State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n\n\tAdded version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\tOK\n\t```\n\n1. Add a new attribute to the schemas by editing their Avro schema avsc files:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\tcat demoshema-ui.avsc\n\n\t{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI_USER1\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"},\n\t\t{\"name\": \"attribute1\",\"type\":\"string\"}]\n\t}\n\t```\n\n#### UI\n\n1. Click on the schema you want to create a new version for.\n1. Click on the _Add new version_ button on the left hand side.\n1. Click on _Upload definition_ button on the left hand side.\n1. Select the Avro schema avsc file and click ok.\n\n\t![8](./images/8-v10.png)\n\n<InlineNotification kind=\"error\">\n\n**ERROR:** The error we are seeing on the screen is because the IBM Event Streams Schema Registtry enforces full compatibility: <https://ibm.github.io/event-streams/schemas/creating/#adding-new-schema-versions>\n\n</InlineNotification>\n\n**Full compatibility** for data schemas means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. More on data schema compatibility on the section [Data Evolution](#data-evolution) towards the end of this lab.\n\nAs explained in the error notification above, we need to add a default value for our new attribute in our data schema so that messages serialized with an older version of the data schema which won't contain this new attribute can later be deserialized with the newer version of the data schema that expects such attribute. By providing a default value, we allow deserializers to consume messages that do not contain newer attributes.\n\n1. Add a default value for the new attribute:\n\n\t<InlineNotification kind=\"warning\">\n\n\tChange USER1\n\n\t</InlineNotification>\n\n\t```bash\n\tcat demoshema-ui.avsc\n\n\t{\n\t\"type\":\"record\",\n\t\"name\":\"demoSchema_UI_USER1\",\n\t\"namespace\": \"schemas.demo.ui\",\n\t\"fields\":[\n\t\t{\"name\": \"eventKey\",\"type\":\"string\"},\n\t\t{\"name\": \"message\",\"type\":\"string\"},\n\t\t{\"name\": \"attribute1\",\"type\":\"string\",\"default\": \"whatever\"}]\n\t}\n\t```\n\n1. Repeat the steps for adding a new version of a schema above.\n1. This time you should see that the schema is valid:\n\n\t![9](./images/9-v10.png)\n\n1. However, it still does not let us add this new version to the data schema until we actually provide a version for it. Click on the _Add +_ link on the right of the version attribute of the schema and give it `2.0.0` for example (hit enter for the version to take the value you type in).\n1. Click on _Add schema_.\n1. You should now see the two versions for your data schema on the left hand side.\n\n\t![10](./images/10-v10.png)\n\n1. If you go back to the Schema Registry page where all your schemas are listed, you should now see that the latest version for your data schema is `2.0.0` now.\n\n#### CLI\n\n1. If we try to add the new version of the schema from its `demoschema-cli.avsc` Avro schema file, we will get the same error as in the previous UI example:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 400. Avro schema is not compatible with latest schema version: Compatibility type 'MUTUAL_READ' does not hold between 1 schema(s) in the chronology because: Schema[0] has incompatibilities: ['READER_FIELD_MISSING_DEFAULT_VALUE: attribute1' at '/fields/2'].\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\t```\n\n1. Add the default value for the new attribute in your Avro schema avsc file and try to add that new version of the schema:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc\n\n\tFAILED\n\tEvent Streams API request failed:\n\tError response from server. Status code: 409. Schema version name already exists\n\n\tUnable to add version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\t```\n\n1. We see that we still have an error because we have not specified a new version value. Specify a new version value when adding this new version of the schema:\n\n\t```bash\n\tcloudctl es schema-add --file demoshema-cli.avsc --version 2.0.0\n\n\tSchema demoSchema_CLI_USER1 is active.\n\n\tVersion   Version ID   Schema                 State    Updated                         Comment\n\t1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n\t2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\n\n\tAdded version 2.0.0 of schema demoSchema_CLI_USER1 to the registry.\n\tOK\n\t```\n\n### Get latest version of a schema\n\n#### UI\n\nIn order to see the latest version of a data schema using the UI, we just need to go to the Schema Registry web user interface and click on the expand arrow buttton that is on the left:\n\n ![11](./images/11.png)\n\n#### CLI\n\nIn order to see the latest version of a data schema using the CLI, we simply need to run the following command:\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1 --version 2\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI_USER1\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"attribute1\",\n      \"type\": \"string\",\n      \"default\": \"whatever\"\n    }\n  ]\n}\n```\n\n(\\*) The version you specify is actually the version ID (2) rather than the version name we gave to the newer schema version (2.0.0):\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n```\n\n### Get specific version of a schema\n\n#### UI\n\nTo see a specific version of a schema, go to the Schema Registry web user interface and click on the schema you want to see the version for. You will now see how many version of the schema you have and you can click on any of these in order to see more details about it.\n\n  ![12](./images/12-v10.png)\n\n#### CLI\n\nTo see a specific version of a schema using the CLI, simply run the following command with the version ID you would like to get retrieved:\n\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1 --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI_USER1\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n### Listing all versions of a schema\n\n#### UI\n\nTo list all versions of schema in the Schema Registry user interface, you simply need to click on the data schema you want and a new page will display these:\n\n  ![12](./images/12-v10.png)\n\n#### CLI\n\nIn order to display all versions of a schema, run the following command:\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n```\n\n### Deleting a version of a schema\n\n#### UI\n\nIn order to delete a version of a schema using the Schema Registry user interface,\n\n1. Click on the data schema you want a version of it deleted for.\n1. Select the version you want to delete on the left hand side.\n1. Click on _Manage version_ button that is on the top right corner within the main box in the center of the page.\n1. Click on _Remove version_.\n\n\t![13](./images/13-v10.png)\n\n#### CLI\n\nIn order to delete a version of a schema through the CLI, execute the following command:\n\n```bash\ncloudctl es schema-remove demoSchema_CLI_USER1 --version 1\n\nRemove version with ID 1 of schema demoSchema_CLI_USER1? [y/n]> y\nVersion with ID 1 of schema demoSchema_CLI_USER1 removed.\nOK\n```\n\nWe can see only version 2 now:\n\n```bash\ncloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n```\n\n## IBM Event Streams Credentials\n\nWe have seen how to interact with the IBM Event Streams Schema Registry in order to create, delete, update, etc schemas that our applications will theoretically used for data correctness and application robusteness. However, the first thing that we need to set up in our IBM Event Streams instance are these applications' **service credentials** to be able to interact with IBM Event Streams and its Schema Registry. For doing so, we can either use either the GUI or the CLI.\n\n### GUI\n\n1. Go to you IBM Event Streams instance console\n\n\t![1](./images/1.png)\n\n1. Click on **Connect to this cluster**\n\n\t![2](./images/2.png)\n\nIn this panel, you will find\n\n1. The **Botstrap server** to connect your applications to in order to send and receive messages from your IBM Event Streams instance. We can see we have one **external listener** (whith SCRAM-SHA authentication) and one internal listener (Depending your IBM Event Streams installation you might have different listeners and authentications for these).\n1. The **Schema Registry url** your applications will need to work with Apache Avro data schemas.\n1. A _Generate SCRAM credentials_ button to generate the **SCRAM credentials** your applications will need to authenticate with.\n1. A _Certificates_ section to download either the **Java PKCS12** or the **PEM** certificates (or both) that your applications will need in order to be able to establish the communitaction with your IBM Event Streams instance.\n\n\t![3](./images/3-v10.png)\n\nTo generate the SCRAM credentials needed by your application to get authenticated against IBM Event Streams to be able to produce and consume messages as well as to create, delete, etc topics and schemas, we need to create a **KafkaUser** (this happens behind the scenes) which we will set some permissions and get the corresponding SCRAM usernname and password for to be used in our applications kafka clients configuration:\n\n1. Click on _Generate SCRAM credentials_\n1. Enter a user name for your credentials and click next (leave the last option selected: _Produce messages, consume messages and create topics and schemas_ so that we give full access to our user for simplicity)\n\n\t![scram1](./images/scram-1-v10.png)\n\n1. Select all topics and click next\n\n\t![scram2](./images/scram-2-v10.png)\n\n1. Select all consumer groups and click next\n\n\t![scram3](./images/scram-3-v10.png)\n\n1. Select all transactional IDs and click next\n\n\t![scram4](./images/scram-4-v10.png)\n\nOnce you have created your new KafkaUser, you get the SCRAM credentials displayed on the screen:\n\n![scram5](./images/scram-5-v10.png)\n\n<InlineNotification kind=\"info\">\n\nYou can download the PEM certificate from the UI and then use the IBM Cloud Shell upload file option on the top bar or you can download it from within your IBM Cloud Shell by using the CLI (see below)\n\n</InlineNotification>\n\n### CLI\n\n<InlineNotification kind=\"info\">\n\nThe following two steps should have been completed already in the previous [Accessing the Schema Registry](#accessing-the-schema-registry) section\n\n</InlineNotification>\n\n1. Log into your cluster with the IBM CloudPak CLI\n\n\t```shell\n\tcloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n\n\tUsername> admin\n\n\tPassword>\n\tAuthenticating...\n\tOK\n\n\tTargeted account mycluster Account\n\n\tEnter a namespace > integration\n\tTargeted namespace integration\n\n\tConfiguring kubectl ...\n\tProperty \"clusters.mycluster\" unset.\n\tProperty \"users.mycluster-user\" unset.\n\tProperty \"contexts.mycluster-context\" unset.\n\tCluster \"mycluster\" set.\n\tUser \"mycluster-user\" set.\n\tContext \"mycluster-context\" created.\n\tSwitched to context \"mycluster-context\".\n\tOK\n\n\tConfiguring helm: /Users/jesusalmaraz/.helm\n\tOK\n\t```\n\n1. Initialize the Event Streams CLI plugin\n\n\t```shell\n\tcloudctl es init\n\n\tIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-solutions.gse-ocp.net\n\tNamespace:                                     integration\n\tName:                                          es-1\n\tIBM Cloud Pak for Integration UI address:      No instance of Cloud Pak for Integration has been found. Please check that you have access to it.\n\tEvent Streams API endpoint:                    https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams API status:                      OK\n\tEvent Streams UI address:                      https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams Schema Registry endpoint:        https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\n\tEvent Streams bootstrap address:               es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n\tOK\n\t```\n\nWe can see above the Event Streams **bootstrap address** and **Schema Registry url** that our applications will need in order to connect to this Event Streams instance\n\nTo be able to establish communication and authenticate against your IBM Event Streams instance, you will need the PEM certificate and an the SCRAM credentials:\n\n1. To download your **PEM certificate**, you can use the following command:\n\n\t```shell\n\tcloudctl es certificates --format pem\n\n\tCertificate successfully written to /home/ALMARAZJ/es-cert.pem.\n\tOK\n\t```\n\n1. To generate your SCRAM credentials, we first need to create a **KafkaUser**, you can use the following command:\n\n\t```shell\n\tcloudctl es kafka-user-create --name my-user1 --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n\n\tKafkaUser name   Authentication   Authorization   Username                                                Secret\n\tmy-user1         scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret\n\n\tResource type     Name        Pattern type   Host   Operation\n\ttopic             *           literal        *      Read\n\ttopic             __schema_   prefix         *      Read\n\ttopic             *           literal        *      Write\n\ttopic             *           literal        *      Create\n\ttopic             __schema_   prefix         *      Alter\n\tgroup             *           literal        *      Read\n\ttransactionalId   *           literal        *      Write\n\n\tCreated KafkaUser my-user1.\n\tOK\n\t```\n\n<InlineNotification kind=\"warning\">\n\nWe recommend to carefully set appropriate roles as well as access to topics, groups, transaction IDs and schemas for the API keys that you generate.\n\n</InlineNotification>\n\nWhen a KafkaUser custom resource is created, the Entity Operator within Event Streams will create the principal in ZooKeeper with appropriate ACL entries. It will also create a Kubernetes Secret that contains the Base64-encoded SCRAM password for the scram-sha-512 authentication type, or the Base64-encoded certificates and keys for the tls authentication type.\n\n1. Retrieve the username and the secret name containing the password of your SCRAM credentials for your KafkaUser:\n\n\t```shell\n\toc get kafkauser my-user1 --namespace integration -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'\n\n\tusername: my-user1\n\tsecret-name: my-user1\n\t```\n1. Retrieve the password of your SCRAM credentials from the secret above:\n\n\t```shell\n\toc get secret my-user1 --namespace integration -o jsonpath='{.data.password}' | base64 --decode\n\n\t*****\n\t```\n\n### Environment variables\n\nNow that we have generated the appropriate IBM Event Streams credentials for applications to be able to establish communication and authenticate against our IBM Event Streams instance, we are going to set some environment variables that will be used by our Python application:\n\n1. **KAFKA_BROKERS** which should take the value of **bootstrap server**:\n\n\t```shell\n\texport KAFKA_BROKERS=es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n\t```\n\n1. **SCRAM_USERNAME** which should take the value of the **SCRAM username** you have generated:\n\n\t```shell\n\texport SCRAM_USERNAME=my-user1\n\t```\n\n1. **SCRAM_PASSWORD** which should take the value of the **SCRAM password** you have generated:\n\n\t```shell\n\texport SCRAM_PASSWORD=*****\n\t```\n\n1. **PEM_CERT** which should take the value of the location where the PEM certificate is within your IBM Cloud Shell:\n\n\t<InlineNotification kind=\"warning\">\n\n\tSet the path appropriately to your IBM Cloud Shell\n\n\t</InlineNotification>\n\n\t```shell\n\texport PEM_CERT=~/es-cert.pem\n\t```\n\n\t(\\*) Don't forget to download both the PEM certificate to your IBM Cloud Shell through the CLI or upload it to your IBM Cloud Shell from your laptop if you used the UI to get the certificate. Review previous section if needed.\n\n1. **SCHEMA_REGISTRY_URL** which should be a combination of the **SCRAM username**, the **SCRAM password** and the **Schema Registry url** in the form of:\n\n\t`https://<SCRAM_username>:<SCRAM_password>@<Schema_Registry_url>`\n\n\t```shell\n\texport SCHEMA_REGISTRY_URL=https://${SCRAM_USERNAME}:${SCRAM_PASSWORD}@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\n\t```\n\n## Python Application\n\nThe Python application we have built to see how to produce and consume messages (either plain messages or Avro encoded messages based on Avro Data Schemas) to and from an IBM Event Streams instance installed through the IBM Cloud Pak for Integration is public at the following GitHub repo: <https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cp4i-schema-lab-v10>\n\n### Clone\n\nIn order to use and work with this Python application, the first thing we need to do is to clone the GitHub repository where it is published.\n\n1. Clone the github repository on your workstation on the location of your choice:\n\n\t```shell\n\tgit clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\n\n\tCloning into 'refarch-eda-tools'...\n\tremote: Enumerating objects: 185, done.\n\tremote: Counting objects: 100% (185/185), done.\n\tremote: Compressing objects: 100% (148/148), done.\n\tremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\n\tReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\n\tResolving deltas: 100% (23/23), done.\n\t```\n\n1. Change directory into `refarch-eda-tools/labs/es-cp4i-schema-lab-v10` to find the assets we will we working from now on for the python demo environment and few other scripts/applications:\n\n\t```shell\n\tcd refarch-eda-tools/labs/es-cp4i-schema-lab-v10\n\n\t$ ls -all\n\ttotal 240\n\tdrwxr-xr-x   9 user  staff     288 20 May 19:33 .\n\tdrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n\t-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\n\tdrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\n\tdrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n\t```\n\nIn the next sections, we are going to briefly explain the implementation of this Python application so that you understand what is being done behind the scenes and more importantly, if you are a developer, how to do so.\n\n## Python Avro Producer\n\nIn this section we describe the Python scripts we will be using in order to be able to produce **avro** messages to a Kafka topic.\n\n### Produce Message\n\nThe python script that we will use to send an avro message to a Kafka topic is [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/src/ProduceAvroMessage.py) where we have the following:\n\n1. A function to parse the arguments:\n\n\t```python\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for this script are: \" , str(sys.argv))\n\t\tif len(sys.argv) == 2:\n\t\t\tTOPIC_NAME = sys.argv[1]\n\t\telse:\n\t\t\tprint(\"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\")\n\t\t\texit(1)\n\t```\n\n1. A function to create the event to be sent:\n\n\t```python\n\tdef createEvent():\n\t\tprint('Creating event...')\n\n\t\tkey = {\"key\": 1}\n\t\tvalue = {\"message\" : \"This is a test message\"}\n\n\t\tprint(\"DONE\")\n\n\t\treturn json.dumps(value), json.dumps(key)\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments\n\t1. Get the Avro schemas for the key and value of the event\n\t1. Create the Event to be sent\n\t1. Print it out for reference\n\t1. Create the Kafka Avro Producer and configure it\n\t1. Send the event\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Get the Kafka topic name\n\t\tparseArguments()\n\t\t# Get the avro schemas for the message's key and value\n\t\tevent_value_schema = getDefaultEventValueSchema(DATA_SCHEMAS)\n\t\tevent_key_schema = getDefaultEventKeySchema(DATA_SCHEMAS)\n\t\t# Create the event\n\t\tevent_value, event_key = createEvent()\n\t\t# Print out the event to be sent\n\t\tprint(\"--- Event to be published: ---\")\n\t\tprint(event_key)\n\t\tprint(event_value)\n\t\tprint(\"----------------------------------------\")\n\t\t# Create the Kafka Avro Producer\n\t\tkafka_producer = KafkaProducer(KAFKA_BROKERS,SCRAM_USERNAME,SCRAM_PASSWORD,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the Kafka Avro Producer\n\t\tkafka_producer.prepareProducer(\"ProduceAvroMessagePython\",event_key_schema,event_value_schema)\n\t\t# Publish the event\n\t\tkafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n\t```\n\nAs you can see, this python code depends on an **Avro Utils** for loading the Avro schemas and a **Kafka Avro Producer** to send the messages. These are explained next.\n\n### Avro Utils\n\nThis script, called [avroEDAUtils.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/avro_files/utils/avroEDAUtils.py), contains some very simple utility functions to be able to load Avro schemas from their **avsc** files in order to be used by the Kafka Avro Producer.\n\n1. A function to get the key and value Avro schemas for the messages to be sent:\n\n\t```python\n\tdef getDefaultEventValueSchema(schema_files_location):\n\t# Get the default event value data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_value_schema = LoadAvsc(schema_files_location + \"/default_value.avsc\", known_schemas)\n\treturn default_event_value_schema\n\n\tdef getDefaultEventKeySchema(schema_files_location):\n\t# Get the default event key data schema\n\tknown_schemas = avro.schema.Names()\n\tdefault_event_key_schema = LoadAvsc(schema_files_location + \"/default_key.avsc\", known_schemas)\n\treturn default_event_key_schema\n\t```\n\t(\\*) Where `known_schemas` is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this.\n\n1. A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary:\n\n\t```python\n\tdef LoadAvsc(file_path, names=None):\n\t# Load avsc file\n\t# file_path: path to schema file\n\t# names(optional): avro.schema.Names object\n\tfile_text = open(file_path).read()\n\tjson_data = json.loads(file_text)\n\tschema = avro.schema.SchemaFromJSONData(json_data, names)\n\treturn schema\n\t```\n\n### Kafka Avro Producer\n\nThis script, called [KcAvroProducer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/kafka/KcAvroProducer.py), will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method:\n\n1. Initialize and prepare the Kafka Producer\n\n\t```python\n\tclass KafkaProducer:\n\n\t\tdef __init__(self,kafka_brokers = \"\",scram_username = \"\",scram_password = \"\",schema_registry_url = \"\"):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.scram_username = scram_username\n\t\t\tself.scram_password = scram_password\n\t\t\tself.schema_registry_url = schema_registry_url\n\n\t\tdef prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n\t\t\toptions ={\n\t\t\t\t\t'bootstrap.servers':  self.kafka_brokers,\n\t\t\t\t\t'schema.registry.url': self.schema_registry_url,\n\t\t\t\t\t'group.id': groupID,\n\t\t\t\t\t'security.protocol': 'SASL_SSL',\n\t\t\t\t\t'sasl.mechanisms': 'SCRAM-SHA-512',\n\t\t\t\t\t'sasl.username': self.scram_username,\n\t\t\t\t\t'sasl.password': self.scram_password,\n\t\t\t\t\t'ssl.ca.location': os.environ['PEM_CERT'],\n\t\t\t\t\t'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print out the configuration\n\t\t\tprint(\"--- This is the configuration for the avro producer: ---\")\n\t\t\tprint(options)\n\t\t\tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro Producer\n\t\t\tself.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n\t```\n\n1. Publish method\n\n\t```python\n\tdef publishEvent(self, topicName, value, key):\n\t\t# Produce the Avro message\n\t\t# Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n\t\tself.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n\t\t# Flush\n\t\tself.producer.flush()\n\t```\n\n## Python Avro Consumer\n\nIn this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.\n\n### Consume Message\n\nThe python script that we will use to consume an Avro message from a Kafka topic is [ConsumeAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/src/ConsumeAvroMessage.py) where we have the following:\n\n1. A function to parse arguments:\n\n\t```python\n\t# Parse arguments to get the container ID to poll for\n\tdef parseArguments():\n\t\tglobal TOPIC_NAME\n\t\tprint(\"The arguments for the script are: \" , str(sys.argv))\n\t\tif len(sys.argv) != 2:\n\t\t\tprint(\"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\")\n\t\t\texit(1)\n\t\tTOPIC_NAME = sys.argv[1]\n\t```\n\n1. The main where we will:\n\t1. Parse the arguments to get the topic to read from\n\t1. Create the Kafka Consumer and configure it\n\t1. Poll for next avro message\n\t1. Close the Kafka consumer\n\n\t```python\n\tif __name__ == '__main__':\n\t\t# Parse arguments\n\t\tparseArguments()\n\t\t# Create the Kafka Avro consumer\n\t\tkafka_consumer = KafkaConsumer(KAFKA_BROKERS,SCRAM_USERNAME,SCRAM_PASSWORD,TOPIC_NAME,SCHEMA_REGISTRY_URL)\n\t\t# Prepare the consumer\n\t\tkafka_consumer.prepareConsumer()\n\t\t# Consume next Avro event\n\t\tkafka_consumer.pollNextEvent()\n\t\t# Close the Avro consumer\n\t\tkafka_consumer.close()\n\t```\n\nAs you can see, this python code depends on a **Kafka Avro Consumer** to consume messages. This is explained next.\n\n### Kafka Avro Consumer\n\nThis script, called [KcAvroConsumer.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab/kafka/KcAvroConsumer.py), will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method:\n\n1. Initialize and prepare the new Kafka consumer:\n\n\t```python\n\tclass KafkaConsumer:\n\n\t\tdef __init__(self, kafka_brokers = \"\", scram_username = \"\",scram_password = \"\", topic_name = \"\", schema_registry_url = \"\", autocommit = True):\n\t\t\tself.kafka_brokers = kafka_brokers\n\t\t\tself.scram_username = scram_username\n\t\t\tself.scram_password = scram_password\n\t\t\tself.topic_name = topic_name\n\t\t\tself.schema_registry_url = schema_registry_url\n\t\t\tself.kafka_auto_commit = autocommit\n\n\t\t# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n\t\tdef prepareConsumer(self, groupID = \"pythonconsumers\"):\n\t\t\toptions ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'group.id': groupID,\n                'auto.offset.reset': 'earliest',\n                'schema.registry.url': self.schema_registry_url,\n                'enable.auto.commit': self.kafka_auto_commit,\n                'security.protocol': 'SASL_SSL',\n                'sasl.mechanisms': 'SCRAM-SHA-512',\n                'sasl.username': self.scram_username,\n                'sasl.password': self.scram_password,\n                'ssl.ca.location': os.environ['PEM_CERT'],\n                'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n\t\t\t}\n\t\t\t# Print the configuration\n\t\t\tprint(\"--- This is the configuration for the Avro consumer: ---\")\n        \tprint(options)\n        \tprint(\"---------------------------------------------------\")\n\t\t\t# Create the Avro consumer\n\t\t\tself.consumer = AvroConsumer(options)\n\t\t\t# Subscribe to the topic\n\t\t\tself.consumer.subscribe([self.topic_name])\n\t```\n\n1. Poll next event method:\n\n\t```python\n\t# Prints out the message\n\tdef traceResponse(self, msg):\n        print('[Message] - Next message consumed from {} partition: [{}] at offset {} with key {} and value {}'\n                    .format(msg.topic(), msg.partition(), msg.offset(), msg.key(), msg.value() ))\n\n\t# Polls for next event\n\tdef pollNextEvent(self):\n\t\t# Poll for messages\n\t\tmsg = self.consumer.poll(timeout=10.0)\n\t\t# Validate the returned message\n\t\tif msg is None:\n\t\t\tprint(\"[INFO] - No new messages on the topic\")\n\t\telif msg.error():\n\t\t\tif (\"PARTITION_EOF\" in msg.error()):\n\t\t\t\tprint(\"[INFO] - End of partition\")\n\t\t\telse:\n\t\t\t\tprint(\"[ERROR] - Consumer error: {}\".format(msg.error()))\n\t\telse:\n\t\t\t# Print the message\n\t\t\tmsgStr = self.traceResponse(msg)\n\t```\n\n## Schemas and Messages\n\nIn this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python application presented above in the [Python Avro Producer](#python-avro-producer) and [Python Avro Consumer](#python-avro-consumer) sections.\n\n**IMPORTANT:** Before start using our Python application we must set the **PYTHONPATH** environment variable to point to where we have all the Python scripts that make up our application in order for Python to find these at execution time.\n\n1. Set the PYTHONPATH variable to the location where you cloned the GitHub repository containing the Python application we are going to be working with\n\n```shell\nexport PYTHONPATH=~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10\n```\n\n### Produce a message\n\nIn order to produce a message, we execute the `ProduceAvroMessage.py`. This script, as you could see in the [Python Avro Producer](#python-avro-producer) section, is sending the event with key `{'key': '1'}` and value `{'message': 'This is a test message'}` according to the schemas defined in [default_key.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cp4i-schema-lab-v10/avro_files/default_key.avsc) and [default_value.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cp4i-schema-lab-v10/avro_files/default_value.avsc) for the key and value of the event respectively.\n\n<InlineNotification kind=\"warning\">\n\nMake sure you are on the right path where the python scripts live: **~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10/src**\n\n</InlineNotification>\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema-user1 [0]\n```\n\nWe can see our new message delivered in the `test-schema-user1` topic by\n\n1. Go into the topics page in the IBM Event Streams UI\n\n\t![14](./images/14-v10.png)\n\n1. Click on the topic and then on the _Messages_ tab at the top. Finally, click on a message to see it displayed on the right hand side of the screen\n\n\t![15](./images/15-v10.png)\n\n<InlineNotification kind=\"info\">\n\n**INFO:** Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas.\n\n</InlineNotification>\n\n\n**IMPORTANT:** As you can see, we got the `test-schema-user1` topic auto-created when we produced the message. The reason for this is that\n\n1. Kafka is set out of the box to let applications to auto-create topics.\n1. We created the SCRAM credentials for our application to allow the application to create topics.\n\nOn a production-like environment, you don't want developers creating applications that auto-create topics in your IBM Event Streams instance without any control. For that, we would configure Kafka to forbid topic auto-creation (<https://kafka.apache.org/documentation/#auto.create.topics.enable>) as well as thoroughly created the SCRAM credentials with the most strict but appropriate permissions that our application needs.\n\n**IMPORTANT:** Similar to the auto-creation of topics, we can see below that our application got the Avro data schemas for both the key and value of the message produced auto-registered. This is because many client libraries come with a SerDes property to allow them to auto-register the Avro data schemas (<https://docs.confluent.io/current/clients/confluent-kafka-python/#avroserializer>). However, on a production-like environment we don't want applications to auto-register schemas without any control but yet we can not leave it to the developers to set the auto-registration property off on their libraries. Instead, we would create the SCRAM credentials with the most strict but appropriate permissions that our application needs.\n\nIf we look now at the schemas our schema registry has:\n\n```shell\ncloudctl es schemas\n\nSchema                           State    Latest version   Latest version ID   Updated\ndemoSchema_CLI_USER1             active   2.0.0            2                   Fri, 24 Jul 2020 14:09:37 UTC\ndemoSchema_UI_USER1              active   2.0.0            2                   Fri, 24 Jul 2020 14:06:27 UTC\ntest-schema-user1-key-d89uk      active   1                1                   Fri, 24 Jul 2020 15:41:46 UTC\ntest-schema-user1-value-tv5efr   active   1                1                   Fri, 24 Jul 2020 15:41:45 UTC\nOK\n```\n\nwe see two schemas, `test-schema-user1-key-d89uk` and `test-schema-user1-value-tv5efr`, which in fact correspond to the Avro data schema used for the `key` ([default_key.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cloud-schema-lab/avro_files/default_key.avsc)) and the `value` ([default_value.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cloud-schema-lab/avro_files/default_value.avsc)) of events sent to the `test-schema-user1` topic in the [ProduceAvroMessage.py](https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cloud-schema-lab-v10/src/ProduceAvroMessage.py) as explained before sending the message.\n\nTo make sure of what we are saying, we can inspect those schemas:\n\n```shell\ncloudctl es schema test-schema-user1-key-d89uk --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultKey\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"int\",\n      \"name\": \"key\",\n      \"doc\": \"We expect any int as the event key\"\n    }\n  ],\n  \"doc\": \"Default Message's key Avro data schema\"\n}\n```\n\n```shell\ncloudctl es schema test-schema-user1-value-tv5efr --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nIf I now decided that my events should contain another attribute, I would modify the event value schema ([default_value.avsc](https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/master/labs/es-cloud-schema-lab/avro_files/default_value.avsc)) to reflect that as well as `ProduceAvroMessage.py` to send that new attribute in the event it sends:\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\", \"anotherAttribute\": \"Just another test string\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema-user1 [0]\n```\n\nI can see that an event with a new attribute has been sent:\n\n ![16](./images/16-v10.png)\n\nAnd I can also see that the new shcema has got registered as well:\n\n```shell\ncloudctl es schemas\n\nSchema                           State    Latest version   Latest version ID   Updated\ndemoSchema_CLI_USER1             active   2.0.0            2                   Fri, 24 Jul 2020 14:09:37 UTC\ndemoSchema_UI_USER1              active   2.0.0            2                   Fri, 24 Jul 2020 14:06:27 UTC\ntest-schema-user1-key-d89uk      active   1                1                   Fri, 24 Jul 2020 15:41:46 UTC\ntest-schema-user1-value-a5bbaa   active   1                1                   Fri, 24 Jul 2020 15:54:37 UTC\ntest-schema-user1-value-tv5efr   active   1                1                   Fri, 24 Jul 2020 15:41:45 UTC\nOK\n```\n\nIf I inspect that new schema, I see my new attribute in it:\n\n```shell\ncloudctl es schema test-schema-user1-value-a5bbaa --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any other string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\n<InlineNotification kind=\"info\">\n\nThe schema evolution above (test-schema-user1-value-a5bbaa) should have got registered as a new version of the already existing schema (test-schema-user1-value-tv5efr). IBM Event Streams allows schemas to auto-register themselves when these are sent along with a message from a producer application. However, the Schema Registry does not pick \"new\" schemas up as a new version of a previous schema and simply creates a new schema. Anyway, when reading messages off the topic, Schema Registry handles well what schema to return back to the receiver application so messages can get properly deserialized. Will see that in the next section.\n\n</InlineNotification>\n\n<InlineNotification kind=\"error\">\n\n**SECURITY:** As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is **NOT** a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the [Security](#security) section how we can control schema registration and evolution based on roles at the schema level also.\n\n</InlineNotification>\n\n### Create a non-compliant message\n\n\nLet's see what happens if we send a message that does not comply with its Avro data schema. Let's say that I send the following message:\n\n```shell\nkey = {\"key\": 1}\nvalue = {\"message\" : 12345}\n```\n\nand this is the output of that attempt:\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": 12345}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nTraceback (most recent call last):\n  File \"ProduceAvroMessage.py\", line 81, in <module>\n    kafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n  File \"/tmp/lab/kafka/KcAvroProducer.py\", line 43, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 99, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 118, in encode_record_with_schema\n    return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 152, in encode_record_with_schema_id\n    writer(record, outf)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 86, in <lambda>\n    return lambda record, fp: writer.write(record, avro.io.BinaryEncoder(fp))\n  File \"/root/.local/lib/python3.7/site-packages/avro/io.py\", line 771, in write\n    raise AvroTypeException(self.writer_schema, datum)\navro.io.AvroTypeException: The datum {'message': 12345} is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any other string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n```\n\nAs we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply (the message value attribute we are sending is an integer rather than a string and we are missing the second attribute).\n\nTherefore, using Avro schemas with IBM Event Streams give us the ability to build our system with **robustness** protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.\n\n### Consume a message\n\nIn order to consume a message, we execute the `ConsumeAvroMessage.py` within the `/tmp/lab/src` folder in our python demo environment:\n\n<InlineNotification kind=\"warning\">\n\nChange user1\n\n</InlineNotification>\n\n```shell\npython3 ConsumeAvroMessage.py test-schema-user1\n\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema-user1']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema-user1 partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n\npython3 ConsumeAvroMessage.py test-schema-user1\n\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema-user1']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema-user1 partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\nAs you can see, our script was able to read the Avro messages from the `test-schema-user1` topic and map that back to their original structure thanks to the Avro schemas:\n\n```shell\n[Message] - Next message consumed from test-schema partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n[Message] - Next message consumed from test-schema partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n```\n\n## Data Evolution\n\nSo far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich CLI IBM Event Streams provides to interact with.\n\nHowever, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying [Event Storming](/methodology/event-storming/) or [Domain Driven Design](/methodology/domain-driven-design/) for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.\n\nBut it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to [hundreds of years](https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/)) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the **compatibility** of old and new data schemas and, in fact, old and new data at the end of the day.\n\n**The IBM Event Streams Schema Registry enforces full compatibility when creating a new version of a schema**. Full compatibility means that **old data can be read with the new data schema, and new data can also be read with the last data schema**.\n\nIn data formats like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. Let's see then how this affects when you want your data to evolve in a way that it needs to add a new attribute or delete an existing attribute.\n\nBut let's see what that means in terms of adding and removing attributes from your data schema.\n\n### Adding a new attribute\n\nAlthough we have already seen this in the adding a new version of a schema section, let's try to add a new version of our `test-schema-value` schema where we have a new attribute. Remember, our `default_schema.avsc` already contains a new attribute than the original one but that it got registered as a new schema rather than as a new version of the original one. Let's reuse that Avro schema file to register it as a new version (**INFO:** you might need to copy/download that file to your local workstation in order to be able to then upload it to the IBM Event Streams through its UI)\n\nWhen doing so from the UI, we see the following error:\n\n  ![17](./images/17-v10.png)\n\nThe reason, as alread explained in the add a new version of a schema section, is because full compatibility dictates that you can only add new attributes to a schema if these have a default value. Reason being that a receiver should be able to deserialize messages produced with an older schema using the newer schema. Because old messages were written with an older schema that did not contain our new attribute, those messages won't have that attribute so we need to provide a default value for it in our never version of the schema so that the receiver is able to deserialize those older messages with the newer schema.\n\nIf we add the default value for the new attribute, we see that our newer version is now compatible:\n\n  ![18](./images/18-v10.png)\n\nand that it gets registered fine:\n\n  ![19](./images/19-v10.png)\n\n### Removing an existing attribute\n\nWhat if we now wanted to remove the original `message` attribute from our schema. Let's remove it from the `default_value.avsc` file and try to register that new version:\n\n  ![20](./images/20-v10.png)\n\nWe, again, get the same error. And the reason is because receivers must be able to read and deserialize messages produced with the newer schema (that is, without the `message` attribute) but with the older schema (that is, with the schema version that enforces the existence of the `message` attribute).\n\nIn order to work this around, what we need to do is to register first an intermediate schema that defines a default value for the `message` attribute:\n\n  ![21](./images/21-v10.png)\n\nOnce we have a default value for the `message` attribute, we can register a new version of the schema that finally removes that attribute:\n\n  ![22](./images/22-v10.png)\n\n## Security\n\nAs we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams), etc since we don't want everyone and everything to be, for instance, creating or deleting topics, schemas, etc.\n\nYou can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in Access Control List (ACL) rules:\n\n* **Topics (topic):** you can control the ability of users and applications to create, delete, read, and write to a topic.\n* **Consumer groups (group):** you can control an application’s ability to join a consumer group.\n* **Transactional IDs (transactionalId):** you can control the ability to use the transaction capability in Kafka.\n\nNote: Schemas in the Event Streams Schema Registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas.\n\nYou can find more information about how to secure your IBM Event Streams resources in the official documentation at: <https://ibm.github.io/event-streams/security/managing-access/>\n","frontmatter":{"title":"IBM Event Streams Schema Registry from IBM CloudPak for Integration","description":"Hands on lab to understand IBM Event Streams from the IBM CloudPak for Integration Schema Registry feature"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/schema-registry-on-ocp/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}