{"componentChunkName":"component---src-pages-use-cases-monitoring-on-ocp-index-mdx","path":"/use-cases/monitoring-on-ocp/","result":{"pageContext":{"frontmatter":{"title":"Monitoring IBM Event Streams on OpenShift Cloud Platform","description":"Monitoring Kafka performance metrics and activity when deployed via the IBM Cloud Pak for Integration on Red Hat OpenShift Container Platform."},"relativePagePath":"/use-cases/monitoring-on-ocp/index.mdx","titleType":"append","MdxNode":{"id":"08e0e581-8c6b-5aad-87ad-c75880a3d91a","children":[],"parent":"ab2e98ee-5f71-59b8-8d95-d276cbbeb723","internal":{"content":"---\ntitle: Monitoring IBM Event Streams on OpenShift Cloud Platform\ndescription: Monitoring Kafka performance metrics and activity when deployed via the IBM Cloud Pak for Integration on Red Hat OpenShift Container Platform.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Scenario Prereqs</AnchorLink>\n  <AnchorLink>Generate Event Load</AnchorLink>\n  <AnchorLink>Explore the preconfigured Event Streams Dashboard</AnchorLink>\n  <AnchorLink>Import Grafana Dashboards</AnchorLink>\n  <AnchorLink>View Grafana Dashboards</AnchorLink>\n  <AnchorLink>Create an Alert</AnchorLink>\n  <AnchorLink>External Monitoring Tools</AnchorLink>\n  <AnchorLink>Advanced Scenarios</AnchorLink>\n  <AnchorLink>Additional Reading</AnchorLink>\n</AnchorLinks>\n\n<!--\n  <AnchorLink>Import Kibana Dashboards</AnchorLink>\n  <AnchorLink>View Kibana Dashboards</AnchorLink>\n-->\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong> - needs screenshots</InlineNotification>\n\n<InlineNotification kind=\"info\">This tutorial was developed with and validated against IBM Cloud Pak for Integration Version 2020.2.1 and IBM Event Streams Version 10.0.0. Any deviation from those versions while performing the tasks in this tutorial may produced unexpected results.</InlineNotification>\n\n## Overview\n\nDeploying IBM Event Streams on OpenShift Cloud Platform (OCP) as the Apache Kafka-based event backbone is a great first step in your Event-Driven Architecture implementation. However, now you must maintain that Kafka cluster and understand the intricate details of what a _\"healthy\"_ cluster looks like. This tutorial will walk you through some of the initial monitoring scenarios that are available for IBM Event Streams deployed on OCP.\n\nThe raw monitoring use cases and capabilities are available from the official IBM Event Streams documentation via the links below:\n- [Monitoring deployment health](https://ibm.github.io/event-streams/administering/deployment-health/)\n- [Monitoring Kafka cluster health](https://ibm.github.io/event-streams/administering/cluster-health/)\n- [Monitoring topic health](https://ibm.github.io/event-streams/administering/topic-health/)\n- [Monitoring Kafka consumer group lag](https://ibm.github.io/event-streams/administering/consumer-lag/)\n\nThis tutorial will focus on a more guided approach to understanding the foundation of Apache Kafka monitoring capabilities provided by IBM Event Streams and the IBM Cloud Pak for Integration. Upon completion of this tutorial, you can extend your own experience through the [Advanced Scenarios](#advanced-scenarios) section to adapt Kafka monitoring capabilites to your project's needs.\n\n## Scenario Prereqs\n\n**OpenShift Container Platform**\n\n- This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of `4.4`.\n\n**Cloud Pak for Integration**\n\n- This deployment scenario was developed for use with the 2020.2.x release of the [IBM Cloud Pak for Integration](https://www.ibm.com/support/knowledgecenter/en/SSGT7J_20.2/welcome.html), installed on OpenShift 4.4.\n\n**IBM Event Streams**\n\n- This deployment scenario requires a working installation of [IBM Event Streams V10.0](https://ibm.github.io/event-streams/) or greater, deployed on the Cloud Pak for Integration environment mentioned above.\n- For Cloud Pak installation guidance, you can follow the [Cloud Pak Playbook](https://cloudpak8s.io/integration/cp4i-deploy-eventstreams/) installation instructions.\n\n**Git**\n\n- We will need to clone repositories.\n\n**Java**\n\n- Java Development Kit (JDK) v1.8+ (Java 8+)\n\n**Maven**\n\n- The scenario uses Maven v3.6.x\n\n## Generate Event Load\n\nThis section details walking through the generation of a starter application for usage with IBM Event Streams, as documented in the [official product documentation](https://ibm.github.io/event-streams/getting-started/generating-starter-app/).\n\n- Access the Event Streams Dashboard via `https://es-1-ibm-es-ui-integration.apps.[cluster-name]` and login.\n- Click the **Try the starter application** button from the _Getting Started_ page\n- Click **Download JAR from GitHub**. This will open a new window to `https://github.com/ibm-messaging/kafka-java-vertx-starter/releases`\n  - Click the link for `demo-all.jar` from the latest release available. At the time of this writing, the latest version was `1.0.0`.\n\n- Return to the Event Streams console and click **Generate properties**.\n- In dialog that pops up from the right-hand side of the screen, enter the following information:\n  - **Starter application name:** `monitoring-lab-[your-initials]`\n  - Leave **New topic** selected and enter a **Topic name** of `monitoring-lab-topic-[your-initials]`.\n  - Click **Generate and download .zip**\n\n- In a Terminal window, unzip the generated ZIP file from the previous window to the same directory with the `demo-all.jar` file.\n- Review the extracted `kafka.properties` to understand how Event Streams has generated credentials and configuration information for this sample application to connect.\n- Run the command `java -Dproperties_path=./kafka.properties -jar demo-all.jar`.\n\n- Wait until you see the string `Application started in X ms` in the output and then visit the application's user interface via `http://localhost:8080`.\n- Once in the User Interface, enter a message to be contained for the Kafka record value then click **Start producing**.\n- Wait a few moments until the UI updates to show some of the confirmed produced messages and offsets, then click on **Start consuming** on the right side of the application.\n- You can let the application continue running while you continue with the rest of this lab.\n  - If you would like to stop the application from producing, you can click **Stop producing**.\n  - If you would like to stop the application from consuming, you can click **Stop consuming**.\n  - If you would like to stop the application entirely, you can input `Control+C` in the Terminal session where the application is running.\n\nAn [alternative sample application](https://ibm.github.io/event-streams/getting-started/testing-loads/) can be leveraged from the official documentation to generate higher amounts of load.\n\n## Explore the preconfigured Event Streams Dashboard\n\nThis section will walk through the default dashboard and user interface available on every IBM Event Streams deployment.\n\n- Access the Event Streams Dashboard via `https://es-1-ibm-es-ui-integration.apps.[cluster-name]` and login.\n- Click the **Monitoring** tab from the primary navigation menu.\n- From here, you can view information on messages, partitions, and replicas for the past hour, day, week, or month.\n- Click the **Topics** tab from the primary navigation menu.\n- Click the name of your topic that you previously created in the [Generate Event Load](#generate-event-load) section. This should be in the format of `monitoring-lab-topic-[your-initials]`.\n- You are presented with a **Producers** page showing the number of active producers, as well as the average message size produced per second and average number of messages produced per second. You can modify the time window by changing the values in the _View producers by time_ box.\n- Click the **Messages** tab to view all the data and metadata for events stored in the topic.\n- You can view messages across partitions or on specific partitions, as well as jump to specific offsets or timestamps.\n- Click **Consumer Groups** to be shown the number of consumer groups that have previously registered or are currently registered as consuming from the topic.\n- You are able to see how many active members a consumer group has, as well as have many unconsumed partitions a topic has inside of a consumer group (also known as _consumer group lag_)- a key metric for driving parallelism in event-driven microservices!\n\n## Import Grafana Dashboards\n\nThis section will walk through the Grafana Dashboard capabilities documented in the [official IBM Event Streams documentation](https://ibm.github.io/event-streams/administering/cluster-health/#grafana).\n\n1. Apply the Grafana Dashboard for overall Kafka Health via a `MonitoringDashboard` custom resource:\n\n```bash\noc apply -f https://raw.githubusercontent.com/ibm-messaging/event-streams-operator-resources/master/grafana-dashboards/ibm-eventstreams-kafka-health-dashboard.yaml\n```\n\n## View Grafana Dashboards\n\nTo view the newly imported Event Streams Grafana dashboard for overall Kafka Health, follow these steps:\n\n- Navigate to the IBM Cloud Platform Common Services console homepage via `https://cp-console.apps.[cluster-name]`\n- Click the hamburger icon in the top left.\n- Expand **Monitor Health**.\n- Click the **Monitoring** in the expanded menu to open the Grafana homepage.\n- Click the user icon in the bottom left corner to open the user profile page.\n- In the **Organizations** table, find the namespace where you installed the Event Streams `monitoringdashboard` custom resource, and switch the user profile to that namespace.\n- Hover over the _Dashboards_ square on the left and click **Manage**.\n- Click on **IBM Event Streams Kafka** dashboard in the Dashboard table to view the newly imported resource.\n- Using the drop-down selectors at the top, select the following:\n  - **Namespace** which has the running instance of your Event Streams deployment,\n  - **Cluster Name** for the desired Event Streams cluster\n  - **Topic** that matches desired topics for viewing _(only topics that have been published to will appear in this list)_\n  - **Broker** to select individual or multiple brokers in the cluster.\n\n**Note:** Not all of the metrics that Kafka uses are published to Prometheus by default. The metrics that are published are controlled by a ConfigMap. You can publish metrics by adding them to the ConfigMap.\n\n<!--\n## Import Kibana Dashboards\n\n<InlineNotification kind=\"info\"><strong>TODO PREREQ</strong> - https://docs.openshift.com/container-platform/4.4/logging/cluster-logging-deploying.html</InlineNotification>\n\n- https://ibm.github.io/event-streams/administering/cluster-health/#kibana\n\n## View Kibana Dashboards\n\nTBD\n-->\n\n## Create an Alert\n\nA monitoring system is only as good as the alerts it can send out, since you're not going to be watching that Grafana dashboard all day and night! This section will walk through the creation of a quick alert rule which will automatically trigger, as well as how to view and silence that alert in the provided Alertmanager interface.\n\nThe official [Event Streams documentation](https://ibm.github.io/event-streams/tutorials/monitoring-alerts/#selecting-the-metric-to-monitor) provides a walkthrough of selecting the desired metrics to monitor, but for our example, we will leverage the `kafka_server_replicamanager_partitioncount_value` metric as an indicator of topic creation _(as the overall partition count will increase when a topic is first created)_.\n\n- On the command line, create this sample rule which will fire whenever the partition count is over 50 _(which is the baseline number of partitions the Event Streams system uses for its internal topic partitions)_.\n- Save the file below as `prom-rule-partitions.yaml`:\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n    labels:\n        component: icp-prometheus\n    name: demo-partition-count\nspec:\n    groups:\n      - name: PartitionCount\n        rules:\n          - alert: PartitionCount\n            expr: kafka_server_replicamanager_partitioncount_value > 50\n            for: 10s\n            labels:\n              severity: critical\n            annotations:\n              identifier: 'Partition count'\n              description: 'There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}'\n```\n- Create the alert rule via the OpenShift CLI:\n```bash\noc apply -f prom-rule-partitions.yaml\n```\n- You can view the creation and status of your alert via the OpenShift CLI:\n```bash\noc get PrometheusRule demo-partition-count\noc describe PrometheusRule demo-partition-count\n```\n- Access the Prometheus monitoring backend that is provided in the IBM Cloud Pak Common Services via `https://cp-console.apps.[cluster-name]/prometheus`.\n- Click the **Alerts** button in the header.\n- You should see your new **PartitionCount** rule firing and highlighted in red.\n  - _**NOTE:**_ If you do not see your PrometheusRule, you may need to create it in the `ibm-common-services` namespace depending upon your OpenShift cluster and Cloud Pak operator configuration. This can be done by supplying the `-n ibm-common-services` flag to the `oc apply -f prom-rule-partitions.yaml` command.\n- Click on the **PartitionCount** alert to expand the details and see which components are triggering the alert.\n\nNow that we have created alerts from the monitoring system, you will want a way to manage those alerts. The default _Alertmanager_ component provides a way to manage firing alerts, notifications, and silences. Prometheus is capable of integrating with many notification systems - from Slack to PagerDuty to HipChat to common HTTP webhooks. For further information on the extensibility of Prometheus, you can view the [Alerting configuration section](https://prometheus.io/docs/alerting/latest/configuration/) of the official docs. For configuring the IBM Cloud Pak Common Services deployed instance of Prometheus, you can view the [Configuring Alertmanager section](https://www.ibm.com/support/knowledgecenter/SSHKN6/monitoring/1.x.x/monitoring_service.html#configuring-alertmanager) of the official docs.\n\nIn this section of the tutorial, you will walk through the Alertmanager interface and silence the previously created alerts.\n\n- Access the default Alertmanager instance via `https://cp-console.apps.[cluster-name]/alertmanager/`.\n- You should see the newly created `PartitionCount` alerts listed as firing.\n- Click on the **Info** button for the first alert to see the additional context provided by the alert definition _(ie there are more than 50 partitions)_\n- As alerts fire and become acknowledged, you can silence them to mark them as known, acknowledged, or resolved. To do this, you create a Silence. Click the **Silence** button for one of the alerts in the list.\n- You will see a start time, a duration, and an end time by default. This gives you initial control over what you are silencing and for how long.\n- Next, you will see a list of _Name_ and _Value_ pairs that are filled with the information from the alert instance you clicked on.\n- Delete the elements in the **Matchers** list until only the following items are left. This will allow for a robust capture of all the _PartitionCount_ alerts for the same Event Streams cluster.\n  - `alertname`\n  - `app_kubernetes_io_instance`\n  - `app_kubernetes_io_part_of`\n  - `kubernetes_namespace`\n- Your username should already be filled in for the **Creator**, so enter a **Comment** of _\"Silencing demo alerts\" and click **Preview Alerts**.\n- Once the affected number of alerts matches the same number of _PartitionCount_ alerts that were listed as firing in Prometheus, click **Create**.\n- Clicking on the **Alerts** tab in the header, you will now see those alerts are silenced - meaning you acknowledged them.\n- To make them visible again prior to the expiration of the created Silence, click on the **Silences** tab from the header.\n- This page lists all the Active, Pending, and Expired silences in the system. You can view, edit, and expire any active Silence to again have the alerts show up in Alertmanager or anywhere else Prometheus is sending notifications.\n\n## Next Steps\n\n### External Monitoring Tools\n\nIBM Event Streams supports additional monitoring capabilities with third-party monitoring tools via a connection to the clusters JMX port on the Kafka brokers.\n\nYou must first [configure](https://ibm.github.io/event-streams/installing/configuring/#configuring-external-monitoring-through-jmx) your IBM Event Streams instance for specific access by these external monitoring tools.\n\nYou can then follow along with the [tutorials](https://ibm.github.io/event-streams/tutorials/) defined in the official IBM Event Streams documentation to monitor Event Streams with tools such as Datadog and Splunk.\n\n### Advanced Scenarios\n\nAs shown in this tutorial, IBM Event Streams provides a robust default set of monitoring metrics which are available to use right out of the box. However, you will most likely need to define custom metrics or extend existing metrics for use in custom dashboards or reporting processes. The following links _(in order of recommended usage)_ discuss additional monitoring capabilities, technologies, and endpoints that are supported with IBM Event Streams to extend your custom monitoring solution as needed:\n\n- [**Kafka Exporter**](https://ibm.github.io/event-streams/installing/configuring/#configuring-the-kafka-exporter) - You can use Event Streams to export metrics to Prometheus. These metrics are otherwise only accessible through the Kafka command line tools and allow per-topic metrics, such as consumer group lag, to be colleced.\n\n- [**JMX Exporter**](https://ibm.github.io/event-streams/installing/configuring#configuring-the-jmx-exporter) - You can use Event Streams to collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus via the [Prometheus JMX Exporter](https://github.com/prometheus/jmx_exporter).\n\n- [**JmxTrans**](https://ibm.github.io/event-streams/security/secure-jmx-connections/#configuring-a-jmxtrans-deployment) - JmxTrans can be used to push JMX metrics from Kafka brokers to external applications or databases.\n\n### Additional Reading\n\n- [Monitoring Kafka](https://kafka.apache.org/documentation/#monitoring) via official Apache Kafka documentation\n- [Monitoring Kafka performance metrics](https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/) via **Datadog**\n- [How to Monitor Kafka](https://blog.serverdensity.com/how-to-monitor-kafka/) via **Server Density**\n- [OpenShift Day 2 Monitoring](https://cloudpak8s.io/day2/Monitoring/) via **IBM Cloud Paks Playbook**\n- [Monitoring Kafka cluster health](https://ibm.github.io/event-streams/administering/cluster-health/) via **IBM Event Streams documentation**\n- [Configuring the monitoring stack](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html) via **Red Hat OpenShift** documentation\n- [Examining cluster metrics](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/examining-cluster-metrics.html) via **Red Hat OpenShift** documentation\n","type":"Mdx","contentDigest":"1c133f28d9a73dc15cc9fba1a15beca2","counter":642,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Monitoring IBM Event Streams on OpenShift Cloud Platform\ndescription: Monitoring Kafka performance metrics and activity when deployed via the IBM Cloud Pak for Integration on Red Hat OpenShift Container Platform.\n---\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Scenario Prereqs</AnchorLink>\n  <AnchorLink>Generate Event Load</AnchorLink>\n  <AnchorLink>Explore the preconfigured Event Streams Dashboard</AnchorLink>\n  <AnchorLink>Import Grafana Dashboards</AnchorLink>\n  <AnchorLink>View Grafana Dashboards</AnchorLink>\n  <AnchorLink>Create an Alert</AnchorLink>\n  <AnchorLink>External Monitoring Tools</AnchorLink>\n  <AnchorLink>Advanced Scenarios</AnchorLink>\n  <AnchorLink>Additional Reading</AnchorLink>\n</AnchorLinks>\n\n<!--\n  <AnchorLink>Import Kibana Dashboards</AnchorLink>\n  <AnchorLink>View Kibana Dashboards</AnchorLink>\n-->\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong> - needs screenshots</InlineNotification>\n\n<InlineNotification kind=\"info\">This tutorial was developed with and validated against IBM Cloud Pak for Integration Version 2020.2.1 and IBM Event Streams Version 10.0.0. Any deviation from those versions while performing the tasks in this tutorial may produced unexpected results.</InlineNotification>\n\n## Overview\n\nDeploying IBM Event Streams on OpenShift Cloud Platform (OCP) as the Apache Kafka-based event backbone is a great first step in your Event-Driven Architecture implementation. However, now you must maintain that Kafka cluster and understand the intricate details of what a _\"healthy\"_ cluster looks like. This tutorial will walk you through some of the initial monitoring scenarios that are available for IBM Event Streams deployed on OCP.\n\nThe raw monitoring use cases and capabilities are available from the official IBM Event Streams documentation via the links below:\n- [Monitoring deployment health](https://ibm.github.io/event-streams/administering/deployment-health/)\n- [Monitoring Kafka cluster health](https://ibm.github.io/event-streams/administering/cluster-health/)\n- [Monitoring topic health](https://ibm.github.io/event-streams/administering/topic-health/)\n- [Monitoring Kafka consumer group lag](https://ibm.github.io/event-streams/administering/consumer-lag/)\n\nThis tutorial will focus on a more guided approach to understanding the foundation of Apache Kafka monitoring capabilities provided by IBM Event Streams and the IBM Cloud Pak for Integration. Upon completion of this tutorial, you can extend your own experience through the [Advanced Scenarios](#advanced-scenarios) section to adapt Kafka monitoring capabilites to your project's needs.\n\n## Scenario Prereqs\n\n**OpenShift Container Platform**\n\n- This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of `4.4`.\n\n**Cloud Pak for Integration**\n\n- This deployment scenario was developed for use with the 2020.2.x release of the [IBM Cloud Pak for Integration](https://www.ibm.com/support/knowledgecenter/en/SSGT7J_20.2/welcome.html), installed on OpenShift 4.4.\n\n**IBM Event Streams**\n\n- This deployment scenario requires a working installation of [IBM Event Streams V10.0](https://ibm.github.io/event-streams/) or greater, deployed on the Cloud Pak for Integration environment mentioned above.\n- For Cloud Pak installation guidance, you can follow the [Cloud Pak Playbook](https://cloudpak8s.io/integration/cp4i-deploy-eventstreams/) installation instructions.\n\n**Git**\n\n- We will need to clone repositories.\n\n**Java**\n\n- Java Development Kit (JDK) v1.8+ (Java 8+)\n\n**Maven**\n\n- The scenario uses Maven v3.6.x\n\n## Generate Event Load\n\nThis section details walking through the generation of a starter application for usage with IBM Event Streams, as documented in the [official product documentation](https://ibm.github.io/event-streams/getting-started/generating-starter-app/).\n\n- Access the Event Streams Dashboard via `https://es-1-ibm-es-ui-integration.apps.[cluster-name]` and login.\n- Click the **Try the starter application** button from the _Getting Started_ page\n- Click **Download JAR from GitHub**. This will open a new window to `https://github.com/ibm-messaging/kafka-java-vertx-starter/releases`\n  - Click the link for `demo-all.jar` from the latest release available. At the time of this writing, the latest version was `1.0.0`.\n\n- Return to the Event Streams console and click **Generate properties**.\n- In dialog that pops up from the right-hand side of the screen, enter the following information:\n  - **Starter application name:** `monitoring-lab-[your-initials]`\n  - Leave **New topic** selected and enter a **Topic name** of `monitoring-lab-topic-[your-initials]`.\n  - Click **Generate and download .zip**\n\n- In a Terminal window, unzip the generated ZIP file from the previous window to the same directory with the `demo-all.jar` file.\n- Review the extracted `kafka.properties` to understand how Event Streams has generated credentials and configuration information for this sample application to connect.\n- Run the command `java -Dproperties_path=./kafka.properties -jar demo-all.jar`.\n\n- Wait until you see the string `Application started in X ms` in the output and then visit the application's user interface via `http://localhost:8080`.\n- Once in the User Interface, enter a message to be contained for the Kafka record value then click **Start producing**.\n- Wait a few moments until the UI updates to show some of the confirmed produced messages and offsets, then click on **Start consuming** on the right side of the application.\n- You can let the application continue running while you continue with the rest of this lab.\n  - If you would like to stop the application from producing, you can click **Stop producing**.\n  - If you would like to stop the application from consuming, you can click **Stop consuming**.\n  - If you would like to stop the application entirely, you can input `Control+C` in the Terminal session where the application is running.\n\nAn [alternative sample application](https://ibm.github.io/event-streams/getting-started/testing-loads/) can be leveraged from the official documentation to generate higher amounts of load.\n\n## Explore the preconfigured Event Streams Dashboard\n\nThis section will walk through the default dashboard and user interface available on every IBM Event Streams deployment.\n\n- Access the Event Streams Dashboard via `https://es-1-ibm-es-ui-integration.apps.[cluster-name]` and login.\n- Click the **Monitoring** tab from the primary navigation menu.\n- From here, you can view information on messages, partitions, and replicas for the past hour, day, week, or month.\n- Click the **Topics** tab from the primary navigation menu.\n- Click the name of your topic that you previously created in the [Generate Event Load](#generate-event-load) section. This should be in the format of `monitoring-lab-topic-[your-initials]`.\n- You are presented with a **Producers** page showing the number of active producers, as well as the average message size produced per second and average number of messages produced per second. You can modify the time window by changing the values in the _View producers by time_ box.\n- Click the **Messages** tab to view all the data and metadata for events stored in the topic.\n- You can view messages across partitions or on specific partitions, as well as jump to specific offsets or timestamps.\n- Click **Consumer Groups** to be shown the number of consumer groups that have previously registered or are currently registered as consuming from the topic.\n- You are able to see how many active members a consumer group has, as well as have many unconsumed partitions a topic has inside of a consumer group (also known as _consumer group lag_)- a key metric for driving parallelism in event-driven microservices!\n\n## Import Grafana Dashboards\n\nThis section will walk through the Grafana Dashboard capabilities documented in the [official IBM Event Streams documentation](https://ibm.github.io/event-streams/administering/cluster-health/#grafana).\n\n1. Apply the Grafana Dashboard for overall Kafka Health via a `MonitoringDashboard` custom resource:\n\n```bash\noc apply -f https://raw.githubusercontent.com/ibm-messaging/event-streams-operator-resources/master/grafana-dashboards/ibm-eventstreams-kafka-health-dashboard.yaml\n```\n\n## View Grafana Dashboards\n\nTo view the newly imported Event Streams Grafana dashboard for overall Kafka Health, follow these steps:\n\n- Navigate to the IBM Cloud Platform Common Services console homepage via `https://cp-console.apps.[cluster-name]`\n- Click the hamburger icon in the top left.\n- Expand **Monitor Health**.\n- Click the **Monitoring** in the expanded menu to open the Grafana homepage.\n- Click the user icon in the bottom left corner to open the user profile page.\n- In the **Organizations** table, find the namespace where you installed the Event Streams `monitoringdashboard` custom resource, and switch the user profile to that namespace.\n- Hover over the _Dashboards_ square on the left and click **Manage**.\n- Click on **IBM Event Streams Kafka** dashboard in the Dashboard table to view the newly imported resource.\n- Using the drop-down selectors at the top, select the following:\n  - **Namespace** which has the running instance of your Event Streams deployment,\n  - **Cluster Name** for the desired Event Streams cluster\n  - **Topic** that matches desired topics for viewing _(only topics that have been published to will appear in this list)_\n  - **Broker** to select individual or multiple brokers in the cluster.\n\n**Note:** Not all of the metrics that Kafka uses are published to Prometheus by default. The metrics that are published are controlled by a ConfigMap. You can publish metrics by adding them to the ConfigMap.\n\n<!--\n## Import Kibana Dashboards\n\n<InlineNotification kind=\"info\"><strong>TODO PREREQ</strong> - https://docs.openshift.com/container-platform/4.4/logging/cluster-logging-deploying.html</InlineNotification>\n\n- https://ibm.github.io/event-streams/administering/cluster-health/#kibana\n\n## View Kibana Dashboards\n\nTBD\n-->\n\n## Create an Alert\n\nA monitoring system is only as good as the alerts it can send out, since you're not going to be watching that Grafana dashboard all day and night! This section will walk through the creation of a quick alert rule which will automatically trigger, as well as how to view and silence that alert in the provided Alertmanager interface.\n\nThe official [Event Streams documentation](https://ibm.github.io/event-streams/tutorials/monitoring-alerts/#selecting-the-metric-to-monitor) provides a walkthrough of selecting the desired metrics to monitor, but for our example, we will leverage the `kafka_server_replicamanager_partitioncount_value` metric as an indicator of topic creation _(as the overall partition count will increase when a topic is first created)_.\n\n- On the command line, create this sample rule which will fire whenever the partition count is over 50 _(which is the baseline number of partitions the Event Streams system uses for its internal topic partitions)_.\n- Save the file below as `prom-rule-partitions.yaml`:\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n    labels:\n        component: icp-prometheus\n    name: demo-partition-count\nspec:\n    groups:\n      - name: PartitionCount\n        rules:\n          - alert: PartitionCount\n            expr: kafka_server_replicamanager_partitioncount_value > 50\n            for: 10s\n            labels:\n              severity: critical\n            annotations:\n              identifier: 'Partition count'\n              description: 'There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}'\n```\n- Create the alert rule via the OpenShift CLI:\n```bash\noc apply -f prom-rule-partitions.yaml\n```\n- You can view the creation and status of your alert via the OpenShift CLI:\n```bash\noc get PrometheusRule demo-partition-count\noc describe PrometheusRule demo-partition-count\n```\n- Access the Prometheus monitoring backend that is provided in the IBM Cloud Pak Common Services via `https://cp-console.apps.[cluster-name]/prometheus`.\n- Click the **Alerts** button in the header.\n- You should see your new **PartitionCount** rule firing and highlighted in red.\n  - _**NOTE:**_ If you do not see your PrometheusRule, you may need to create it in the `ibm-common-services` namespace depending upon your OpenShift cluster and Cloud Pak operator configuration. This can be done by supplying the `-n ibm-common-services` flag to the `oc apply -f prom-rule-partitions.yaml` command.\n- Click on the **PartitionCount** alert to expand the details and see which components are triggering the alert.\n\nNow that we have created alerts from the monitoring system, you will want a way to manage those alerts. The default _Alertmanager_ component provides a way to manage firing alerts, notifications, and silences. Prometheus is capable of integrating with many notification systems - from Slack to PagerDuty to HipChat to common HTTP webhooks. For further information on the extensibility of Prometheus, you can view the [Alerting configuration section](https://prometheus.io/docs/alerting/latest/configuration/) of the official docs. For configuring the IBM Cloud Pak Common Services deployed instance of Prometheus, you can view the [Configuring Alertmanager section](https://www.ibm.com/support/knowledgecenter/SSHKN6/monitoring/1.x.x/monitoring_service.html#configuring-alertmanager) of the official docs.\n\nIn this section of the tutorial, you will walk through the Alertmanager interface and silence the previously created alerts.\n\n- Access the default Alertmanager instance via `https://cp-console.apps.[cluster-name]/alertmanager/`.\n- You should see the newly created `PartitionCount` alerts listed as firing.\n- Click on the **Info** button for the first alert to see the additional context provided by the alert definition _(ie there are more than 50 partitions)_\n- As alerts fire and become acknowledged, you can silence them to mark them as known, acknowledged, or resolved. To do this, you create a Silence. Click the **Silence** button for one of the alerts in the list.\n- You will see a start time, a duration, and an end time by default. This gives you initial control over what you are silencing and for how long.\n- Next, you will see a list of _Name_ and _Value_ pairs that are filled with the information from the alert instance you clicked on.\n- Delete the elements in the **Matchers** list until only the following items are left. This will allow for a robust capture of all the _PartitionCount_ alerts for the same Event Streams cluster.\n  - `alertname`\n  - `app_kubernetes_io_instance`\n  - `app_kubernetes_io_part_of`\n  - `kubernetes_namespace`\n- Your username should already be filled in for the **Creator**, so enter a **Comment** of _\"Silencing demo alerts\" and click **Preview Alerts**.\n- Once the affected number of alerts matches the same number of _PartitionCount_ alerts that were listed as firing in Prometheus, click **Create**.\n- Clicking on the **Alerts** tab in the header, you will now see those alerts are silenced - meaning you acknowledged them.\n- To make them visible again prior to the expiration of the created Silence, click on the **Silences** tab from the header.\n- This page lists all the Active, Pending, and Expired silences in the system. You can view, edit, and expire any active Silence to again have the alerts show up in Alertmanager or anywhere else Prometheus is sending notifications.\n\n## Next Steps\n\n### External Monitoring Tools\n\nIBM Event Streams supports additional monitoring capabilities with third-party monitoring tools via a connection to the clusters JMX port on the Kafka brokers.\n\nYou must first [configure](https://ibm.github.io/event-streams/installing/configuring/#configuring-external-monitoring-through-jmx) your IBM Event Streams instance for specific access by these external monitoring tools.\n\nYou can then follow along with the [tutorials](https://ibm.github.io/event-streams/tutorials/) defined in the official IBM Event Streams documentation to monitor Event Streams with tools such as Datadog and Splunk.\n\n### Advanced Scenarios\n\nAs shown in this tutorial, IBM Event Streams provides a robust default set of monitoring metrics which are available to use right out of the box. However, you will most likely need to define custom metrics or extend existing metrics for use in custom dashboards or reporting processes. The following links _(in order of recommended usage)_ discuss additional monitoring capabilities, technologies, and endpoints that are supported with IBM Event Streams to extend your custom monitoring solution as needed:\n\n- [**Kafka Exporter**](https://ibm.github.io/event-streams/installing/configuring/#configuring-the-kafka-exporter) - You can use Event Streams to export metrics to Prometheus. These metrics are otherwise only accessible through the Kafka command line tools and allow per-topic metrics, such as consumer group lag, to be colleced.\n\n- [**JMX Exporter**](https://ibm.github.io/event-streams/installing/configuring#configuring-the-jmx-exporter) - You can use Event Streams to collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus via the [Prometheus JMX Exporter](https://github.com/prometheus/jmx_exporter).\n\n- [**JmxTrans**](https://ibm.github.io/event-streams/security/secure-jmx-connections/#configuring-a-jmxtrans-deployment) - JmxTrans can be used to push JMX metrics from Kafka brokers to external applications or databases.\n\n### Additional Reading\n\n- [Monitoring Kafka](https://kafka.apache.org/documentation/#monitoring) via official Apache Kafka documentation\n- [Monitoring Kafka performance metrics](https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/) via **Datadog**\n- [How to Monitor Kafka](https://blog.serverdensity.com/how-to-monitor-kafka/) via **Server Density**\n- [OpenShift Day 2 Monitoring](https://cloudpak8s.io/day2/Monitoring/) via **IBM Cloud Paks Playbook**\n- [Monitoring Kafka cluster health](https://ibm.github.io/event-streams/administering/cluster-health/) via **IBM Event Streams documentation**\n- [Configuring the monitoring stack](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html) via **Red Hat OpenShift** documentation\n- [Examining cluster metrics](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/examining-cluster-metrics.html) via **Red Hat OpenShift** documentation\n","frontmatter":{"title":"Monitoring IBM Event Streams on OpenShift Cloud Platform","description":"Monitoring Kafka performance metrics and activity when deployed via the IBM Cloud Pak for Integration on Red Hat OpenShift Container Platform."},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/use-cases/monitoring-on-ocp/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}