{"componentChunkName":"component---src-pages-deployments-strimzi-deploy-mdx","path":"/deployments/strimzi/deploy/","result":{"pageContext":{"frontmatter":{"title":"Strimzi Kafka deployment on Openshift or Kubernetes","description":"Strimzi Kafka deployment on Openshift or Kubernetes"},"relativePagePath":"/deployments/strimzi/deploy.mdx","titleType":"append","MdxNode":{"id":"c4422887-1b14-506c-ad75-272e93ee6887","children":[],"parent":"bd833c80-7bc2-5e9d-a751-786840c04268","internal":{"content":"---\ntitle: Strimzi Kafka deployment on Openshift or Kubernetes\ndescription: Strimzi Kafka deployment on Openshift or Kubernetes\n---\n\n[Strimzi](https://strimzi.io/) uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka or Kafka Connect cluster configuration.\n\n## Create a namespace or openshift project\n\n```shell\nkubectl create namespace kafka-strimzi\n\noc create project kafka-strimzi\n```\n\n## Download the strimzi artefacts\n\nFor the last [release github](https://github.com/strimzi/strimzi-kafka-operator/releases). Then modify the Role binding yaml files with the namespace set in previous step.\n\n```shell\nsed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml\n```\n\n## Define Custom Resource Definition for kafka\n\n```shell\noc apply -f install/cluster-operator/ -n jb-kafka-strimzi\n```\n\nThis should create the following resources:\n\n| Names | Resource | Command |\n| :---: | :---: | :---: |\n| strimzi-cluster-operator | Service account | oc get sa |\n| strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation | Role binding | oc get rolebinding |\n| strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator | Cluster Role | oc get clusterrole |\n| strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation | Cluster Role Binding | oc get clusterrolebinding |\n| kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers | Custom Resource Definition | oc get customresourcedefinition |\n\n## Deploy Kafka cluster\n\nChange the name of the cluster in one the yaml in the `examples/kafka` folder.\n\nUsing non presistence:\n\n```shell\noc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi\noc get kafka\n# NAME         DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS\n# my-cluster   3                        3\n# Or\nkubectl  apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi\n```\n\nWhen looking at the pods running we can see the three kafka and zookeeper nodes, but also an entity operator pod.\n\nUsing persistence:\n\n```shell\noc apply -f examples/kafka/kafka-persistent.yaml -n jb-kafka-strimzi\n```\n\n## Topic Operator\n\nThe role of the `Topic Operator` is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics.\n\n### Deploy the operator\n\n```shell\noc apply -f install/topic-operator/ -n jb-kafka-strimzi\n```\n\nThis will add the following:\n\n| Names | Resource | Command |\n| :---: | :---: | :---: |\n| strimzi-topic-operator | Service account | oc get sa |\n| strimzi-topic-operator| Role binding | oc get rolebinding |\n| kafkatopics | Custom Resource Definition | oc get customresourcedefinition |\n\n### Create a topic\n\nEdit a yaml file like the following:\n\n```yaml\napiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaTopic\nmetadata:\n  name: test\n  labels:\n    strimzi.io/cluster: my-cluster\nspec:\n  partitions: 1\n  replicas: 3\n  config:\n    retention.ms: 7200000\n    segment.bytes: 1073741824\n```\n\n```shell\noc apply -f test.yaml -n jb-kafka-strimzi\n\noc get kafkatopics\n```\n\nThis creates a topic `test` in your kafka cluster.\n\n## Test with producer and consumer pods\n\nUse kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag).\n\n```shell\n# Start a consumer on test topic\n\noc run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning\n# Start a text producer\noc run kafka-producer -ti --image=strimzi/kafka:latest-kafka-2.4.0  --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test\n# enter text\n```\n\nIf you want to use the strimzi kafka docker image to run the above scripts locally but remotely connect to a kafka cluster you need multiple things to happen:\n\n* Be sure the kafka yaml file include the external route stamza:\n\n```yaml\nspec:\n  kafka:\n    version: 2.4.0\n    replicas: 3\n    listeners:\n      plain: {}\n      tls: {}\n      external:\n        type: route\n```\n\n* Get the host ip address from the Route resource\n\n```shell\noc get routes my-cluster-kafka-bootstrap -o=jsonpath='{.status.ingress[0].host}{\"\\n\"}'\n```\n\n* Get the TLS certificate from the broker\n\n```shell\noc get secrets\noc extract secret/my-cluster-cluster-ca-cert --keys=ca.crt --to=- > ca.crt\n# transform it fo java truststore\nkeytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt\n```\n\n* Start the docker container by mounting the local folder with the truststore.jks to the `/home`\n\n```shell\ndocker run -ti -v $(pwd):/home strimzi/kafka:latest-kafka-2.4.0  bash\n# inside the container uses the consumer tool\nbash-4.2$ cd /opt/kafka/bin\nbash-4.2$ ./kafka-console-consumer.sh --bootstrap-server  my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol=SSL --consumer-property ssl.truststore.password=password --consumer-property ssl.truststore.location=/home/truststore.jks --topic test --from-beginning\n```\n\n* For a producer the approach is the same but using the producer properties:\n\n```\n./kafka-console-producer.sh --broker-list  my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol=SSL --producer-property ssl.truststore.password=password --producer-property ssl.truststore.location=/home/truststore.jks --topic test   \n```\n\nThose properties can be in file\n\n```shell\nbootstrap.servers=my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud\nsecurity.protocol=SSL\nssl.truststore.password=password\nssl.truststore.location=/home/truststore.jks\n```\n\nand then use the following parameters in the command line:\n\n```shell\n./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test\n\n./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443  --topic test  --consumer.config /home/strimzi.properties --from-beginning \n```","type":"Mdx","contentDigest":"099470e8d748bcf9b9c15a38c34e6a08","counter":269,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Strimzi Kafka deployment on Openshift or Kubernetes","description":"Strimzi Kafka deployment on Openshift or Kubernetes"},"exports":{},"rawBody":"---\ntitle: Strimzi Kafka deployment on Openshift or Kubernetes\ndescription: Strimzi Kafka deployment on Openshift or Kubernetes\n---\n\n[Strimzi](https://strimzi.io/) uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka or Kafka Connect cluster configuration.\n\n## Create a namespace or openshift project\n\n```shell\nkubectl create namespace kafka-strimzi\n\noc create project kafka-strimzi\n```\n\n## Download the strimzi artefacts\n\nFor the last [release github](https://github.com/strimzi/strimzi-kafka-operator/releases). Then modify the Role binding yaml files with the namespace set in previous step.\n\n```shell\nsed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml\n```\n\n## Define Custom Resource Definition for kafka\n\n```shell\noc apply -f install/cluster-operator/ -n jb-kafka-strimzi\n```\n\nThis should create the following resources:\n\n| Names | Resource | Command |\n| :---: | :---: | :---: |\n| strimzi-cluster-operator | Service account | oc get sa |\n| strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation | Role binding | oc get rolebinding |\n| strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator | Cluster Role | oc get clusterrole |\n| strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation | Cluster Role Binding | oc get clusterrolebinding |\n| kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers | Custom Resource Definition | oc get customresourcedefinition |\n\n## Deploy Kafka cluster\n\nChange the name of the cluster in one the yaml in the `examples/kafka` folder.\n\nUsing non presistence:\n\n```shell\noc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi\noc get kafka\n# NAME         DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS\n# my-cluster   3                        3\n# Or\nkubectl  apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi\n```\n\nWhen looking at the pods running we can see the three kafka and zookeeper nodes, but also an entity operator pod.\n\nUsing persistence:\n\n```shell\noc apply -f examples/kafka/kafka-persistent.yaml -n jb-kafka-strimzi\n```\n\n## Topic Operator\n\nThe role of the `Topic Operator` is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics.\n\n### Deploy the operator\n\n```shell\noc apply -f install/topic-operator/ -n jb-kafka-strimzi\n```\n\nThis will add the following:\n\n| Names | Resource | Command |\n| :---: | :---: | :---: |\n| strimzi-topic-operator | Service account | oc get sa |\n| strimzi-topic-operator| Role binding | oc get rolebinding |\n| kafkatopics | Custom Resource Definition | oc get customresourcedefinition |\n\n### Create a topic\n\nEdit a yaml file like the following:\n\n```yaml\napiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaTopic\nmetadata:\n  name: test\n  labels:\n    strimzi.io/cluster: my-cluster\nspec:\n  partitions: 1\n  replicas: 3\n  config:\n    retention.ms: 7200000\n    segment.bytes: 1073741824\n```\n\n```shell\noc apply -f test.yaml -n jb-kafka-strimzi\n\noc get kafkatopics\n```\n\nThis creates a topic `test` in your kafka cluster.\n\n## Test with producer and consumer pods\n\nUse kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag).\n\n```shell\n# Start a consumer on test topic\n\noc run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning\n# Start a text producer\noc run kafka-producer -ti --image=strimzi/kafka:latest-kafka-2.4.0  --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test\n# enter text\n```\n\nIf you want to use the strimzi kafka docker image to run the above scripts locally but remotely connect to a kafka cluster you need multiple things to happen:\n\n* Be sure the kafka yaml file include the external route stamza:\n\n```yaml\nspec:\n  kafka:\n    version: 2.4.0\n    replicas: 3\n    listeners:\n      plain: {}\n      tls: {}\n      external:\n        type: route\n```\n\n* Get the host ip address from the Route resource\n\n```shell\noc get routes my-cluster-kafka-bootstrap -o=jsonpath='{.status.ingress[0].host}{\"\\n\"}'\n```\n\n* Get the TLS certificate from the broker\n\n```shell\noc get secrets\noc extract secret/my-cluster-cluster-ca-cert --keys=ca.crt --to=- > ca.crt\n# transform it fo java truststore\nkeytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt\n```\n\n* Start the docker container by mounting the local folder with the truststore.jks to the `/home`\n\n```shell\ndocker run -ti -v $(pwd):/home strimzi/kafka:latest-kafka-2.4.0  bash\n# inside the container uses the consumer tool\nbash-4.2$ cd /opt/kafka/bin\nbash-4.2$ ./kafka-console-consumer.sh --bootstrap-server  my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol=SSL --consumer-property ssl.truststore.password=password --consumer-property ssl.truststore.location=/home/truststore.jks --topic test --from-beginning\n```\n\n* For a producer the approach is the same but using the producer properties:\n\n```\n./kafka-console-producer.sh --broker-list  my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol=SSL --producer-property ssl.truststore.password=password --producer-property ssl.truststore.location=/home/truststore.jks --topic test   \n```\n\nThose properties can be in file\n\n```shell\nbootstrap.servers=my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud\nsecurity.protocol=SSL\nssl.truststore.password=password\nssl.truststore.location=/home/truststore.jks\n```\n\nand then use the following parameters in the command line:\n\n```shell\n./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test\n\n./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443  --topic test  --consumer.config /home/strimzi.properties --from-beginning \n```","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs-gatsby/src/pages/deployments/strimzi/deploy.mdx"}}}}