{"componentChunkName":"component---src-pages-aaa-orphans-deployments-eventstreams-es-ibm-cloud-mdx","path":"/aaa_ORPHANS/deployments/eventstreams/es-ibm-cloud/","result":{"pageContext":{"frontmatter":{"title":"Event Stream Service on IBM Cloud","description":"Event Stream Service on IBM Cloud"},"relativePagePath":"/aaa_ORPHANS/deployments/eventstreams/es-ibm-cloud.mdx","titleType":"append","MdxNode":{"id":"f8b72195-66bb-5c61-b4a4-333d3d9b326c","children":[],"parent":"f3ffb06e-0397-50c8-ba45-5d1a3662fc64","internal":{"content":"---\ntitle: Event Stream Service on IBM Cloud\ndescription: Event Stream Service on IBM Cloud\n---\n\n\nThe installation instructions are here: [ibm.github.io/event-streams](https://ibm.github.io/event-streams/installing/installing/).\n\n## Things to consider before installation\n\n* Software and hardware [pre-requisites](https://ibm.github.io/event-streams/installing/prerequisites/) \n* Can start small with the 3 brokers cluster. See [configuration note](https://ibm.github.io/event-streams/installing/configuring/)\n* Be sure the servers running the nodes and then event streams pods have enough resource available, using `free -m` linux command for memory and `df` for disk\n* Define the persistence strategy to support persisting logs from the Kafka topic and zookeepers metadata. \n* Do you need to encrypt traffic to the cluster with your own certificates?\n* Do you want to use TLS for pod to pod communication?\n* For geo-replication defines the number of workers dedicated to do replication.\n\n## Provision the service\nTo provision your service, go to the IBM Cloud Catalog and search for `Event Streams`. It is in the *Integration* category. Create the service and specify a name, a region and a resource group, add a tag if you want to, then select the enterprise plan. \n\n![](images/IES-service.png)\n\nOnce the service is provisioned you should reach the welcome page:\n\n![](images/es-ic-service.png)\n\n\n## Add service credentials\n\nIn the service credentials create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumers or producers.\n\n ![](./images/IES-IC-credentials.png)\n\nThe api key and the broker urls will be used to set kubernetes secrets needed by the producer and consumer services. \n\n## Add topic with advanced properties\n\nLaunch the dashboard:\n\n![](images/es-ic-dashboard.png)\n\nand then select the 'create a topic' tile to add the topics needed for the solution. Enter a name and select Advanced switch:\n\n![](images/es-ic-topic-1.png)\n\nIn the advanced configuration, specify the number of partition and event retention time: for topic with not a lot of events, topic can keep data for a long period of time. \n\n![](images/es-ic-topic-2.png)\n\nAs events are persisted in log, we need to specify the cleanup policy, `delete or comprest`, and the size limit for retention, which control the maximum size a partition (which consists of log segments) can grow to before brokers discard old log segments to free up space if topic is set to use the \"delete\" retention policy.\n\nLog segment size correspond to the topic configuration, `segment.bytes`, which controls the segment file size for the log. Retention and cleaning are always done one file at a time, so a larger segment size means fewer files but less granular control over retention.\n\n![](images/es-ic-topic-3.png)\n\n`segment.ms` cotrols the time period after which Kafka will force the log to roll even if the segment file isn't full to ensure that retention can delete or compact old data.\n\n`segment.index.bytes` controls the size of the index that maps offsets to file positions. You generally should not need to change this setting.\n\n![](images/es-ic-topic-4.png)\n\nThe admin REST api offers way to configure more parameters.","type":"Mdx","contentDigest":"f28658ecdbc9f84004fba5968efbd599","counter":645,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Event Stream Service on IBM Cloud\ndescription: Event Stream Service on IBM Cloud\n---\n\n\nThe installation instructions are here: [ibm.github.io/event-streams](https://ibm.github.io/event-streams/installing/installing/).\n\n## Things to consider before installation\n\n* Software and hardware [pre-requisites](https://ibm.github.io/event-streams/installing/prerequisites/) \n* Can start small with the 3 brokers cluster. See [configuration note](https://ibm.github.io/event-streams/installing/configuring/)\n* Be sure the servers running the nodes and then event streams pods have enough resource available, using `free -m` linux command for memory and `df` for disk\n* Define the persistence strategy to support persisting logs from the Kafka topic and zookeepers metadata. \n* Do you need to encrypt traffic to the cluster with your own certificates?\n* Do you want to use TLS for pod to pod communication?\n* For geo-replication defines the number of workers dedicated to do replication.\n\n## Provision the service\nTo provision your service, go to the IBM Cloud Catalog and search for `Event Streams`. It is in the *Integration* category. Create the service and specify a name, a region and a resource group, add a tag if you want to, then select the enterprise plan. \n\n![](images/IES-service.png)\n\nOnce the service is provisioned you should reach the welcome page:\n\n![](images/es-ic-service.png)\n\n\n## Add service credentials\n\nIn the service credentials create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumers or producers.\n\n ![](./images/IES-IC-credentials.png)\n\nThe api key and the broker urls will be used to set kubernetes secrets needed by the producer and consumer services. \n\n## Add topic with advanced properties\n\nLaunch the dashboard:\n\n![](images/es-ic-dashboard.png)\n\nand then select the 'create a topic' tile to add the topics needed for the solution. Enter a name and select Advanced switch:\n\n![](images/es-ic-topic-1.png)\n\nIn the advanced configuration, specify the number of partition and event retention time: for topic with not a lot of events, topic can keep data for a long period of time. \n\n![](images/es-ic-topic-2.png)\n\nAs events are persisted in log, we need to specify the cleanup policy, `delete or comprest`, and the size limit for retention, which control the maximum size a partition (which consists of log segments) can grow to before brokers discard old log segments to free up space if topic is set to use the \"delete\" retention policy.\n\nLog segment size correspond to the topic configuration, `segment.bytes`, which controls the segment file size for the log. Retention and cleaning are always done one file at a time, so a larger segment size means fewer files but less granular control over retention.\n\n![](images/es-ic-topic-3.png)\n\n`segment.ms` cotrols the time period after which Kafka will force the log to roll even if the segment file isn't full to ensure that retention can delete or compact old data.\n\n`segment.index.bytes` controls the size of the index that maps offsets to file positions. You generally should not need to change this setting.\n\n![](images/es-ic-topic-4.png)\n\nThe admin REST api offers way to configure more parameters.","frontmatter":{"title":"Event Stream Service on IBM Cloud","description":"Event Stream Service on IBM Cloud"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/aaa_ORPHANS/deployments/eventstreams/es-ibm-cloud.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}