{"componentChunkName":"component---src-pages-aaa-orphans-deployments-eventstreams-readme-mdx","path":"/aaa_ORPHANS/deployments/eventstreams/README/","result":{"pageContext":{"frontmatter":{"title":"Install IBM Event Streams on ICP","description":"Install IBM Event Streams on ICP"},"relativePagePath":"/aaa_ORPHANS/deployments/eventstreams/README.mdx","titleType":"append","MdxNode":{"id":"e0f4ccba-b1c9-5c80-bf3a-63a685659876","children":[],"parent":"aac9e47a-5585-5e3a-86ab-01b648d2b816","internal":{"content":"---\ntitle: Install IBM Event Streams on ICP\ndescription: Install IBM Event Streams on ICP\n---\n\n*(Tested on September 2019 using ibm-eventstreams-prod helm chart 1.3.0 on ICP 3.2.0)*\n\nYou can use the `ibm-eventstreams-dev` or `ibm-eventstreams-prod` Helm chart from ICP catalog. The product installation instructions can be found [in event stream documentation](https://ibm.github.io/event-streams/installing/installing/).  \n\n!!! note\n        If you need to upload the tar file for the event streams production (downloaded from IBM passport advantage or other support sites) use the following command:\n        ```\n        cloudctl catalog load-archive --archive eventstreams.pak.tar.gz\n        ```\n\nAs we do not want to rewrite the very good [product documentation](https://ibm.github.io/event-streams/installing/installing/), we just want to highlight what we did for our own deployment. Our cluster has the following characteristics:\n\n* Three masters also running ETCD cluster on 3 nodes\n* Three management nodes\n* Three proxy\n* Six worker nodes\n\nFor worker nodes we need good CPUs and hard disk space. We allocated 12 CPUs - 32 Gb RAM per worker nodes and around 300GB for each worker node.\n\nYou need to decide if persistence should be enabled for ZooKeepers and Kafka brokers. Pre allocate one Persistence Volume per Kafka broker and one per ZooKeeper server.  If you use dynamic persistence volume provisioning, ensure the expected volumes are present at installation time.\n\nThe yaml file for PV creation is in the `refarch-eda/deployments/eventstreams` folder. The command:\n\n```\nkubectl apply -f ibm-es-pv.yaml -n eventstreams\n```\n\ncreates 7 volumes: 3 for zookeeper, and 3 for kafka and 1 for schema registry. The configuration is for development purpose, and uses local host path, so if the VM has an issue data will be lost. \n\n```\nkubectl get pv -n eventstreams\n```\n\nWe also created an empty config map so we can update the kafka server.properites in the future.\n\n```\nkubectl create configmap greencompute-events-km -n eventstreams\n```\n\n### Configuration Parameters \n\nThe following parameters were changed from default settings:  \n\n | Parameter    | Description | Value    |\n | :------------- | :------------- | :------------- |\n | Kafka.autoCreateTopicsEnable     | Enable auto-creation of topics       | true |\n | persistence.enabled | enable persistent storage for the Kafka brokers | true |\n | persistence.useDynamicProvisioning | dynamically create persistent volume claims | false |\n | zookeeper.persistence.enabled | use persistent storage for the ZooKeeper nodes | true |\n  | zookeeper.persistence.useDynamicProvisioning | dynamically create persistent volume claims for the ZooKeeper nodes | false |\n  | proxy.externalAccessEnabled | allow external access to Kafka from outside the Kubernetes cluster | true |\n\nThe matching `server.properties` file is under the `deployments/eventstreams` folder. See parameters description in the [product documentation](https://kafka.apache.org/documentation/#brokerconfigs) \n\nYou can get the details of the release with: `helm list 'green-events-streams' --tls` or access helm detail via ICP console: Here is the helm release details:\n\n![](images/ies-helm-rel01.png)\n\n\nThe figure above shows the following elements:\n\n* ConfigMaps for UI, Kafka proxy\n* The five deployments for each major components: UI, REST, proxy and access controller.\n\nNext is the job list which shows what was run during installation. The panel lists also the current network policies: \n\n![](images/ies-helm-rel02.png)\n\n> A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. As soon as there are policies defined, pods will reject connections not allowed by any policies.\n\nThe pods running in the platform. (One pod was a job)\n\n![](images/ies-helm-pods.png)  \n\n\nAs we can see there are 3 kafka brokers, 3 zookeepers, 2 proxies, 2 access controllers. \n\nYou can see the pods running on a node using the command: \n`kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.50.219`\n\nThe figure below is for roles, rolebinding and secret as part of the Role Based Access Control settings.\n\n![](images/ies-helm-rel03.png)\n\n\nThe figure below shows the services for zookeeper, Kafka and Event Stream REST api and user interface:  \n\n![](images/ies-helm-serv.png)\n\nThe services expose capabilities to external world via nodePort type:\n\n* The IBM Event Streams admin console is visible at the port 31253 on the k8s proxy IP address: 172.16.50.227\n* The REST api port 30121\n* stream proxy port bootstrap: 31348, broker 0: 32489...\n\nYou get access to the Event Streams admin console by using the IP address of the master  / proxy node and the port number of the service, which you can get using the kubectl get service command like:\n```\nkubectl get svc -n streaming \"green-events-streams-ibm-es-ui-svc\" -o 'jsonpath={.spec.ports[?(@.name==\"admin-ui-https\")].nodePort}'\n\nkubectl cluster-info | grep \"catalog\" | awk 'match($0, /([0-9]{1,3}\\.){3}[0-9]{1,3}/) { print substr( $0, RSTART, RLENGTH )}'\n```\n\nHere is the admin console home page:\n\n![](images/event-stream-admin.png)\n\nTo connect an application or tool to this cluster, you will need the address of a bootstrap server, a certificate and an API key. The page to access this information, is on the top right corner: `Connect to this cluster`:\n\n![](images/ies-cluster-connection.png)\n\nDownload certificate and Java truststore files, and the generated API key. A key can apply to all groups or being specific to a group. \n\nIn Java to leverage the api key the code needs to set the some properties:\n\n```java\nproperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");\nproperties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");\nproperties.put(SaslConfigs.SASL_JAAS_CONFIG,\n        \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\"\n                + env.get(\"KAFKA_APIKEY\") + \"\\\";\");\nproperties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");\nproperties.put(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, \"TLSv1.2\");\nproperties.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, \"HTTPS\");\n```\n\nSee code example in [ApplicationConfig.java](https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/a6b6a2d15085029f5d46b9826806e722a8c27bff/order-command-ms/src/main/java/ibm/labs/kc/order/command/kafka/ApplicationConfig.java#L71-L86).\n\n## Some challenges during the installation\n\nAs presented in the high availability discussion in [this note](../kafka#high-availability-in-the-context-of-kubernetes-deployment), normally we need 6 worker nodes to avoid allocating zookeeper and kafka servers on the same kubernetes nodes. The community edition installation is permissive on that constraint, so both products could co-exist but in that case, ensure to have enough physical resources. \nWe have seen some Kafka brokers that could not be scheduled because some nodes have taints (can't meet the specs for the stateful set) and the remaining worker nodes don't have enough memory.\n\n### Helm release does not install\n\nIf you do not see a helm release added to the helm release list, try to see if the installation pod is working. For example the following command, returns a pod name for the release creation job:\n\n```\nkubctl get pods -n eventstreams\n\nNAME                                               READY   STATUS             RESTARTS   AGE\neventstreams-ibm-es-release-cm-creater-job-gkgx2   0/1     ImagePullBackOff   0          1m\n```\n\nYou caan then access the pod logs to assess what's going on:\n\n```\nkubectl logs eventstreams-ibm-es-release-cm-creater-job-gkgx2 -n eventstreams\n```\n\nOne possible common issue is related to the pod trying and failing to pull image from local repository. To find the solution, you need to know the name of the helm repository: \n\n\n## Getting started application\n\nUse the Event Stream Toolbox to download a getting started application we can use to test the deployment and as code base for future Kafka consumer / producer development.\n\n![](images/ies-starter-app.png)  \n\n![](images/ies-starter-app2.png)  \n\n One example of the generated app is in this repository under `gettingStarted/EDAIEWStarterApp` folder, and a description on how to compile, package and run it: see the ./gettingStarted/EDAIEWStarterApp/README.md.\n\nThe application runs in Liberty at the URL: http://localhost:9080/EDAIESStarterApp/ and delivers a simple user interface splitted into two panels: producer and consumer.\n\n![](images/ies-start-app-run.png)  \n\nThe figure below illustrates the fact that the connetion to the broker was not working for a short period of time, so the producer has error, but because of the buffering capabilities, it was able to pace and then as soon as the connection was re-established the consumer started to get the messages. No messages were lost!.\n\n![](images/ies-start-app-run2.png) \n\nWe have two solution implementations using Kafka and Event Streams [the manufacturing asset analytics](https://github.com/ibm-cloud-architecture/refarch-asset-analytics) and the  most recent [KC container shipment solution](https://github.com/ibm-cloud-architecture/refarch-kc). We recommend using the second implementation.\n\n## Verifying ICP Events Streams installation\n\nOnce connected to the cluster with kubectl, get the list of pods for the namespace you used to install Kafka or IBM Event Streams:\n```\n$ kubectl get pods -n streaming\n\nNAME                                                              READY     STATUS    RESTARTS\ngreen-even-c353-ibm-es-elas-ad8d-0                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-elas-ad8d-1                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-kafka-sts-0                                4/4       Running   2          3d\ngreen-even-c353-ibm-es-kafka-sts-1                                4/4       Running   2          3d\ngreen-even-c353-ibm-es-kafka-sts-2                                4/4       Running   5          3d\ngreen-even-c353-ibm-es-zook-c4c0-0                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-zook-c4c0-1                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-zook-c4c0-2                                1/1       Running   0          3d\ngreen-events-streams-ibm-es-access-controller-deploy-7cbf8jjs9n   2/2       Running   0          3d\ngreen-events-streams-ibm-es-access-controller-deploy-7cbf8st95z   2/2       Running   0          3d\ngreen-events-streams-ibm-es-indexmgr-deploy-6ff759779-c8ddc       1/1       Running   0          3d\ngreen-events-streams-ibm-es-proxy-deploy-777d6cf76c-bxjtq         1/1       Running   0          3d\ngreen-events-streams-ibm-es-proxy-deploy-777d6cf76c-p8rkc         1/1       Running   0          3d\ngreen-events-streams-ibm-es-rest-deploy-547cc6f9b-774xx           3/3       Running   0          3d\ngreen-events-streams-ibm-es-ui-deploy-7f9b9c6c6f-kvvs2            3/3       Running   0          3d\n\n```\n\nSelect the first pod: green-even-c353-ibm-es-kafka-sts-0 , then execute a bash shell so you can access the Kafka tools:\n```\n$ kubectl exec green-even-c353-ibm-es-kafka-sts-0 -itn streaming -- bash\nbash-3.4# cd /opt/Kafka/bin\n```\nNow you have access to the kafka tools. The most important thing is to get the hostname and port number of the zookeeper server. To do so use the kubectl command:\n```\n$ kubectl describe pods green-even-c353-ibm-es-zook-c4c0-0  --namespace streaming\n```\nIn the long result get the client port ( ZK_CLIENT_PORT: 2181) information and IP address (IP: 192.168.76.235). Using this information, in the bash shell within the Kafka broker server we can do the following command to get the topics configured.\n\n```shell\n$ ./Kafka-topics.sh --list -zookeeper  192.168.76.235:2181\n# We can also use the service name of zookeeper and let k8s DNS resolve the IP address\n$ ./Kafka-topics.sh --list -zookeeper  green-even-c353-ibm-es-zook-c4c0-0.streaming.svc.cluster.local:2181\n```\n\n\n### Using the Event Stream CLI\n\nIf not done already, you can install the Event Stream CLI on top of IBM cloud CLI by first downloading it from the Event Stream console and then running this command:\n```\n$ cloudctl plugin install ./es-plugin\n```\n\nHere is a simple summary of the possible `cloudctl es` commands:\n```shell\n# Connect to the cluster\ncloudctl es init\n\n# create a topic  - default is 3 replicas\ncloudctl es topic-create streams-plaintext-input\ncloudctl es topic-create streams-wordcount-output --replication-factor 1 --partitions 1\n\n# list topics\ncloudctl es topics\n\n# delete topic\ncloudctl es topic-delete streams-plaintext-input\n```\n\n## Further Readings\n\n* [IBM Event Streams main page](https://www.ibm.com/cloud/event-streams)\n* [IBM Event Streams Product Documentation](https://ibm.github.io/event-streams)","type":"Mdx","contentDigest":"0b07f59ee45dda47b388561f97afe70a","counter":359,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Install IBM Event Streams on ICP","description":"Install IBM Event Streams on ICP"},"exports":{},"rawBody":"---\ntitle: Install IBM Event Streams on ICP\ndescription: Install IBM Event Streams on ICP\n---\n\n*(Tested on September 2019 using ibm-eventstreams-prod helm chart 1.3.0 on ICP 3.2.0)*\n\nYou can use the `ibm-eventstreams-dev` or `ibm-eventstreams-prod` Helm chart from ICP catalog. The product installation instructions can be found [in event stream documentation](https://ibm.github.io/event-streams/installing/installing/).  \n\n!!! note\n        If you need to upload the tar file for the event streams production (downloaded from IBM passport advantage or other support sites) use the following command:\n        ```\n        cloudctl catalog load-archive --archive eventstreams.pak.tar.gz\n        ```\n\nAs we do not want to rewrite the very good [product documentation](https://ibm.github.io/event-streams/installing/installing/), we just want to highlight what we did for our own deployment. Our cluster has the following characteristics:\n\n* Three masters also running ETCD cluster on 3 nodes\n* Three management nodes\n* Three proxy\n* Six worker nodes\n\nFor worker nodes we need good CPUs and hard disk space. We allocated 12 CPUs - 32 Gb RAM per worker nodes and around 300GB for each worker node.\n\nYou need to decide if persistence should be enabled for ZooKeepers and Kafka brokers. Pre allocate one Persistence Volume per Kafka broker and one per ZooKeeper server.  If you use dynamic persistence volume provisioning, ensure the expected volumes are present at installation time.\n\nThe yaml file for PV creation is in the `refarch-eda/deployments/eventstreams` folder. The command:\n\n```\nkubectl apply -f ibm-es-pv.yaml -n eventstreams\n```\n\ncreates 7 volumes: 3 for zookeeper, and 3 for kafka and 1 for schema registry. The configuration is for development purpose, and uses local host path, so if the VM has an issue data will be lost. \n\n```\nkubectl get pv -n eventstreams\n```\n\nWe also created an empty config map so we can update the kafka server.properites in the future.\n\n```\nkubectl create configmap greencompute-events-km -n eventstreams\n```\n\n### Configuration Parameters \n\nThe following parameters were changed from default settings:  \n\n | Parameter    | Description | Value    |\n | :------------- | :------------- | :------------- |\n | Kafka.autoCreateTopicsEnable     | Enable auto-creation of topics       | true |\n | persistence.enabled | enable persistent storage for the Kafka brokers | true |\n | persistence.useDynamicProvisioning | dynamically create persistent volume claims | false |\n | zookeeper.persistence.enabled | use persistent storage for the ZooKeeper nodes | true |\n  | zookeeper.persistence.useDynamicProvisioning | dynamically create persistent volume claims for the ZooKeeper nodes | false |\n  | proxy.externalAccessEnabled | allow external access to Kafka from outside the Kubernetes cluster | true |\n\nThe matching `server.properties` file is under the `deployments/eventstreams` folder. See parameters description in the [product documentation](https://kafka.apache.org/documentation/#brokerconfigs) \n\nYou can get the details of the release with: `helm list 'green-events-streams' --tls` or access helm detail via ICP console: Here is the helm release details:\n\n![](images/ies-helm-rel01.png)\n\n\nThe figure above shows the following elements:\n\n* ConfigMaps for UI, Kafka proxy\n* The five deployments for each major components: UI, REST, proxy and access controller.\n\nNext is the job list which shows what was run during installation. The panel lists also the current network policies: \n\n![](images/ies-helm-rel02.png)\n\n> A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. As soon as there are policies defined, pods will reject connections not allowed by any policies.\n\nThe pods running in the platform. (One pod was a job)\n\n![](images/ies-helm-pods.png)  \n\n\nAs we can see there are 3 kafka brokers, 3 zookeepers, 2 proxies, 2 access controllers. \n\nYou can see the pods running on a node using the command: \n`kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.50.219`\n\nThe figure below is for roles, rolebinding and secret as part of the Role Based Access Control settings.\n\n![](images/ies-helm-rel03.png)\n\n\nThe figure below shows the services for zookeeper, Kafka and Event Stream REST api and user interface:  \n\n![](images/ies-helm-serv.png)\n\nThe services expose capabilities to external world via nodePort type:\n\n* The IBM Event Streams admin console is visible at the port 31253 on the k8s proxy IP address: 172.16.50.227\n* The REST api port 30121\n* stream proxy port bootstrap: 31348, broker 0: 32489...\n\nYou get access to the Event Streams admin console by using the IP address of the master  / proxy node and the port number of the service, which you can get using the kubectl get service command like:\n```\nkubectl get svc -n streaming \"green-events-streams-ibm-es-ui-svc\" -o 'jsonpath={.spec.ports[?(@.name==\"admin-ui-https\")].nodePort}'\n\nkubectl cluster-info | grep \"catalog\" | awk 'match($0, /([0-9]{1,3}\\.){3}[0-9]{1,3}/) { print substr( $0, RSTART, RLENGTH )}'\n```\n\nHere is the admin console home page:\n\n![](images/event-stream-admin.png)\n\nTo connect an application or tool to this cluster, you will need the address of a bootstrap server, a certificate and an API key. The page to access this information, is on the top right corner: `Connect to this cluster`:\n\n![](images/ies-cluster-connection.png)\n\nDownload certificate and Java truststore files, and the generated API key. A key can apply to all groups or being specific to a group. \n\nIn Java to leverage the api key the code needs to set the some properties:\n\n```java\nproperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");\nproperties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");\nproperties.put(SaslConfigs.SASL_JAAS_CONFIG,\n        \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\"\n                + env.get(\"KAFKA_APIKEY\") + \"\\\";\");\nproperties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");\nproperties.put(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, \"TLSv1.2\");\nproperties.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, \"HTTPS\");\n```\n\nSee code example in [ApplicationConfig.java](https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/a6b6a2d15085029f5d46b9826806e722a8c27bff/order-command-ms/src/main/java/ibm/labs/kc/order/command/kafka/ApplicationConfig.java#L71-L86).\n\n## Some challenges during the installation\n\nAs presented in the high availability discussion in [this note](../kafka#high-availability-in-the-context-of-kubernetes-deployment), normally we need 6 worker nodes to avoid allocating zookeeper and kafka servers on the same kubernetes nodes. The community edition installation is permissive on that constraint, so both products could co-exist but in that case, ensure to have enough physical resources. \nWe have seen some Kafka brokers that could not be scheduled because some nodes have taints (can't meet the specs for the stateful set) and the remaining worker nodes don't have enough memory.\n\n### Helm release does not install\n\nIf you do not see a helm release added to the helm release list, try to see if the installation pod is working. For example the following command, returns a pod name for the release creation job:\n\n```\nkubctl get pods -n eventstreams\n\nNAME                                               READY   STATUS             RESTARTS   AGE\neventstreams-ibm-es-release-cm-creater-job-gkgx2   0/1     ImagePullBackOff   0          1m\n```\n\nYou caan then access the pod logs to assess what's going on:\n\n```\nkubectl logs eventstreams-ibm-es-release-cm-creater-job-gkgx2 -n eventstreams\n```\n\nOne possible common issue is related to the pod trying and failing to pull image from local repository. To find the solution, you need to know the name of the helm repository: \n\n\n## Getting started application\n\nUse the Event Stream Toolbox to download a getting started application we can use to test the deployment and as code base for future Kafka consumer / producer development.\n\n![](images/ies-starter-app.png)  \n\n![](images/ies-starter-app2.png)  \n\n One example of the generated app is in this repository under `gettingStarted/EDAIEWStarterApp` folder, and a description on how to compile, package and run it: see the ./gettingStarted/EDAIEWStarterApp/README.md.\n\nThe application runs in Liberty at the URL: http://localhost:9080/EDAIESStarterApp/ and delivers a simple user interface splitted into two panels: producer and consumer.\n\n![](images/ies-start-app-run.png)  \n\nThe figure below illustrates the fact that the connetion to the broker was not working for a short period of time, so the producer has error, but because of the buffering capabilities, it was able to pace and then as soon as the connection was re-established the consumer started to get the messages. No messages were lost!.\n\n![](images/ies-start-app-run2.png) \n\nWe have two solution implementations using Kafka and Event Streams [the manufacturing asset analytics](https://github.com/ibm-cloud-architecture/refarch-asset-analytics) and the  most recent [KC container shipment solution](https://github.com/ibm-cloud-architecture/refarch-kc). We recommend using the second implementation.\n\n## Verifying ICP Events Streams installation\n\nOnce connected to the cluster with kubectl, get the list of pods for the namespace you used to install Kafka or IBM Event Streams:\n```\n$ kubectl get pods -n streaming\n\nNAME                                                              READY     STATUS    RESTARTS\ngreen-even-c353-ibm-es-elas-ad8d-0                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-elas-ad8d-1                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-kafka-sts-0                                4/4       Running   2          3d\ngreen-even-c353-ibm-es-kafka-sts-1                                4/4       Running   2          3d\ngreen-even-c353-ibm-es-kafka-sts-2                                4/4       Running   5          3d\ngreen-even-c353-ibm-es-zook-c4c0-0                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-zook-c4c0-1                                1/1       Running   0          3d\ngreen-even-c353-ibm-es-zook-c4c0-2                                1/1       Running   0          3d\ngreen-events-streams-ibm-es-access-controller-deploy-7cbf8jjs9n   2/2       Running   0          3d\ngreen-events-streams-ibm-es-access-controller-deploy-7cbf8st95z   2/2       Running   0          3d\ngreen-events-streams-ibm-es-indexmgr-deploy-6ff759779-c8ddc       1/1       Running   0          3d\ngreen-events-streams-ibm-es-proxy-deploy-777d6cf76c-bxjtq         1/1       Running   0          3d\ngreen-events-streams-ibm-es-proxy-deploy-777d6cf76c-p8rkc         1/1       Running   0          3d\ngreen-events-streams-ibm-es-rest-deploy-547cc6f9b-774xx           3/3       Running   0          3d\ngreen-events-streams-ibm-es-ui-deploy-7f9b9c6c6f-kvvs2            3/3       Running   0          3d\n\n```\n\nSelect the first pod: green-even-c353-ibm-es-kafka-sts-0 , then execute a bash shell so you can access the Kafka tools:\n```\n$ kubectl exec green-even-c353-ibm-es-kafka-sts-0 -itn streaming -- bash\nbash-3.4# cd /opt/Kafka/bin\n```\nNow you have access to the kafka tools. The most important thing is to get the hostname and port number of the zookeeper server. To do so use the kubectl command:\n```\n$ kubectl describe pods green-even-c353-ibm-es-zook-c4c0-0  --namespace streaming\n```\nIn the long result get the client port ( ZK_CLIENT_PORT: 2181) information and IP address (IP: 192.168.76.235). Using this information, in the bash shell within the Kafka broker server we can do the following command to get the topics configured.\n\n```shell\n$ ./Kafka-topics.sh --list -zookeeper  192.168.76.235:2181\n# We can also use the service name of zookeeper and let k8s DNS resolve the IP address\n$ ./Kafka-topics.sh --list -zookeeper  green-even-c353-ibm-es-zook-c4c0-0.streaming.svc.cluster.local:2181\n```\n\n\n### Using the Event Stream CLI\n\nIf not done already, you can install the Event Stream CLI on top of IBM cloud CLI by first downloading it from the Event Stream console and then running this command:\n```\n$ cloudctl plugin install ./es-plugin\n```\n\nHere is a simple summary of the possible `cloudctl es` commands:\n```shell\n# Connect to the cluster\ncloudctl es init\n\n# create a topic  - default is 3 replicas\ncloudctl es topic-create streams-plaintext-input\ncloudctl es topic-create streams-wordcount-output --replication-factor 1 --partitions 1\n\n# list topics\ncloudctl es topics\n\n# delete topic\ncloudctl es topic-delete streams-plaintext-input\n```\n\n## Further Readings\n\n* [IBM Event Streams main page](https://www.ibm.com/cloud/event-streams)\n* [IBM Event Streams Product Documentation](https://ibm.github.io/event-streams)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/aaa_ORPHANS/deployments/eventstreams/README.mdx"}}}}