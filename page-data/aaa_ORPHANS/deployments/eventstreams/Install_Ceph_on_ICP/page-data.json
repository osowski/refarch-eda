{"componentChunkName":"component---src-pages-aaa-orphans-deployments-eventstreams-install-ceph-on-icp-mdx","path":"/aaa_ORPHANS/deployments/eventstreams/Install_Ceph_on_ICP/","result":{"pageContext":{"frontmatter":{"title":"Install and configure Ceph for IBM Cloud Private","description":"Install and configure Ceph for IBM Cloud Private"},"relativePagePath":"/aaa_ORPHANS/deployments/eventstreams/Install_Ceph_on_ICP.mdx","titleType":"append","MdxNode":{"id":"4d855040-b908-55f9-ba1e-364cb5d2eb46","children":[],"parent":"8a42e3bb-da89-57b0-afad-f4382396f513","internal":{"content":"---\ntitle: Install and configure Ceph for IBM Cloud Private\ndescription: Install and configure Ceph for IBM Cloud Private\n---\n\n[Ceph](https://ceph.com/) is open source software designed to provide highly scalable object, block and file-based storage under a unified system.\n\nCeph provides a POSIX-compliant network file system (CephFS) that aims for high performance, large data storage, and maximum compatibility with legacy applications.\n\n[Rook](https://github.com/rook/rook) is an open source orchestrator for distributed storage systems running in cloud native environments.\n\nRook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties.\n\n!!! note\n    The Helm chart *ibm-rook-rbd-cluster* is used for setting up Ceph Cluster in IBM Cloud Private.   \n\n## Environment\n\nA typical IBM Cloud Private Environment includes Boot node, Master node, Management node, Proxy node and Worker nodes. When the Ceph RBD Cluster is used for providing storage for API Connect, any three worker nodes should be configured to have additional raw disks.\n\nThe following set of systems can be used as reference for building *development (non-HA) environment* that runs IBM API Connect workload on IBM Cloud Private.\n\n| Node type | Number of nodes | CPU | Memory (GB) | Disk (GB) |\n| :---: | :---: | :---: | :---: | :---: |\n|\tBoot (FTP Server) | 1\t| 8\t| 32 | 2048 |\n|\tMaster\t| 1\t| 8\t| 32 | 300 |\n|\tManagement | 1\t| 8\t| 32 | 300 |\n|\tProxy\t| 1\t| 4\t| 16 | 300 |\n|\tWorker | 3 | 8 | 32\t| 300+500(disk2)|\n|\tTotal |\t7 | 52 | 208 | 3848+1500(disk2) |\n\nThe following set of systems can be used as reference for building *production (HA) environment* that runs IBM API Connect workload on IBM Cloud Private.\n\n| Node type | Number of nodes | CPU | Memory (GB) | Disk (GB) |\n| :---: | :---: | :---: | :---: | :---: |\n|\tBoot (FTP Server)\t| 1\t| 8\t| 32 | 2048 |\n|\tMaster\t| 3\t| 8\t| 32 | 300 |\n|\tManagement | 2\t| 8\t| 32 | 300 |\n|\tProxy\t| 3\t| 4\t| 16 | 300 |\n|\tWorker  | 3 | 16 | 64 | 300+750(disk2)|\n|\tTotal |\t12\t| 108| 432 | 5348+2250(disk2) |\n\n!!! note\n    Additional worker nodes will be required when there is a a need to run workloads other than IBM API Connect on IBM Cloud Private.\n\n## Setup\n\nThis document covers the setup of *Ceph* storage using *Rook*.\n\nThe following tasks are performed for setting up the Ceph Cluster. \n\n1. [Download the required setup files](#1-download-the-required-setup-files)\n2. [Logon to IBM Cloud Private Cluster](#2-logon-to-ibm-cloud-private-cluster)\n3. [Setup Ceph Cluster](#3-setup-ceph-cluster)\n4. [Verify Ceph cluster](#4-verify-ceph-cluster)\n5. [Troubleshooting Ceph setup](#5-troubleshooting-ceph-setup)\n\n\n### 1. Download the required setup files\n\n!!! note\n    The following files are required for installing *ibm-rook-rbd-cluster* chart and setting up Ceph cluster\n\n- [login.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/scripts/icp/login.sh) - Utility for logging onto IBM Cloud Private\n- [ibm-rook-rbd-cluster-0.8.3.tgz](https://raw.githubusercontent.com/IBM/charts/master/repo/stable/ibm-rook-rbd-cluster-0.8.3.tgz) - IBM Chart for Rook RBD Cluster\n- [ceph-values.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/ceph-values.yaml) - Sample values.yaml for installing Ceph Cluster \n- [rook-ceph-cluster-role-binding.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-ceph-cluster-role-binding.yaml) - ClusterRoleBinding for the service account rook-ceph-cluster\n- [rook-ceph-operator-values.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-ceph-operator-values.yaml) - Sample values.yaml for installing rook operator\n- [rook-cluster-role.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-cluster-role.yaml) - ClusterRole for the resource rook-privileged\n- [rook-pod-security-policy.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-pod-security-policy.yaml) - Define PodSecurityPolicy rook-privileged\n- [setup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/setup.sh) - Utility for setting up Ceph Cluster \n- [status.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/status.sh) - Utility for verifying Ceph Cluster \n- [cleanup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/cleanup.sh) - Utility for cleaning up Ceph Cluster \n\n\n### 2. Logon to IBM Cloud Private Cluster\n\nThe script [login.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/scripts/icp/login.sh) can be run to login to IBM Cloud Private Cluster. \n\n!! note\n    The script should be updated to include the correct value for *CLUSTER_NAME*.\n\nSample run of the login script is as follows: \n\n![](./images/loginToDefaultNamespace.png)\n\n\n### 3. Setup Ceph Cluster \n\n**Step #1** Update the [ceph-values.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/ceph-values.yaml) to match your environment.\n\nThe file *ceph-values.yaml* needs to be updated to list the IP address of the storage node within the IBM Cloud Private cluster. \n\n```\n...\n#\n# UPDATE VARIABLES TO MATCH THE ENVIRONMENT\n#   \n    nodes:\n    - name: \"X.X.X.X\"\n      devices:\n      - name: \"DISK_NAME\"\n    - name: \"Y.Y.Y.Y\"\n      devices:\n      - name: \"DISK_NAME\"\n    - name: \"Z.Z.Z.Z\"\n      devices:\n      - name: \"DISK_NAME\"\n...\n...\n```\n\n**Step #2** Modify and run the setup script to install Rook Operator chart and the IBM Rook RBD Cluster chart\n\nThe contents of the script [setup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/setup.sh) is as follows: \n\n```\n#\n# UPDATE VARIABLES TO MATCH THE ENVIRONMENT\n#\n\n# Define the location of images \nIMAGE_DIR=/DIRECTORY_HAVING_IMAGES\n\n...\n```\n\n!!! note\n    The script should be updated to include the correct location for *IMAGE_DIR* that has the location where the chart ibm-rook-rbd-cluster-0.8.3.tgz is downloaded and unzipped. \n\nThe output of Ceph install is listed below for reference: \n \n- [ceph_install.log](./samples/ceph_install.log)\n\n\n### 4. Verify Ceph cluster\n\nThe script [status.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/status.sh) can be run to check if Ceph cluster is working as expected. \n\nThe contents of the script [status.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/status.sh) is as follows: \n\n```\n./deployments/ceph/status.sh \n``` \n\nExpected output is listed below. \n\n![](./images/cephGetStatus.png)\n\n\n### 5. Troubleshooting Ceph setup\n\n#### 5.1 Steps for reseting an used disk \n\nIt is possible that sometimes OSD pods does't start up even though the OSD prepare jobs have completed successfully. \nIt could happen when the device you have specified does not have a raw disk and the device name you have listed was used for other storage like GlusterFS cluster.\n\nIn such case the following commands can be run to collect the Logical Volume group ID and Physical volume and remove it fully so that the raw disk is made available for the Ceph cluster.\n\n```\npvs\npvdisplay\nvgremove LOGIOCAL_VOLUME_GROUP_ID -y \npvremove PHYSICAL_VOLUME\n```\n\nThe output of the aforesaid commands is listed below.\n\n```\n[root@rsun-rhel-glusterfs03 ~]# pvs\n  PV         VG                                  Fmt  Attr PSize   PFree  \n  /dev/sda2  rhel                                lvm2 a--   39.00g      0 \n  /dev/sdb   vg_687894352b254c630b291bf094a8d43d lvm2 a--  499.87g 499.87g\n  /dev/sdc   rhel                                lvm2 a--  500.00g      0 \n[root@rsun-rhel-glusterfs03 ~]# pvdisplay\n  --- Physical volume ---\n  PV Name               /dev/sdb\n  VG Name               vg_687894352b254c630b291bf094a8d43d\n  PV Size               500.00 GiB / not usable 132.00 MiB\n  Allocatable           yes \n  PE Size               4.00 MiB\n  Total PE              127967\n  Free PE               127967\n  Allocated PE          0\n  PV UUID               v6xOuh-M2ot-oXfl-IWyf-TnYL-nX3a-kzqizN\n   \n  --- Physical volume ---\n  PV Name               /dev/sda2\n  VG Name               rhel\n  PV Size               39.00 GiB / not usable 3.00 MiB\n  Allocatable           yes (but full)\n  PE Size               4.00 MiB\n  Total PE              9983\n  Free PE               0\n  Allocated PE          9983\n  PV UUID               tNjUif-RlBT-kdDn-PWwE-LHlq-3w9O-65Hlph\n   \n  --- Physical volume ---\n  PV Name               /dev/sdc\n  VG Name               rhel\n  PV Size               500.00 GiB / not usable 4.00 MiB\n  Allocatable           yes (but full)\n  PE Size               4.00 MiB\n  Total PE              127999\n  Free PE               0\n  Allocated PE          127999\n  PV UUID               7CXpz5-95hb-0WAC-3Efe-XrY1-s6E6-dqLasC\n   \n[root@rsun-rhel-glusterfs03 ~]# vgremove vg_687894352b254c630b291bf094a8d43d -y \n  Volume group \"vg_687894352b254c630b291bf094a8d43d\" successfully removed\n[root@rsun-rhel-glusterfs03 ~]# pvremove /dev/sdb \n  Labels on physical volume \"/dev/sdb\" successfully wiped.\n```\n\n#### 5.2 Steps for uninstalling the rook-ceph setup \n\n**Step #1** The script [cleanup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/cleanup.sh) can be run to remove the Ceph setup completely.\n\n```\n./deployments/ceph/cleanup.sh\n``` \n\n**Step #2** Remove the contents of the temporary directory used by rook: */var/lib/rook*\n\nThe following command should run on all the worker nodes: \n\n```\nrm -fr /var/lib/rook\n```\n\n#  Ceph Cluster Management\n\nThe following links has additional details on how to diagnose, troubleshoot, monitor and report Ceph cluster storage: \n\n* [https://github.com/rook/rook/tree/master/Documentation](https://github.com/rook/rook/tree/master/Documentation)\n* [https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques](https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques)\n* [https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/](https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/)\n* [https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know](https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know)\n* [https://sabaini.at/pages/ceph-cheatsheet.html](https://sabaini.at/pages/ceph-cheatsheet.html)\n\nThe *Ceph Monitor* pod can be attached using the following command:\n\n```\nkubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" -o jsonpath='{.items[0].metadata.name}') bash\n```\n\nAfter being attached to the *Ceph Monitor* pod, the following commands can be run which provides status and statistics of the *Ceph Cluster*.\n\n```\nceph health \nceph status \nceph df \nceph osd stat\nceph osd tree\nceph osd df \nceph osd df tree\nceph osd perf \nceph osd pool stats\nceph osd status \nceph osd utilization\nceph auth list \nceph quorum_status\nceph mon_status\nceph mon dump \nceph pg dump \nceph pg stat\n```\n\nThe following link has details on how to add and remove Ceph storage: \n\n* [https://github.com/rook/rook/blob/master/design/cluster-update.md](https://github.com/rook/rook/blob/master/design/cluster-update.md)\n\nThe following link can be used as reference for backing up and restoring the images stored in the Ceph Pool. \n\n* [https://nicksabine.com/post/ceph-backup/](https://nicksabine.com/post/ceph-backup/)\n\nRelated commands are: \n\n```\nrbd ls -p replicapool\nrbd export \nrbd import \n```\n\nThe aforesaid commands can be run after being attached to the Ceph Monitor pod. ","type":"Mdx","contentDigest":"74aa2266d85687badb3a989c31d05614","owner":"gatsby-plugin-mdx","counter":598},"frontmatter":{"title":"Install and configure Ceph for IBM Cloud Private","description":"Install and configure Ceph for IBM Cloud Private"},"exports":{},"rawBody":"---\ntitle: Install and configure Ceph for IBM Cloud Private\ndescription: Install and configure Ceph for IBM Cloud Private\n---\n\n[Ceph](https://ceph.com/) is open source software designed to provide highly scalable object, block and file-based storage under a unified system.\n\nCeph provides a POSIX-compliant network file system (CephFS) that aims for high performance, large data storage, and maximum compatibility with legacy applications.\n\n[Rook](https://github.com/rook/rook) is an open source orchestrator for distributed storage systems running in cloud native environments.\n\nRook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties.\n\n!!! note\n    The Helm chart *ibm-rook-rbd-cluster* is used for setting up Ceph Cluster in IBM Cloud Private.   \n\n## Environment\n\nA typical IBM Cloud Private Environment includes Boot node, Master node, Management node, Proxy node and Worker nodes. When the Ceph RBD Cluster is used for providing storage for API Connect, any three worker nodes should be configured to have additional raw disks.\n\nThe following set of systems can be used as reference for building *development (non-HA) environment* that runs IBM API Connect workload on IBM Cloud Private.\n\n| Node type | Number of nodes | CPU | Memory (GB) | Disk (GB) |\n| :---: | :---: | :---: | :---: | :---: |\n|\tBoot (FTP Server) | 1\t| 8\t| 32 | 2048 |\n|\tMaster\t| 1\t| 8\t| 32 | 300 |\n|\tManagement | 1\t| 8\t| 32 | 300 |\n|\tProxy\t| 1\t| 4\t| 16 | 300 |\n|\tWorker | 3 | 8 | 32\t| 300+500(disk2)|\n|\tTotal |\t7 | 52 | 208 | 3848+1500(disk2) |\n\nThe following set of systems can be used as reference for building *production (HA) environment* that runs IBM API Connect workload on IBM Cloud Private.\n\n| Node type | Number of nodes | CPU | Memory (GB) | Disk (GB) |\n| :---: | :---: | :---: | :---: | :---: |\n|\tBoot (FTP Server)\t| 1\t| 8\t| 32 | 2048 |\n|\tMaster\t| 3\t| 8\t| 32 | 300 |\n|\tManagement | 2\t| 8\t| 32 | 300 |\n|\tProxy\t| 3\t| 4\t| 16 | 300 |\n|\tWorker  | 3 | 16 | 64 | 300+750(disk2)|\n|\tTotal |\t12\t| 108| 432 | 5348+2250(disk2) |\n\n!!! note\n    Additional worker nodes will be required when there is a a need to run workloads other than IBM API Connect on IBM Cloud Private.\n\n## Setup\n\nThis document covers the setup of *Ceph* storage using *Rook*.\n\nThe following tasks are performed for setting up the Ceph Cluster. \n\n1. [Download the required setup files](#1-download-the-required-setup-files)\n2. [Logon to IBM Cloud Private Cluster](#2-logon-to-ibm-cloud-private-cluster)\n3. [Setup Ceph Cluster](#3-setup-ceph-cluster)\n4. [Verify Ceph cluster](#4-verify-ceph-cluster)\n5. [Troubleshooting Ceph setup](#5-troubleshooting-ceph-setup)\n\n\n### 1. Download the required setup files\n\n!!! note\n    The following files are required for installing *ibm-rook-rbd-cluster* chart and setting up Ceph cluster\n\n- [login.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/scripts/icp/login.sh) - Utility for logging onto IBM Cloud Private\n- [ibm-rook-rbd-cluster-0.8.3.tgz](https://raw.githubusercontent.com/IBM/charts/master/repo/stable/ibm-rook-rbd-cluster-0.8.3.tgz) - IBM Chart for Rook RBD Cluster\n- [ceph-values.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/ceph-values.yaml) - Sample values.yaml for installing Ceph Cluster \n- [rook-ceph-cluster-role-binding.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-ceph-cluster-role-binding.yaml) - ClusterRoleBinding for the service account rook-ceph-cluster\n- [rook-ceph-operator-values.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-ceph-operator-values.yaml) - Sample values.yaml for installing rook operator\n- [rook-cluster-role.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-cluster-role.yaml) - ClusterRole for the resource rook-privileged\n- [rook-pod-security-policy.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/rook-pod-security-policy.yaml) - Define PodSecurityPolicy rook-privileged\n- [setup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/setup.sh) - Utility for setting up Ceph Cluster \n- [status.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/status.sh) - Utility for verifying Ceph Cluster \n- [cleanup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/cleanup.sh) - Utility for cleaning up Ceph Cluster \n\n\n### 2. Logon to IBM Cloud Private Cluster\n\nThe script [login.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/scripts/icp/login.sh) can be run to login to IBM Cloud Private Cluster. \n\n!! note\n    The script should be updated to include the correct value for *CLUSTER_NAME*.\n\nSample run of the login script is as follows: \n\n![](./images/loginToDefaultNamespace.png)\n\n\n### 3. Setup Ceph Cluster \n\n**Step #1** Update the [ceph-values.yaml](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/ceph-values.yaml) to match your environment.\n\nThe file *ceph-values.yaml* needs to be updated to list the IP address of the storage node within the IBM Cloud Private cluster. \n\n```\n...\n#\n# UPDATE VARIABLES TO MATCH THE ENVIRONMENT\n#   \n    nodes:\n    - name: \"X.X.X.X\"\n      devices:\n      - name: \"DISK_NAME\"\n    - name: \"Y.Y.Y.Y\"\n      devices:\n      - name: \"DISK_NAME\"\n    - name: \"Z.Z.Z.Z\"\n      devices:\n      - name: \"DISK_NAME\"\n...\n...\n```\n\n**Step #2** Modify and run the setup script to install Rook Operator chart and the IBM Rook RBD Cluster chart\n\nThe contents of the script [setup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/setup.sh) is as follows: \n\n```\n#\n# UPDATE VARIABLES TO MATCH THE ENVIRONMENT\n#\n\n# Define the location of images \nIMAGE_DIR=/DIRECTORY_HAVING_IMAGES\n\n...\n```\n\n!!! note\n    The script should be updated to include the correct location for *IMAGE_DIR* that has the location where the chart ibm-rook-rbd-cluster-0.8.3.tgz is downloaded and unzipped. \n\nThe output of Ceph install is listed below for reference: \n \n- [ceph_install.log](./samples/ceph_install.log)\n\n\n### 4. Verify Ceph cluster\n\nThe script [status.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/status.sh) can be run to check if Ceph cluster is working as expected. \n\nThe contents of the script [status.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/status.sh) is as follows: \n\n```\n./deployments/ceph/status.sh \n``` \n\nExpected output is listed below. \n\n![](./images/cephGetStatus.png)\n\n\n### 5. Troubleshooting Ceph setup\n\n#### 5.1 Steps for reseting an used disk \n\nIt is possible that sometimes OSD pods does't start up even though the OSD prepare jobs have completed successfully. \nIt could happen when the device you have specified does not have a raw disk and the device name you have listed was used for other storage like GlusterFS cluster.\n\nIn such case the following commands can be run to collect the Logical Volume group ID and Physical volume and remove it fully so that the raw disk is made available for the Ceph cluster.\n\n```\npvs\npvdisplay\nvgremove LOGIOCAL_VOLUME_GROUP_ID -y \npvremove PHYSICAL_VOLUME\n```\n\nThe output of the aforesaid commands is listed below.\n\n```\n[root@rsun-rhel-glusterfs03 ~]# pvs\n  PV         VG                                  Fmt  Attr PSize   PFree  \n  /dev/sda2  rhel                                lvm2 a--   39.00g      0 \n  /dev/sdb   vg_687894352b254c630b291bf094a8d43d lvm2 a--  499.87g 499.87g\n  /dev/sdc   rhel                                lvm2 a--  500.00g      0 \n[root@rsun-rhel-glusterfs03 ~]# pvdisplay\n  --- Physical volume ---\n  PV Name               /dev/sdb\n  VG Name               vg_687894352b254c630b291bf094a8d43d\n  PV Size               500.00 GiB / not usable 132.00 MiB\n  Allocatable           yes \n  PE Size               4.00 MiB\n  Total PE              127967\n  Free PE               127967\n  Allocated PE          0\n  PV UUID               v6xOuh-M2ot-oXfl-IWyf-TnYL-nX3a-kzqizN\n   \n  --- Physical volume ---\n  PV Name               /dev/sda2\n  VG Name               rhel\n  PV Size               39.00 GiB / not usable 3.00 MiB\n  Allocatable           yes (but full)\n  PE Size               4.00 MiB\n  Total PE              9983\n  Free PE               0\n  Allocated PE          9983\n  PV UUID               tNjUif-RlBT-kdDn-PWwE-LHlq-3w9O-65Hlph\n   \n  --- Physical volume ---\n  PV Name               /dev/sdc\n  VG Name               rhel\n  PV Size               500.00 GiB / not usable 4.00 MiB\n  Allocatable           yes (but full)\n  PE Size               4.00 MiB\n  Total PE              127999\n  Free PE               0\n  Allocated PE          127999\n  PV UUID               7CXpz5-95hb-0WAC-3Efe-XrY1-s6E6-dqLasC\n   \n[root@rsun-rhel-glusterfs03 ~]# vgremove vg_687894352b254c630b291bf094a8d43d -y \n  Volume group \"vg_687894352b254c630b291bf094a8d43d\" successfully removed\n[root@rsun-rhel-glusterfs03 ~]# pvremove /dev/sdb \n  Labels on physical volume \"/dev/sdb\" successfully wiped.\n```\n\n#### 5.2 Steps for uninstalling the rook-ceph setup \n\n**Step #1** The script [cleanup.sh](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/ceph/cleanup.sh) can be run to remove the Ceph setup completely.\n\n```\n./deployments/ceph/cleanup.sh\n``` \n\n**Step #2** Remove the contents of the temporary directory used by rook: */var/lib/rook*\n\nThe following command should run on all the worker nodes: \n\n```\nrm -fr /var/lib/rook\n```\n\n#  Ceph Cluster Management\n\nThe following links has additional details on how to diagnose, troubleshoot, monitor and report Ceph cluster storage: \n\n* [https://github.com/rook/rook/tree/master/Documentation](https://github.com/rook/rook/tree/master/Documentation)\n* [https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques](https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques)\n* [https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/](https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/)\n* [https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know](https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know)\n* [https://sabaini.at/pages/ceph-cheatsheet.html](https://sabaini.at/pages/ceph-cheatsheet.html)\n\nThe *Ceph Monitor* pod can be attached using the following command:\n\n```\nkubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" -o jsonpath='{.items[0].metadata.name}') bash\n```\n\nAfter being attached to the *Ceph Monitor* pod, the following commands can be run which provides status and statistics of the *Ceph Cluster*.\n\n```\nceph health \nceph status \nceph df \nceph osd stat\nceph osd tree\nceph osd df \nceph osd df tree\nceph osd perf \nceph osd pool stats\nceph osd status \nceph osd utilization\nceph auth list \nceph quorum_status\nceph mon_status\nceph mon dump \nceph pg dump \nceph pg stat\n```\n\nThe following link has details on how to add and remove Ceph storage: \n\n* [https://github.com/rook/rook/blob/master/design/cluster-update.md](https://github.com/rook/rook/blob/master/design/cluster-update.md)\n\nThe following link can be used as reference for backing up and restoring the images stored in the Ceph Pool. \n\n* [https://nicksabine.com/post/ceph-backup/](https://nicksabine.com/post/ceph-backup/)\n\nRelated commands are: \n\n```\nrbd ls -p replicapool\nrbd export \nrbd import \n```\n\nThe aforesaid commands can be run after being attached to the Ceph Monitor pod. ","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/aaa_ORPHANS/deployments/eventstreams/Install_Ceph_on_ICP.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}