{"componentChunkName":"component---src-pages-aaa-orphans-training-eda-skill-journey-mdx","path":"/aaa_ORPHANS/training/eda-skill-journey/","result":{"pageContext":{"frontmatter":{"title":"EDA Skill Journey","description":"EDA Skill Journey"},"relativePagePath":"/aaa_ORPHANS/training/eda-skill-journey.mdx","titleType":"append","MdxNode":{"id":"142c35fd-2958-5ee5-afc2-41900230fd39","children":[],"parent":"0277cca2-c7e4-5d6a-b3ff-c059abf420a6","internal":{"content":"---\ntitle: EDA Skill Journey\ndescription: EDA Skill Journey\n---\n\nImplementing cloud native, event-driven solution with microservices deployed on kubernetes involves a broad skill set. In this article, we are proposing a deep dive learning journey for developers to adopt event-driven microservice implementation.\n\nThis EDA project includes best practices around the technologies used in any event-driven microservice solution implementation. Our [Reefer solution implementation](https://ibm-cloud-architecture.github.io/refarch-kc) tries to illustrate most of those patterns. It includes a set of technologies that represent the modern landscape of cloud native applications (Kafka, maven, java, microprofile, kafka API, Kafka Stream API, Spring boot, Python, Nodejs, and Postgresql) but also some specific analytics and AI components like IBM Streams analytics and machine learning with Jupyter notebook to develop predictive scoring model.\n\nA developer who wants to consume this content does not need to know everything at the expert level. You can progress steps by steps and it will take a 2 to 3 weeks to digest everything.\n\n!!! note\n    We expect you have some good understanding of the following technologies:\n\n    * Nodejs / Javascript / Typescripts\n    * Java 1.8 amd microprofile architecture\n    * Python 3.6\n    * Angular 7, HTML, CSS  - This is for the user interface but this is more optional.\n    * Maven, npm, bash\n    * WebSphere Liberty or OpenLiberty\n    * Docker\n    * Docker compose\n    * Helm\n    * Kubernetes\n    * Apache Kafka, Kafka API\n\n    We have build a getting started and tutorial list for you to study [here](core-techno-getstarted.md).\n\n## Event Driven Concepts\n\nNow the development of event driven solution involves specific technologies and practices. The following links should be studied in the proposed order:\n\n* [Why Event Driven Architecture now?](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture)\n\n### 1- Reading: Understand EDA fundamentals (8 hours)\n\nThe following content is for architects, and developers who want to understand the technologies and capabilities of an event driven architecture.\n\n* Understand the [Key EDA concepts](../concepts/README.md) like events, event streams, events and messages differences...\n* Be confortable with the [EDA reference architecture with event backbone, microservices and real time analytics](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/reference-architecture).\n* Which is extended [with machine learning and AI integrated with real time analytics reference architecture](https://www.ibm.com/cloud/garage/architectures/eventDrivenExtendedArchitecture), integrating machine learning workbench and event sourcing as data source, and real time analytics for deployment.\n* Review [Event sources - as event producers article](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-event-sources).\n* Read the concept of [Event backbone where Kafka is the main implementation](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-event-backbone).\n* As kafka is the event backbone, review its [key concepts and building blocks](../kafka/readme.md) and then review how to support [high availability (HA) and disaster recovery (DR) with IBM Event Streams or Kafka Architecture Considerations](../kafka/arch.md).\n* Review one of the common industry use case using Kafka, to perform [data replication using kafka and Change Data Capture](https://ibm-cloud-architecture.github.io/refarch-data-ai-analytics/preparation/data-replication/).\n\n\n### 2- Hands on Lab: Getting started with Event Streams and Kafka (3 hours)\n\n* Start by creating an Event Stream service in IBM Cloud by performing the [IBM Event Stream Getting started](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-getting_started&locale=en-US). To provision your service, go to the IBM Cloud Catalog and search for `Event Streams`. It is in the *Integration* category. Create the service and specify a name, a region/location (select the same as your cluster), and a resource group, add a tag if you want to, then select the standard plan.\n\n!!! warning\n    If you are using a non default resource group, you need to be sure your userid as editor role to the resource group to be able to create service under the resource group.\n\n![](../deployments/eventstreams/images/IES-service.png)\n\nSee [this note](../deployments/eventstreams//es-ibm-cloud.md) for creating Event Stream with Enterprise plan.\n\n* Review another getting started from [Apache Kafka quickstart](https://kafka.apache.org/quickstart)\n* Finally review the Confluent point of view on [getting started](https://www.confluent.io/blog/apache-kafka-getting-started/) as it covers some of the persona involved in a kafka based solution.\n\n### 3 - Readings : Methodology (2 hours read)\n\nIt is important to understand how to start a project using event. This lab persents how to start an event driven solution implementation using the event storming and domain driven design:\n\n* We are proposing a [set of activities](../methodology/readme.md) to develop and release an event-driven solution based on the agile development approach described in the IBM Garage method.\n* The adopted approach for event identification is the [Event storming methodology](https://ibm-cloud-architecture.github.io/refarch-eda/methodology/readme/) introduced by Alberto Brandolini in \"Introducing event storming book\". We have extended it with the event insight practice used to identify potential real time analytics use cases.\n* Once the event storming deliver events, commands, aggregates we can start doing some [Domain Design Driven](../methodology/ddd/) exercises and apply it to the business application.\n\nFor those of you, who are interested to know how to apply the event storming and domain driven design methodology to the Reefer shipment process,  you can review the following articles:\n\n* [The solution introduction](https://ibm-cloud-architecture.github.io/refarch-kc/introduction) to get a sense of the goals of this application. (10 minutes read)\n* followed by the [event storming analysis report](https://ibm-cloud-architecture.github.io/refarch-kc/analysis/readme/) (30 minutes read).\n* and [the derived design](https://ibm-cloud-architecture.github.io/refarch-kc/design/readme/) from this analysis. (15 minutes reading)\n\n---\n\n## Hands-on labs\n\nAs next steps beyond getting started and reading our technical point of view, you can try different hands-on exercises based on our [\"Reefer container shipment solution\"](https://ibm-cloud-architecture.github.io/refarch-kc). This application is quite complex and includes different components. You do not need to do all, but you should get a good understanding of all those component implementation as most of the code and approach is reusable for your future implementation.\n\n!!! note\n        At the end of this training you should have the following solution up and running (See detailed description [here](https://ibm-cloud-architecture.github.io/refarch-kc/design/architecture/#components-view)):\n\n    ![](../images/kc-mvp-components.png)\n\nYou can run the solution locally, on IBM Cloud Private, on IBM Kubernetes Services or Openshift.\n\n\n### 4 - Hands on lab: Prepare your local environment (30 mn)\n\n!!! goals\n    Install Kafka - zookeeper and postgresql docker images and start them in docker-compose or minikube environment\n\nFirst be sure to complete the pre-requisites by following [those steps](ttps://ibm-cloud-architecture.github.io/refarch-kc/pre-requisites.md).\n\nThen do one of the following choice:\n\n1. To run a local Kafka / zookeeper backbone using docker compose, in less than 3 minutes follow [the steps described in this note](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/local/#start-kafka-and-zookeeper).\n1. Or use [Minikube/ docker kubernetes](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/minikube/#pre-requisites) to get kafka, zookeeper and postgreSQl up and running on a unique node kubernetes cluster.\n\n### 5 - Hands on lab: Prepare IBM Cloud IKS Openshift environment\n\nYou can run the solution locally, but you can also deploy it to a kubernetes cluster. So this lab is more optional or you can do it later when you are familar with the solution. If you want to build and run the solution locally go to step 8 below.\n\nSo if you are interested by a public cloud deployment, you can follow this tutorial: [\"Creating an IBM Cloud Red Hat OpenShift Container Platform cluster\"](https://cloud.ibm.com/docs/containers?topic=containers-openshift_tutorial).\n\nBe sure to have administration privilege, within your account, to be able to create cluster. It will take less than 30 minutes to get the cluster provisioned.\n\nYou can follow the steps to create the cluster with the console or use the IBM Cloud CLI.\n\n1. Be sure to be logged to your IBM Cloud account:\n\n    ```\n    ibmcloud login -a https://cloud.ibm.com  -u <userid> -p <password> -c <accoundID>\n    ```\n\n1. Get the private and public vlan IP address for your zone:\n\n    ```\n    ibmcloud ks vlans --zone wdc06\n    ```\n\n    It will return something like\n\n    ```\n    ID        Name                     Number   Type      Router         Supports Virtual Workers\n    <private_VLAN_ID to keep secret>          2445     private   bcr01a.wdc06   true\n    <public_VLAN_ID to keep secret>           1305    public    fcr01a.wdc06   true\n\n    ```\n\n1. Create a 3 nodes kubernetes cluster using the small hardware footprint, and openshift 3.11 image:\n\n    ```\n    ibmcloud ks cluster-create --name greencluster --location wdc06 --kube-version 3.11_openshift --machine-type b3c.4x16.encrypted  --workers 3 --public-vlan <public_VLAN_ID> --private-vlan <private_VLAN_ID>\n    ```\n\n1. Verify your cluster once created:\n\n    ```\n    ibmcloud ks cluster-get --cluster  greencluster\n    ```\n\n    ```\n    Retrieving cluster greencluster...\n    OK\n\n    Name:                           greencluster\n    ID:                             <keep it secret>\n    State:                          normal\n    Created:                        2019-07-16T20:47:34+0000\n    Location:                       wdc06\n    Master URL:                     https://<secret_too>.us-east.containers.cloud.ibm.com:21070\n    Public Service Endpoint URL:    https://<secret_too>.us-east.containers.cloud.ibm.com:21070\n    Private Service Endpoint URL:   -\n    Master Location:                Washington D.C.\n    Master Status:                  Ready (2 days ago)\n    Master State:                   deployed\n    Master Health:                  normal\n    Ingress Subdomain:              greencluster.us-east.containers.appdomain.cloud\n    Ingress Secret:                 greencluster\n    Workers:                        3\n    Worker Zones:                   wdc06\n    Version:                        3.11.104_1507_openshift\n    Owner:                          <secret_too>\n    Monitoring Dashboard:           -\n    Resource Group ID:              <secret_too>\n    Resource Group Name:            default\n\n    ```\n\n1. Download the configuration files to connect to your cluster\n\n    ```\n    ibmcloud ks cluster-config --cluster greencluster\n    ```\n\n    Then export the KUBECONFIG variable.\n\n    ```\n    export KUBECONFIG=/Users/<you on your computer>/.bluemix/plugins/container-service/clusters/greencluster/kube-config-wdc06-greencluster.yml\n    ```\n    Now any `oc` command will work against your remote cluster.\n\n1. Access the Openshift container platform console using the master URL\n\n    Something like: https://<secret_too>.us-east.containers.cloud.ibm.com:21070\n\n    ![](os-console.png)\n\n### 6 - Hands on lab: Create Kafka topics and get service credentials\n\nFrom the Event Stream services you created in [Lab 2](#lab-2-getting-started-with-event-streams-and-kafka-3-hours) go to the service credentials page, create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumer or producer code.\n\n ![](../deployments/eventstreams/images/IES-IC-credentials.png)\n\nThe Event streams broker API key is needed to connect any consumers or producers to access the service in IBM Cloud.\n\nWhen those producers and consumers are deployed to kubernetes, the way to share security keys, is to define a kubernetes secret and deploy it to the IKS cluster.\n\n* Define a Event Stream API key secret: to configure a secret under the `greencompute` namespace.\n\n    For kubernetes platform:\n\n    ```shell\n    kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n greencompute\n    ```\n\n    For Openshift use the name of the project as namespace\n    ```\n    kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n reefer-shipment-solution\n    ```\n\n* Verify the secrets:\n\n    ```\n    kubectl describe secrets -n greencompute\n    ```\n\n    or for openshift using project:\n\n    ```\n    kubectl describe secrets -n reefer-shipment-solution\n    ```\n\nThis secret is used by all the solution microservices which are using Kafka / Event Streams. The detail of how we use it with environment variables, is described in one of the project [here.](https://github.com/ibm-cloud-architecture/refarch-kc-ms/blob/master/fleet-ms/README.md#run-on-ibm-cloud-with-kubernetes-service)\n\n* Finally in the *Manage* panel add the topics needed for the solution. We need at least the following:\n\n ![](../deployments/eventstreams/images/IES-IC-topics.png)\n\n\n!!! Note\n        In your local deployment the kafka topics are created automatically using the launch script.\n\n\n### 7 - Hands on lab: Get a simple getting started event producer deployed on openshift with Event Stream\n\n!!! Note\n        For local deployment, [smoke test](#perform-the-smoke-tests-locally) scripts helps to show the event produced and consumed.\n\n\nTo validate the IBM Event Stream and the openshift app are communicating, we use a simple order producer app we have developed as part of the Reefer container solution. It is done with python and can be found in [this repository](https://github.com/jbcodeforce/order-producer-python). We use Openshift source to image workflow to deploy this app to Openshift. Follow the steps in the readme as part of the lab.\n\n![](order-producer-python.png)\n\nThere are other tools to use to quickly\n\n### 8 - Hands on lab: Build and run the solution locally\n\n!!! goals\n    Build and run the solution so you can understand the Java-maven, Nodejs build process with docker stage build.\n\n* [Build and deploy the solution locally using docker compose](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/local/)\n\n\n### 9 - Hands on: Perform tests\n\n#### Perform the smoke tests locally\n\nFinally, run the smoke tests to assess all the components are running well. For that in the `refarch-kc` project run the script:\n\nFor docker-compose:\n```\n./scripts/smokeTests.sh LOCAL\n```\n\nfor MINIKUBE:\n\n```\n./scripts/smokeTests.sh MINIKUBE\n```\n\nYou should see an Order created for the \"GoodManuf\" customer. Then the order is visible in the command and the query microservices.\n\n!!! warning\n    To stop docker-compose deployment use the following command:\n    ```\n    ./scripts/stopLocalEnv.sh  LOCAL\n    ```\n    or for the minikube\n    ```\n    stopLocalEnv.sh  MINIKUBE\n    ```\n\n#### Optional: Execute integration tests on the local environment\n\n [Execute the integration tests](https://ibm-cloud-architecture.github.io/refarch-kc/itg-tests/) to validate the solution end to end.\n\n#### Optional: Execute the demonstration script\n\n[Execute the demonstration script](https://ibm-cloud-architecture.github.io/refarch-kc/demo/readme/)\n\n\n### 10 - Reading: Review the CQRS patterns implementation\n\n* Read [Event driven design patterns for microservice](../design-patterns/ED-patterns.md) with the Command Query Responsability Segregation, event sourcing and saga patterns.\n\n* Review the [Event sourcing design pattern explanations](../design-patterns/event-sourcing.md) and how it is tested with some integration tests:\n\n* Review the [CQRS pattern](../design-patterns/cqrs.md).\n\n* Review the CQRS code in the [order management microservice implementation](https://ibm-cloud-architecture.github.io/refarch-kc-order-ms)\n\n* [Kafka Python API](https://github.com/confluentinc/confluent-kafka-python) and some examples in our [integration tests project](https://ibm-cloud-architecture.github.io/refarch-kc/itg-tests/)\n* [Kafka Nodejs API used in the voyage microservice](https://ibm-cloud-architecture.github.io/refarch-kc-ms/voyagems/)\n\n\n### Lab 11: Run the solution on IBM Cloud\n\n* [Deploying the solution on IBM Cloud Kubernetes Service](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/iks)\n\nPerform smokeTests locally on the solution running on IKS.\n\n### Lab 12: Optional - Run the solution on IBM Cloud Private\n\n* [Deploying the solution on IBM Cloud Private](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/icp)\n\n### Lab 13: Data replication with Kafka\n\nOne of the common usage of using Kafka is to combine it with a Change Data Capture component to get update from a \"legacy\" data base to the new microservice runtime environment.\n\nWe are detailing an approach in [this article](https://ibm-cloud-architecture.github.io/refarch-data-ai-analytics/preparation/data-replication/).\n\n### Lab 14: Real time analytics and Machine learning\n\n* [IBM Cloud Streaming Analytics introduction](https://cloud.ibm.com/catalog/services/streaming-analytics) and [getting started](https://cloud.ibm.com/docs/services/StreamingAnalytics?topic=StreamingAnalytics-gettingstarted#gettingstarted)\n\n* [Apply predictive analytics on container metrics for predictive maintenance use case](https://ibm-cloud-architecture.github.io/refarch-kc-container-ms/metrics/)\n\n\n### Other subjects\n\n* [Develop a toolchain for one of the container manager service](https://ibm-cloud-architecture.github.io/refarch-kc-container-ms/cicd/)\n* [Our Kubernetes troubleshooting notes](https://github.com/ibm-cloud-architecture/refarch-integration/blob/master/docs/icp/troubleshooting.md)\n\n* [Kafka monitoring](../kafka/monitoring.md)\n\n\n* [IBM Event Streams - stream analytics app](https://developer.ibm.com/streamsdev/docs/detect-events-with-streams/) Event detection on continuous feed using Streaming Analytics in IBM Cloud.\n\n* Read how to [process continuous streaming events](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-event-streams)\n* [Event-driven cloud-native applications](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-cloud-native-apps)\n\n* The Cloud Private [IBM Event Streams](https://www.ibm.com/cloud/event-streams) product running on  private cloud\n* Read introduction to [act on events with IBM Cloud Functions](../evt-action/README.md)\n\n## Slack channel\n\nContact us on '#eda-ac` channel under the [ibmcase.slack.com](http://ibmcase.slack.com) workspace.\n","type":"Mdx","contentDigest":"7620f123f94e966908b0a0a6cb073e4f","counter":588,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: EDA Skill Journey\ndescription: EDA Skill Journey\n---\n\nImplementing cloud native, event-driven solution with microservices deployed on kubernetes involves a broad skill set. In this article, we are proposing a deep dive learning journey for developers to adopt event-driven microservice implementation.\n\nThis EDA project includes best practices around the technologies used in any event-driven microservice solution implementation. Our [Reefer solution implementation](https://ibm-cloud-architecture.github.io/refarch-kc) tries to illustrate most of those patterns. It includes a set of technologies that represent the modern landscape of cloud native applications (Kafka, maven, java, microprofile, kafka API, Kafka Stream API, Spring boot, Python, Nodejs, and Postgresql) but also some specific analytics and AI components like IBM Streams analytics and machine learning with Jupyter notebook to develop predictive scoring model.\n\nA developer who wants to consume this content does not need to know everything at the expert level. You can progress steps by steps and it will take a 2 to 3 weeks to digest everything.\n\n!!! note\n    We expect you have some good understanding of the following technologies:\n\n    * Nodejs / Javascript / Typescripts\n    * Java 1.8 amd microprofile architecture\n    * Python 3.6\n    * Angular 7, HTML, CSS  - This is for the user interface but this is more optional.\n    * Maven, npm, bash\n    * WebSphere Liberty or OpenLiberty\n    * Docker\n    * Docker compose\n    * Helm\n    * Kubernetes\n    * Apache Kafka, Kafka API\n\n    We have build a getting started and tutorial list for you to study [here](core-techno-getstarted.md).\n\n## Event Driven Concepts\n\nNow the development of event driven solution involves specific technologies and practices. The following links should be studied in the proposed order:\n\n* [Why Event Driven Architecture now?](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture)\n\n### 1- Reading: Understand EDA fundamentals (8 hours)\n\nThe following content is for architects, and developers who want to understand the technologies and capabilities of an event driven architecture.\n\n* Understand the [Key EDA concepts](../concepts/README.md) like events, event streams, events and messages differences...\n* Be confortable with the [EDA reference architecture with event backbone, microservices and real time analytics](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/reference-architecture).\n* Which is extended [with machine learning and AI integrated with real time analytics reference architecture](https://www.ibm.com/cloud/garage/architectures/eventDrivenExtendedArchitecture), integrating machine learning workbench and event sourcing as data source, and real time analytics for deployment.\n* Review [Event sources - as event producers article](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-event-sources).\n* Read the concept of [Event backbone where Kafka is the main implementation](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-event-backbone).\n* As kafka is the event backbone, review its [key concepts and building blocks](../kafka/readme.md) and then review how to support [high availability (HA) and disaster recovery (DR) with IBM Event Streams or Kafka Architecture Considerations](../kafka/arch.md).\n* Review one of the common industry use case using Kafka, to perform [data replication using kafka and Change Data Capture](https://ibm-cloud-architecture.github.io/refarch-data-ai-analytics/preparation/data-replication/).\n\n\n### 2- Hands on Lab: Getting started with Event Streams and Kafka (3 hours)\n\n* Start by creating an Event Stream service in IBM Cloud by performing the [IBM Event Stream Getting started](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-getting_started&locale=en-US). To provision your service, go to the IBM Cloud Catalog and search for `Event Streams`. It is in the *Integration* category. Create the service and specify a name, a region/location (select the same as your cluster), and a resource group, add a tag if you want to, then select the standard plan.\n\n!!! warning\n    If you are using a non default resource group, you need to be sure your userid as editor role to the resource group to be able to create service under the resource group.\n\n![](../deployments/eventstreams/images/IES-service.png)\n\nSee [this note](../deployments/eventstreams//es-ibm-cloud.md) for creating Event Stream with Enterprise plan.\n\n* Review another getting started from [Apache Kafka quickstart](https://kafka.apache.org/quickstart)\n* Finally review the Confluent point of view on [getting started](https://www.confluent.io/blog/apache-kafka-getting-started/) as it covers some of the persona involved in a kafka based solution.\n\n### 3 - Readings : Methodology (2 hours read)\n\nIt is important to understand how to start a project using event. This lab persents how to start an event driven solution implementation using the event storming and domain driven design:\n\n* We are proposing a [set of activities](../methodology/readme.md) to develop and release an event-driven solution based on the agile development approach described in the IBM Garage method.\n* The adopted approach for event identification is the [Event storming methodology](https://ibm-cloud-architecture.github.io/refarch-eda/methodology/readme/) introduced by Alberto Brandolini in \"Introducing event storming book\". We have extended it with the event insight practice used to identify potential real time analytics use cases.\n* Once the event storming deliver events, commands, aggregates we can start doing some [Domain Design Driven](../methodology/ddd/) exercises and apply it to the business application.\n\nFor those of you, who are interested to know how to apply the event storming and domain driven design methodology to the Reefer shipment process,  you can review the following articles:\n\n* [The solution introduction](https://ibm-cloud-architecture.github.io/refarch-kc/introduction) to get a sense of the goals of this application. (10 minutes read)\n* followed by the [event storming analysis report](https://ibm-cloud-architecture.github.io/refarch-kc/analysis/readme/) (30 minutes read).\n* and [the derived design](https://ibm-cloud-architecture.github.io/refarch-kc/design/readme/) from this analysis. (15 minutes reading)\n\n---\n\n## Hands-on labs\n\nAs next steps beyond getting started and reading our technical point of view, you can try different hands-on exercises based on our [\"Reefer container shipment solution\"](https://ibm-cloud-architecture.github.io/refarch-kc). This application is quite complex and includes different components. You do not need to do all, but you should get a good understanding of all those component implementation as most of the code and approach is reusable for your future implementation.\n\n!!! note\n        At the end of this training you should have the following solution up and running (See detailed description [here](https://ibm-cloud-architecture.github.io/refarch-kc/design/architecture/#components-view)):\n\n    ![](../images/kc-mvp-components.png)\n\nYou can run the solution locally, on IBM Cloud Private, on IBM Kubernetes Services or Openshift.\n\n\n### 4 - Hands on lab: Prepare your local environment (30 mn)\n\n!!! goals\n    Install Kafka - zookeeper and postgresql docker images and start them in docker-compose or minikube environment\n\nFirst be sure to complete the pre-requisites by following [those steps](ttps://ibm-cloud-architecture.github.io/refarch-kc/pre-requisites.md).\n\nThen do one of the following choice:\n\n1. To run a local Kafka / zookeeper backbone using docker compose, in less than 3 minutes follow [the steps described in this note](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/local/#start-kafka-and-zookeeper).\n1. Or use [Minikube/ docker kubernetes](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/minikube/#pre-requisites) to get kafka, zookeeper and postgreSQl up and running on a unique node kubernetes cluster.\n\n### 5 - Hands on lab: Prepare IBM Cloud IKS Openshift environment\n\nYou can run the solution locally, but you can also deploy it to a kubernetes cluster. So this lab is more optional or you can do it later when you are familar with the solution. If you want to build and run the solution locally go to step 8 below.\n\nSo if you are interested by a public cloud deployment, you can follow this tutorial: [\"Creating an IBM Cloud Red Hat OpenShift Container Platform cluster\"](https://cloud.ibm.com/docs/containers?topic=containers-openshift_tutorial).\n\nBe sure to have administration privilege, within your account, to be able to create cluster. It will take less than 30 minutes to get the cluster provisioned.\n\nYou can follow the steps to create the cluster with the console or use the IBM Cloud CLI.\n\n1. Be sure to be logged to your IBM Cloud account:\n\n    ```\n    ibmcloud login -a https://cloud.ibm.com  -u <userid> -p <password> -c <accoundID>\n    ```\n\n1. Get the private and public vlan IP address for your zone:\n\n    ```\n    ibmcloud ks vlans --zone wdc06\n    ```\n\n    It will return something like\n\n    ```\n    ID        Name                     Number   Type      Router         Supports Virtual Workers\n    <private_VLAN_ID to keep secret>          2445     private   bcr01a.wdc06   true\n    <public_VLAN_ID to keep secret>           1305    public    fcr01a.wdc06   true\n\n    ```\n\n1. Create a 3 nodes kubernetes cluster using the small hardware footprint, and openshift 3.11 image:\n\n    ```\n    ibmcloud ks cluster-create --name greencluster --location wdc06 --kube-version 3.11_openshift --machine-type b3c.4x16.encrypted  --workers 3 --public-vlan <public_VLAN_ID> --private-vlan <private_VLAN_ID>\n    ```\n\n1. Verify your cluster once created:\n\n    ```\n    ibmcloud ks cluster-get --cluster  greencluster\n    ```\n\n    ```\n    Retrieving cluster greencluster...\n    OK\n\n    Name:                           greencluster\n    ID:                             <keep it secret>\n    State:                          normal\n    Created:                        2019-07-16T20:47:34+0000\n    Location:                       wdc06\n    Master URL:                     https://<secret_too>.us-east.containers.cloud.ibm.com:21070\n    Public Service Endpoint URL:    https://<secret_too>.us-east.containers.cloud.ibm.com:21070\n    Private Service Endpoint URL:   -\n    Master Location:                Washington D.C.\n    Master Status:                  Ready (2 days ago)\n    Master State:                   deployed\n    Master Health:                  normal\n    Ingress Subdomain:              greencluster.us-east.containers.appdomain.cloud\n    Ingress Secret:                 greencluster\n    Workers:                        3\n    Worker Zones:                   wdc06\n    Version:                        3.11.104_1507_openshift\n    Owner:                          <secret_too>\n    Monitoring Dashboard:           -\n    Resource Group ID:              <secret_too>\n    Resource Group Name:            default\n\n    ```\n\n1. Download the configuration files to connect to your cluster\n\n    ```\n    ibmcloud ks cluster-config --cluster greencluster\n    ```\n\n    Then export the KUBECONFIG variable.\n\n    ```\n    export KUBECONFIG=/Users/<you on your computer>/.bluemix/plugins/container-service/clusters/greencluster/kube-config-wdc06-greencluster.yml\n    ```\n    Now any `oc` command will work against your remote cluster.\n\n1. Access the Openshift container platform console using the master URL\n\n    Something like: https://<secret_too>.us-east.containers.cloud.ibm.com:21070\n\n    ![](os-console.png)\n\n### 6 - Hands on lab: Create Kafka topics and get service credentials\n\nFrom the Event Stream services you created in [Lab 2](#lab-2-getting-started-with-event-streams-and-kafka-3-hours) go to the service credentials page, create new credentials to get the Kafka brokers list, the admim URL and the api_key needed to authenticate the consumer or producer code.\n\n ![](../deployments/eventstreams/images/IES-IC-credentials.png)\n\nThe Event streams broker API key is needed to connect any consumers or producers to access the service in IBM Cloud.\n\nWhen those producers and consumers are deployed to kubernetes, the way to share security keys, is to define a kubernetes secret and deploy it to the IKS cluster.\n\n* Define a Event Stream API key secret: to configure a secret under the `greencompute` namespace.\n\n    For kubernetes platform:\n\n    ```shell\n    kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n greencompute\n    ```\n\n    For Openshift use the name of the project as namespace\n    ```\n    kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n reefer-shipment-solution\n    ```\n\n* Verify the secrets:\n\n    ```\n    kubectl describe secrets -n greencompute\n    ```\n\n    or for openshift using project:\n\n    ```\n    kubectl describe secrets -n reefer-shipment-solution\n    ```\n\nThis secret is used by all the solution microservices which are using Kafka / Event Streams. The detail of how we use it with environment variables, is described in one of the project [here.](https://github.com/ibm-cloud-architecture/refarch-kc-ms/blob/master/fleet-ms/README.md#run-on-ibm-cloud-with-kubernetes-service)\n\n* Finally in the *Manage* panel add the topics needed for the solution. We need at least the following:\n\n ![](../deployments/eventstreams/images/IES-IC-topics.png)\n\n\n!!! Note\n        In your local deployment the kafka topics are created automatically using the launch script.\n\n\n### 7 - Hands on lab: Get a simple getting started event producer deployed on openshift with Event Stream\n\n!!! Note\n        For local deployment, [smoke test](#perform-the-smoke-tests-locally) scripts helps to show the event produced and consumed.\n\n\nTo validate the IBM Event Stream and the openshift app are communicating, we use a simple order producer app we have developed as part of the Reefer container solution. It is done with python and can be found in [this repository](https://github.com/jbcodeforce/order-producer-python). We use Openshift source to image workflow to deploy this app to Openshift. Follow the steps in the readme as part of the lab.\n\n![](order-producer-python.png)\n\nThere are other tools to use to quickly\n\n### 8 - Hands on lab: Build and run the solution locally\n\n!!! goals\n    Build and run the solution so you can understand the Java-maven, Nodejs build process with docker stage build.\n\n* [Build and deploy the solution locally using docker compose](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/local/)\n\n\n### 9 - Hands on: Perform tests\n\n#### Perform the smoke tests locally\n\nFinally, run the smoke tests to assess all the components are running well. For that in the `refarch-kc` project run the script:\n\nFor docker-compose:\n```\n./scripts/smokeTests.sh LOCAL\n```\n\nfor MINIKUBE:\n\n```\n./scripts/smokeTests.sh MINIKUBE\n```\n\nYou should see an Order created for the \"GoodManuf\" customer. Then the order is visible in the command and the query microservices.\n\n!!! warning\n    To stop docker-compose deployment use the following command:\n    ```\n    ./scripts/stopLocalEnv.sh  LOCAL\n    ```\n    or for the minikube\n    ```\n    stopLocalEnv.sh  MINIKUBE\n    ```\n\n#### Optional: Execute integration tests on the local environment\n\n [Execute the integration tests](https://ibm-cloud-architecture.github.io/refarch-kc/itg-tests/) to validate the solution end to end.\n\n#### Optional: Execute the demonstration script\n\n[Execute the demonstration script](https://ibm-cloud-architecture.github.io/refarch-kc/demo/readme/)\n\n\n### 10 - Reading: Review the CQRS patterns implementation\n\n* Read [Event driven design patterns for microservice](../design-patterns/ED-patterns.md) with the Command Query Responsability Segregation, event sourcing and saga patterns.\n\n* Review the [Event sourcing design pattern explanations](../design-patterns/event-sourcing.md) and how it is tested with some integration tests:\n\n* Review the [CQRS pattern](../design-patterns/cqrs.md).\n\n* Review the CQRS code in the [order management microservice implementation](https://ibm-cloud-architecture.github.io/refarch-kc-order-ms)\n\n* [Kafka Python API](https://github.com/confluentinc/confluent-kafka-python) and some examples in our [integration tests project](https://ibm-cloud-architecture.github.io/refarch-kc/itg-tests/)\n* [Kafka Nodejs API used in the voyage microservice](https://ibm-cloud-architecture.github.io/refarch-kc-ms/voyagems/)\n\n\n### Lab 11: Run the solution on IBM Cloud\n\n* [Deploying the solution on IBM Cloud Kubernetes Service](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/iks)\n\nPerform smokeTests locally on the solution running on IKS.\n\n### Lab 12: Optional - Run the solution on IBM Cloud Private\n\n* [Deploying the solution on IBM Cloud Private](https://ibm-cloud-architecture.github.io/refarch-kc/deployments/icp)\n\n### Lab 13: Data replication with Kafka\n\nOne of the common usage of using Kafka is to combine it with a Change Data Capture component to get update from a \"legacy\" data base to the new microservice runtime environment.\n\nWe are detailing an approach in [this article](https://ibm-cloud-architecture.github.io/refarch-data-ai-analytics/preparation/data-replication/).\n\n### Lab 14: Real time analytics and Machine learning\n\n* [IBM Cloud Streaming Analytics introduction](https://cloud.ibm.com/catalog/services/streaming-analytics) and [getting started](https://cloud.ibm.com/docs/services/StreamingAnalytics?topic=StreamingAnalytics-gettingstarted#gettingstarted)\n\n* [Apply predictive analytics on container metrics for predictive maintenance use case](https://ibm-cloud-architecture.github.io/refarch-kc-container-ms/metrics/)\n\n\n### Other subjects\n\n* [Develop a toolchain for one of the container manager service](https://ibm-cloud-architecture.github.io/refarch-kc-container-ms/cicd/)\n* [Our Kubernetes troubleshooting notes](https://github.com/ibm-cloud-architecture/refarch-integration/blob/master/docs/icp/troubleshooting.md)\n\n* [Kafka monitoring](../kafka/monitoring.md)\n\n\n* [IBM Event Streams - stream analytics app](https://developer.ibm.com/streamsdev/docs/detect-events-with-streams/) Event detection on continuous feed using Streaming Analytics in IBM Cloud.\n\n* Read how to [process continuous streaming events](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-event-streams)\n* [Event-driven cloud-native applications](https://www.ibm.com/cloud/garage/architectures/eventDrivenArchitecture/event-driven-cloud-native-apps)\n\n* The Cloud Private [IBM Event Streams](https://www.ibm.com/cloud/event-streams) product running on  private cloud\n* Read introduction to [act on events with IBM Cloud Functions](../evt-action/README.md)\n\n## Slack channel\n\nContact us on '#eda-ac` channel under the [ibmcase.slack.com](http://ibmcase.slack.com) workspace.\n","frontmatter":{"title":"EDA Skill Journey","description":"EDA Skill Journey"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/aaa_ORPHANS/training/eda-skill-journey.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}