{"componentChunkName":"component---src-pages-kafka-faq-mdx","path":"/kafka/FAQ/","result":{"pageContext":{"frontmatter":{"title":"Frequently asked questions","description":"Frequently asked questions"},"relativePagePath":"/kafka/FAQ.mdx","titleType":"append","MdxNode":{"id":"5f758e91-202a-514a-988f-fbfd73a01788","children":[],"parent":"ef8e99ea-6eb2-593d-9a61-cf8249165f32","internal":{"content":"---\ntitle: Frequently asked questions\ndescription: Frequently asked questions\n---\n\n## Kafka concepts?\n\nSee [this introduction](readme.md)\n\n## How to support exactly once delivery?\n\nSee the section in the producer implementation considerations [note](producers.md).\n\nAlso it is important to note that the Kafka Stream API supports exactly once semantics with the config: `processing.guarantee=exactly_once`. Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once. \n\n## Why does kafka use zookeeper?\n\nKafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registrering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains kafka cluster integrity, select broker leader... \n\n## Retention time for topic what does it mean?\n\nThe message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: `retention.ms` and `retention.bytes`. Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to `cleanup.policy` parameter.\n\nSee the kafka documentation on [topic configuration parameters](https://kafka.apache.org/documentation/#topicconfigs). \n\nHere is a command to create a topic with specific retention properties:\n\n```\nbin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config  retention.ms=55000 --add-config  retention.byte=100000\n```\n\nBut there is also the `offsets.retention.minutes` property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers run not continuously. So consumers need to commit their offset. If the consumer settings define: `auto.offset.reset=earliest`, the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to `latest`). When using `latest`, if the consumers are offline for more than the offsets retention time window, they will lose events.\n\n## What are the topic characteristics I need to define during requirements?\n\nThis is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy.\n\n* Number of brokers in the cluster\n* fire or forget or persist data for which amount of time\n* Need for HA, set replicas to number of broker or at least the value of 3\n* Type of data to transport\n* Schema management to control change to the payload definition\n* volume per day\n* Accept snapshot\n* Need to do ge replication to other kafka cluster\n* Network filesystem used on the target kubernetes cluster and current storage class\n\n## What are the impacts of having not enough resource for kafka?\n\nThe table in this [Event Streams product documentation](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues.\n\n## What does out-of-synch partition mean and occur?\n\nWith partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered “committed” when all ISRs for partition wrote to their log. Only committed records are readable from consumer.\n\nSo out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader.\n\n## Differences between Akka and Kafka?\n\n[Akka](https://akka.io/) is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. \nKafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. \n\n[vert.x](https://vertx.io/) is another open source implementation of such internal messaging mechanism but supporting more language:  Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.\n\n\n## Event streams resource requirements \n\nSee the [detailed tables](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) in the product documentation.\n\n## Other FAQs\n\n[IBM Event streams on Cloud FAQ](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-faqs) \n\n[FAQ from Confluent](https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-HowareKafkabrokersdependonZookeeper?)","type":"Mdx","contentDigest":"f56bba1a6579e7e4a011dba1e41ed79c","counter":248,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Frequently asked questions","description":"Frequently asked questions"},"exports":{},"rawBody":"---\ntitle: Frequently asked questions\ndescription: Frequently asked questions\n---\n\n## Kafka concepts?\n\nSee [this introduction](readme.md)\n\n## How to support exactly once delivery?\n\nSee the section in the producer implementation considerations [note](producers.md).\n\nAlso it is important to note that the Kafka Stream API supports exactly once semantics with the config: `processing.guarantee=exactly_once`. Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once. \n\n## Why does kafka use zookeeper?\n\nKafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registrering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains kafka cluster integrity, select broker leader... \n\n## Retention time for topic what does it mean?\n\nThe message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: `retention.ms` and `retention.bytes`. Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to `cleanup.policy` parameter.\n\nSee the kafka documentation on [topic configuration parameters](https://kafka.apache.org/documentation/#topicconfigs). \n\nHere is a command to create a topic with specific retention properties:\n\n```\nbin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config  retention.ms=55000 --add-config  retention.byte=100000\n```\n\nBut there is also the `offsets.retention.minutes` property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers run not continuously. So consumers need to commit their offset. If the consumer settings define: `auto.offset.reset=earliest`, the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to `latest`). When using `latest`, if the consumers are offline for more than the offsets retention time window, they will lose events.\n\n## What are the topic characteristics I need to define during requirements?\n\nThis is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy.\n\n* Number of brokers in the cluster\n* fire or forget or persist data for which amount of time\n* Need for HA, set replicas to number of broker or at least the value of 3\n* Type of data to transport\n* Schema management to control change to the payload definition\n* volume per day\n* Accept snapshot\n* Need to do ge replication to other kafka cluster\n* Network filesystem used on the target kubernetes cluster and current storage class\n\n## What are the impacts of having not enough resource for kafka?\n\nThe table in this [Event Streams product documentation](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues.\n\n## What does out-of-synch partition mean and occur?\n\nWith partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered “committed” when all ISRs for partition wrote to their log. Only committed records are readable from consumer.\n\nSo out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader.\n\n## Differences between Akka and Kafka?\n\n[Akka](https://akka.io/) is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. \nKafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. \n\n[vert.x](https://vertx.io/) is another open source implementation of such internal messaging mechanism but supporting more language:  Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.\n\n\n## Event streams resource requirements \n\nSee the [detailed tables](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) in the product documentation.\n\n## Other FAQs\n\n[IBM Event streams on Cloud FAQ](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-faqs) \n\n[FAQ from Confluent](https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-HowareKafkabrokersdependonZookeeper?)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs-gatsby/src/pages/kafka/FAQ.mdx"}}}}