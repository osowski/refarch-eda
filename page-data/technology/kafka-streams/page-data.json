{"componentChunkName":"component---src-pages-technology-kafka-streams-index-mdx","path":"/technology/kafka-streams/","result":{"pageContext":{"frontmatter":{"title":"Kafka Streams","description":"Kafka Streams"},"relativePagePath":"/technology/kafka-streams/index.mdx","titleType":"append","MdxNode":{"id":"12cdf619-e405-5f25-808f-5b6c26ec35ff","children":[],"parent":"dcece103-e0b7-5f6a-8399-17e77e88f495","internal":{"content":"---\ntitle: Kafka Streams\ndescription: Kafka Streams\n---\n\nKafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams. \n\nWe recommend reading this excellent introduction [Kafka stream made simple](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/) from Jay Kreps from Confluent to get a good understanding of why Kafka stream was created.\n\n<AnchorLinks>\n<AnchorLink>Concepts</AnchorLink>\n<AnchorLink>Code structure</AnchorLink>\n<AnchorLink>Available tutorials</AnchorLink>\n<AnchorLink>Design considerations</AnchorLink>\n<AnchorLink>Interactive queries</AnchorLink>\n<AnchorLink>Faust: a python library to do kafka streaming</AnchorLink>\n<AnchorLink>Further readings</AnchorLink>\n</AnchorLinks>\n\n## Concepts\n\nThe business logic is implemented via **topology** that represents a graph of processing nodes.  Each node within the graph, processes events from the parent node. \n\nTo summarize, **Kafka Streams** has the following capabilities:\n\n* Kafka Streams applications are build on top of producer and consumer APIs and are leveraging Kafka capabilities to do data parallelism processing, support distributed coordination of partition to task assignment, and being fault tolerant.\n* Streams processing is helpful for handling out-of-order data, *reprocessing* input as code changes, and performing stateful computations, like real time analytics. It uses producer / consumer APIs, stateful storage and consumer groups. It treats both past and future data the same way.\n* Kafka Streams is an embedded library to integrate in your Java application. No need for separate processing cluster. As deployable container it can scale horizontally easily within Kubernetes platform. It does not run in Kafka cluster.\n* Topology consumes continuous real time flows of records and publishes new flows to one or more topics.\n* A stream (represented by the KStream API) is a durable, partitioned sequence of immutable events. When a new event is added a stream, it's appended to the partition that its key belongs to.\n* It can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, and horizontally by adding additional machines or pods in kubernetes.  Each deployed instance use the same value for the `application.id` kafka stream property.\n\n ![1](./images/kstreams-parallel.png)\n\n    The assignment of stream partitions to stream tasks never changes, so task is the unit of parallelism. Task executes the topology, and is buffering records coming from the attached partitions. \n\n* KTable is a durable, partitioned collection that models change over time. It's the mutable counterpart of KStreams. It represents what is true at the current moment. Each data record is considered a contextual update. Tables are saved in state store backed up with kafka topic and are queryables. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table’s state store.\n\nThese state stores are being materialized on local disk inside your application instances\n  \n  ![2](./images/kstreams-store.png)\n\n* It supports exactly-once processing semantics to guarantee that each record is processed once and only once even when there is a failure.\n* Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one.\n* Supports stateful and windowing operations by processing one record at a time.\n* An application's processor topology is scaled by breaking it into multiple tasks.\n* Tasks can then instantiate their own processor topology based on the assigned partitions.\n\n ![3](./images/kafka-stream-arch.png)\n\n### Fault tolerance\n\nAs KTables are persisted on state store, they are materialized on local to broker disk, as change log streams:\n\n ![4](./images/kstreams-fault.png)\n\nIn the case of a stream processing task fails, it can rebuild its internal, in memory state store from the kafka topic / change log. Once done it can reconsume messages. The system is fault tolerant.\n\n### Scaling\n\nWhen topics have multiple partitions, each kafka streams task consumes a unique partition. \n\n ![5](./images/kstreams-scale-1.png)\n\nIf for any reasons, we need to scale by adding new instances of the application, so in term of kubernetes, adding more pods, then the system will rebalance the stream tasks allocation to new instances created.\n\n ![6](./images/kstreams-scale-2.png)\n \nWe can start as many threads of the application as there are input Kafka topic partitions.\n\nAnother good example to illustrate threading, task and machine scaling is documented in this [on Confluent article](https://docs.confluent.io/current/streams/architecture.html#example).\n\n## Code structure\n\nIn general the code for processing event does the following:\n\n* Set a properties object to specify which brokers to connect to and what kind of key and value des/serialization mechanisms to use.\n* Define a stream client: if you want to get the stream of records use **KStream**, if you want a changelog with the last value of a given key use **KTable** (For example, using KTable to keep a user profile data by userid key).\n* Create a topology of input source and sink target and the set of actions to perform in between.\n* Start the stream client to consume records.\n\nProgramming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining.\n\nA stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor.\n\nWe recommend at this stage to do our [lab 0 tutorial](../../use-cases/kafka-streams/lab-0) to develop a first simple topology and test it without Kafka cluster using Topology test driver.\n\n\nThe following code extract, is part of the Apache Kafka Word count example and is used to illustrate the programming model used: \n\n```java\n// Streams processing are created from a builder.\nfinal StreamsBuilder builder = new StreamsBuilder();\n// pattern to extract word\nfinal Pattern pattern = Pattern.compile(\"\\\\W+\");\n// source is a kafka topic, and materialized as a KStream\nKStream<String, String> textLines = builder.stream(source);\n// implement the logic to count words\nKTable<String, Long> wordCounts = textLines\n    .flatMapValues(textLine -> Arrays.asList(pattern.split(textLine.toLowerCase())))\n    .print(Printed.toSysOut())\n    .groupBy((key, word) -> word)\n    .count(Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as(\"counts-store\"));\n// sink is another kafka topic. Produce for each word the number of occurrence in the given doc\nwordCounts.toStream().to(sink, Produced.with(Serdes.String(), Serdes.Long()));\n\nKafkaStreams streams = new KafkaStreams(builder.build(), props);\nstreams.start();\n```\n\n* [KStream](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/kstream/KStream.html) represents KeyValue records coming as event stream from the input topic.\n* `flatMapValues()` transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a [ValueMapper](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/kstream/ValueMapper.html) which applies transformation on values but keeps the key. Another important transformation is the [KeyValueMapper](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/kstream/KeyValueMapper.html).\n* `groupBy()` Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key.\n* `count()` counts the number of records in this stream by the grouped key. `Materialized` is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's [RocksDB key value persistence](https://rocksdb.org/) or a log-compacted topic in Kafka.\n* Produced defines how to provide the optional parameter types when producing to new topics.\n* KTable is an abstraction of a changelog stream from a primary-keyed table.\n\nImportant: map, flatMapValues and mapValues ... functions don’t modify the object or value presented as a parameter.\n\n\n## Available tutorials\n\nWe found the following tutorial helpful to grow your competency on Kafka Streams:\n\n* [Word count Kafka Stream example from product documentation](https://kafka.apache.org/26/documentation/streams/tutorial)\n* [Use Quarkus and Kafka Streams to use groupBy, join with another Stream](../../use-cases/kafka-streams/lab-1)\n* [Quarkus and Kafka Streams guides](https://quarkus.io/guides/kafka-streams)\n* [Build an inventory aggregator with Quarkus, with kstreams, ktable and interactive queries, Mutiny](../../use-cases/kafka-streams/lab-3), all deployable on OpenShift with quarkus kubernetes plugin.\n\n## Interactive queries\n\nState store can be queried. Result can be from the local store, if the key is in the local KTable, or remote. The metadata of the key to task allocation is maintained and shared between tasks. \n\nHere is an example of processing. The storeID is the key used in the KTable.\n\n```java\n public InventoryQueryResult getStoreStock(String storeID) {\n        KeyQueryMetadata metadata = metadata = streams.queryMetadataForKey(\n                StoreInventoryAgent.STOCKS_STORE_NAME,\n                storeID,\n                Serdes.String().serializer());\n       \n        if (metadata == null || metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n            return InventoryQueryResult.notFound();\n        } else if (metadata.getActiveHost().host().equals(host)) {\n            Inventory result = getStockStore().get(storeID);\n            ...\n        } else { //call remote node via REST API given the getActiveHost().host() and port()\n\n```\n\n## Design considerations\n\n* Partitions are assigned to a StreamTask, and each StreamTask has its own state store. So important to be sure to have key and kafka will assign records with same key to same partition so lookup inside state store will work.\n* Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory.\n* Reference data can be loaded inside a Ktable for event stream enrichment.\n\n* Table and streams joins: we recommend reading [this deep dive article](https://www.confluent.io/blog/crossing-streams-joins-apache-kafka/) on joining streams and stream with table. The important points from this article:\n    * kstream - kstream joins are windowed to control the size of data to keep in memory to search for the matching records.\n\n## Faust: a python library to do kafka streaming\n\n[Faust](https://faust.readthedocs.io/en/latest/index.html) is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions.\n\nIt uses rocksdb to support tables.\n\nFor the installation, in your python environment do a `pipenv run pip install faust`, or `pip install faust`. Then use faust as a CLI. So to start an agent as worker use:\n\n```shell\nfaust -A nameofthepythoncode -l info\n```\n\nMultiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores.\n\n## Further readings\n\n* The API and [product documentation](https://kafka.apache.org/21/documentation/streams/developer-guide/).\n* [Kafka Streams  concepts from Confluent](https://docs.confluent.io/current/streams/concepts.html)\n* [Deep dive explanation for the differences between KStream and KTable from Michael Noll](https://www.michael-noll.com/blog/2018/04/05/of-stream-and-tables-in-kafka-and-stream-processing-part1/)\n* [Our set of samples to getting started in coding kafka streams](https://github.com/jbcodeforce/kafka-streams-samples) \n* [Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent](https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/)\n* [Confluent Kafka Streams documentation](https://docs.confluent.io/current/streams/index.html#kafka-streams)\n* [Kafka Streams architecture article from Confluent](https://docs.confluent.io/current/streams/architecture.html).\n","type":"Mdx","contentDigest":"c8460a3526d2026d91ddbc9b15988b14","counter":625,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Streams\ndescription: Kafka Streams\n---\n\nKafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams. \n\nWe recommend reading this excellent introduction [Kafka stream made simple](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/) from Jay Kreps from Confluent to get a good understanding of why Kafka stream was created.\n\n<AnchorLinks>\n<AnchorLink>Concepts</AnchorLink>\n<AnchorLink>Code structure</AnchorLink>\n<AnchorLink>Available tutorials</AnchorLink>\n<AnchorLink>Design considerations</AnchorLink>\n<AnchorLink>Interactive queries</AnchorLink>\n<AnchorLink>Faust: a python library to do kafka streaming</AnchorLink>\n<AnchorLink>Further readings</AnchorLink>\n</AnchorLinks>\n\n## Concepts\n\nThe business logic is implemented via **topology** that represents a graph of processing nodes.  Each node within the graph, processes events from the parent node. \n\nTo summarize, **Kafka Streams** has the following capabilities:\n\n* Kafka Streams applications are build on top of producer and consumer APIs and are leveraging Kafka capabilities to do data parallelism processing, support distributed coordination of partition to task assignment, and being fault tolerant.\n* Streams processing is helpful for handling out-of-order data, *reprocessing* input as code changes, and performing stateful computations, like real time analytics. It uses producer / consumer APIs, stateful storage and consumer groups. It treats both past and future data the same way.\n* Kafka Streams is an embedded library to integrate in your Java application. No need for separate processing cluster. As deployable container it can scale horizontally easily within Kubernetes platform. It does not run in Kafka cluster.\n* Topology consumes continuous real time flows of records and publishes new flows to one or more topics.\n* A stream (represented by the KStream API) is a durable, partitioned sequence of immutable events. When a new event is added a stream, it's appended to the partition that its key belongs to.\n* It can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, and horizontally by adding additional machines or pods in kubernetes.  Each deployed instance use the same value for the `application.id` kafka stream property.\n\n ![1](./images/kstreams-parallel.png)\n\n    The assignment of stream partitions to stream tasks never changes, so task is the unit of parallelism. Task executes the topology, and is buffering records coming from the attached partitions. \n\n* KTable is a durable, partitioned collection that models change over time. It's the mutable counterpart of KStreams. It represents what is true at the current moment. Each data record is considered a contextual update. Tables are saved in state store backed up with kafka topic and are queryables. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table’s state store.\n\nThese state stores are being materialized on local disk inside your application instances\n  \n  ![2](./images/kstreams-store.png)\n\n* It supports exactly-once processing semantics to guarantee that each record is processed once and only once even when there is a failure.\n* Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one.\n* Supports stateful and windowing operations by processing one record at a time.\n* An application's processor topology is scaled by breaking it into multiple tasks.\n* Tasks can then instantiate their own processor topology based on the assigned partitions.\n\n ![3](./images/kafka-stream-arch.png)\n\n### Fault tolerance\n\nAs KTables are persisted on state store, they are materialized on local to broker disk, as change log streams:\n\n ![4](./images/kstreams-fault.png)\n\nIn the case of a stream processing task fails, it can rebuild its internal, in memory state store from the kafka topic / change log. Once done it can reconsume messages. The system is fault tolerant.\n\n### Scaling\n\nWhen topics have multiple partitions, each kafka streams task consumes a unique partition. \n\n ![5](./images/kstreams-scale-1.png)\n\nIf for any reasons, we need to scale by adding new instances of the application, so in term of kubernetes, adding more pods, then the system will rebalance the stream tasks allocation to new instances created.\n\n ![6](./images/kstreams-scale-2.png)\n \nWe can start as many threads of the application as there are input Kafka topic partitions.\n\nAnother good example to illustrate threading, task and machine scaling is documented in this [on Confluent article](https://docs.confluent.io/current/streams/architecture.html#example).\n\n## Code structure\n\nIn general the code for processing event does the following:\n\n* Set a properties object to specify which brokers to connect to and what kind of key and value des/serialization mechanisms to use.\n* Define a stream client: if you want to get the stream of records use **KStream**, if you want a changelog with the last value of a given key use **KTable** (For example, using KTable to keep a user profile data by userid key).\n* Create a topology of input source and sink target and the set of actions to perform in between.\n* Start the stream client to consume records.\n\nProgramming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining.\n\nA stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor.\n\nWe recommend at this stage to do our [lab 0 tutorial](../../use-cases/kafka-streams/lab-0) to develop a first simple topology and test it without Kafka cluster using Topology test driver.\n\n\nThe following code extract, is part of the Apache Kafka Word count example and is used to illustrate the programming model used: \n\n```java\n// Streams processing are created from a builder.\nfinal StreamsBuilder builder = new StreamsBuilder();\n// pattern to extract word\nfinal Pattern pattern = Pattern.compile(\"\\\\W+\");\n// source is a kafka topic, and materialized as a KStream\nKStream<String, String> textLines = builder.stream(source);\n// implement the logic to count words\nKTable<String, Long> wordCounts = textLines\n    .flatMapValues(textLine -> Arrays.asList(pattern.split(textLine.toLowerCase())))\n    .print(Printed.toSysOut())\n    .groupBy((key, word) -> word)\n    .count(Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as(\"counts-store\"));\n// sink is another kafka topic. Produce for each word the number of occurrence in the given doc\nwordCounts.toStream().to(sink, Produced.with(Serdes.String(), Serdes.Long()));\n\nKafkaStreams streams = new KafkaStreams(builder.build(), props);\nstreams.start();\n```\n\n* [KStream](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/kstream/KStream.html) represents KeyValue records coming as event stream from the input topic.\n* `flatMapValues()` transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a [ValueMapper](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/kstream/ValueMapper.html) which applies transformation on values but keeps the key. Another important transformation is the [KeyValueMapper](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/kstream/KeyValueMapper.html).\n* `groupBy()` Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key.\n* `count()` counts the number of records in this stream by the grouped key. `Materialized` is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's [RocksDB key value persistence](https://rocksdb.org/) or a log-compacted topic in Kafka.\n* Produced defines how to provide the optional parameter types when producing to new topics.\n* KTable is an abstraction of a changelog stream from a primary-keyed table.\n\nImportant: map, flatMapValues and mapValues ... functions don’t modify the object or value presented as a parameter.\n\n\n## Available tutorials\n\nWe found the following tutorial helpful to grow your competency on Kafka Streams:\n\n* [Word count Kafka Stream example from product documentation](https://kafka.apache.org/26/documentation/streams/tutorial)\n* [Use Quarkus and Kafka Streams to use groupBy, join with another Stream](../../use-cases/kafka-streams/lab-1)\n* [Quarkus and Kafka Streams guides](https://quarkus.io/guides/kafka-streams)\n* [Build an inventory aggregator with Quarkus, with kstreams, ktable and interactive queries, Mutiny](../../use-cases/kafka-streams/lab-3), all deployable on OpenShift with quarkus kubernetes plugin.\n\n## Interactive queries\n\nState store can be queried. Result can be from the local store, if the key is in the local KTable, or remote. The metadata of the key to task allocation is maintained and shared between tasks. \n\nHere is an example of processing. The storeID is the key used in the KTable.\n\n```java\n public InventoryQueryResult getStoreStock(String storeID) {\n        KeyQueryMetadata metadata = metadata = streams.queryMetadataForKey(\n                StoreInventoryAgent.STOCKS_STORE_NAME,\n                storeID,\n                Serdes.String().serializer());\n       \n        if (metadata == null || metadata == KeyQueryMetadata.NOT_AVAILABLE) {\n            return InventoryQueryResult.notFound();\n        } else if (metadata.getActiveHost().host().equals(host)) {\n            Inventory result = getStockStore().get(storeID);\n            ...\n        } else { //call remote node via REST API given the getActiveHost().host() and port()\n\n```\n\n## Design considerations\n\n* Partitions are assigned to a StreamTask, and each StreamTask has its own state store. So important to be sure to have key and kafka will assign records with same key to same partition so lookup inside state store will work.\n* Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory.\n* Reference data can be loaded inside a Ktable for event stream enrichment.\n\n* Table and streams joins: we recommend reading [this deep dive article](https://www.confluent.io/blog/crossing-streams-joins-apache-kafka/) on joining streams and stream with table. The important points from this article:\n    * kstream - kstream joins are windowed to control the size of data to keep in memory to search for the matching records.\n\n## Faust: a python library to do kafka streaming\n\n[Faust](https://faust.readthedocs.io/en/latest/index.html) is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions.\n\nIt uses rocksdb to support tables.\n\nFor the installation, in your python environment do a `pipenv run pip install faust`, or `pip install faust`. Then use faust as a CLI. So to start an agent as worker use:\n\n```shell\nfaust -A nameofthepythoncode -l info\n```\n\nMultiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores.\n\n## Further readings\n\n* The API and [product documentation](https://kafka.apache.org/21/documentation/streams/developer-guide/).\n* [Kafka Streams  concepts from Confluent](https://docs.confluent.io/current/streams/concepts.html)\n* [Deep dive explanation for the differences between KStream and KTable from Michael Noll](https://www.michael-noll.com/blog/2018/04/05/of-stream-and-tables-in-kafka-and-stream-processing-part1/)\n* [Our set of samples to getting started in coding kafka streams](https://github.com/jbcodeforce/kafka-streams-samples) \n* [Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent](https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/)\n* [Confluent Kafka Streams documentation](https://docs.confluent.io/current/streams/index.html#kafka-streams)\n* [Kafka Streams architecture article from Confluent](https://docs.confluent.io/current/streams/architecture.html).\n","frontmatter":{"title":"Kafka Streams","description":"Kafka Streams"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/kafka-streams/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}