{"componentChunkName":"component---src-pages-technology-kafka-connect-index-mdx","path":"/technology/kafka-connect/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect","description":"Kafka Connect"},"relativePagePath":"/technology/kafka-connect/index.mdx","titleType":"append","MdxNode":{"id":"142caa9e-3e4f-5b85-b28c-9cc11ee37e15","children":[],"parent":"ee24c705-6de5-53fb-a81a-0bcfe8db608b","internal":{"content":"---\ntitle: Kafka Connect\ndescription: Kafka Connect\n---\n\n[Kafka connect](https://kafka.apache.org/documentation/#connect) is an open source component for easily integrate external systems with Kafka. It works with any Kafka product like IBM Event Streams. It uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.\n\n![Kafka component](../images/kafka-components.png)\n\nThe general concepts are detailed in the [IBM Event streams product documentation](https://ibm.github.io/event-streams/connecting/connectors/). Here is a quick summary:\n\n* **Connector** represents a logical job to move data from / to kafka  to / from external systems. A lot of [existing connectors](https://ibm.github.io/event-streams/connectors/) can be reused, or you can [implement your own](https://kafka.apache.org/documentation/#connect_development).\n* **Workers** are JVM running the connector. For production deployment workers run in cluster or \"distributed mode\", and leverage the group management protocol to scale task horizontally.\n* **Tasks**: each worker coordinates a set of tasks to copy data. In distributed mode, task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.\n* **REST API** to configure the connectors and monitors the tasks.\n\n![Connectors and tasks](../images/connector-tasks.png)\n\nWhen a connector is submitted to the cluster via a POST operation on its API, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work.\n\n## Characteristics\n\n* Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example.\n* Support streaming and batch.\n* Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster.\n* Copy data, externalizing transformation in other framework.\n* Kafka Connect defines three models: data model, worker model and connector model.\n\n## Installation\n\nThe  Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment.\n\nWe recommend reading the [IBM  event streams documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) for installing Kafka connect with IBM Event Streams or you can also leverage the [Strimzi Kafka connect operator](https://strimzi.io/docs/0.18.0/#kafka-connect-str).\n\nWith IBM Event Streams on premise, the connectors setup is part of the user admin console toolbox:\n\n![Event Streams connector](../images/es-connectors.png)\n\n*Deploying connectors against an IBM Event Streams cluster, you need to have an API key with Manager role, to be able to create topic, produce and consume messages for all topics.*\n\nAs an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker image which needs to be updated with the connector jars and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL.\n\nThe following [public IBM messaging github account](https://github.com/ibm-messaging) includes supported, open sourced, connectors (search for `connector`).\n\nHere is the [list of supported connectors](https://ibm.github.io/event-streams/connectors/) for IBM Event Streams.\n\n## Connecting to IBM Cloud Event Streams remote cluster\n\nWe have developed different scenarios in [this lab](/scenarios/realtime-inventory/) to remotely connect a Kafka Connect distributed cluster to Event Streams on cloud.\n\n## Further Readings\n\n* [Apache Kafka connect documentation](https://kafka.apache.org/documentation/#connect)\n* [Confluent Connector Documentation](https://docs.confluent.io/current/connect/index.html)\n* [IBM Event Streams Connectors](https://ibm.github.io/event-streams/connecting/connectors/) or [the list of supported connectors](https://ibm.github.io/event-streams/connectors/)\n* [MongoDB Connector for Apache Kafka](https://github.com/mongodb/mongo-kafka)\n","type":"Mdx","contentDigest":"6ae070c814449c9c46ce8421462c25f3","counter":544,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Connect\ndescription: Kafka Connect\n---\n\n[Kafka connect](https://kafka.apache.org/documentation/#connect) is an open source component for easily integrate external systems with Kafka. It works with any Kafka product like IBM Event Streams. It uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.\n\n![Kafka component](../images/kafka-components.png)\n\nThe general concepts are detailed in the [IBM Event streams product documentation](https://ibm.github.io/event-streams/connecting/connectors/). Here is a quick summary:\n\n* **Connector** represents a logical job to move data from / to kafka  to / from external systems. A lot of [existing connectors](https://ibm.github.io/event-streams/connectors/) can be reused, or you can [implement your own](https://kafka.apache.org/documentation/#connect_development).\n* **Workers** are JVM running the connector. For production deployment workers run in cluster or \"distributed mode\", and leverage the group management protocol to scale task horizontally.\n* **Tasks**: each worker coordinates a set of tasks to copy data. In distributed mode, task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.\n* **REST API** to configure the connectors and monitors the tasks.\n\n![Connectors and tasks](../images/connector-tasks.png)\n\nWhen a connector is submitted to the cluster via a POST operation on its API, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work.\n\n## Characteristics\n\n* Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example.\n* Support streaming and batch.\n* Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster.\n* Copy data, externalizing transformation in other framework.\n* Kafka Connect defines three models: data model, worker model and connector model.\n\n## Installation\n\nThe  Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment.\n\nWe recommend reading the [IBM  event streams documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) for installing Kafka connect with IBM Event Streams or you can also leverage the [Strimzi Kafka connect operator](https://strimzi.io/docs/0.18.0/#kafka-connect-str).\n\nWith IBM Event Streams on premise, the connectors setup is part of the user admin console toolbox:\n\n![Event Streams connector](../images/es-connectors.png)\n\n*Deploying connectors against an IBM Event Streams cluster, you need to have an API key with Manager role, to be able to create topic, produce and consume messages for all topics.*\n\nAs an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker image which needs to be updated with the connector jars and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL.\n\nThe following [public IBM messaging github account](https://github.com/ibm-messaging) includes supported, open sourced, connectors (search for `connector`).\n\nHere is the [list of supported connectors](https://ibm.github.io/event-streams/connectors/) for IBM Event Streams.\n\n## Connecting to IBM Cloud Event Streams remote cluster\n\nWe have developed different scenarios in [this lab](/scenarios/realtime-inventory/) to remotely connect a Kafka Connect distributed cluster to Event Streams on cloud.\n\n## Further Readings\n\n* [Apache Kafka connect documentation](https://kafka.apache.org/documentation/#connect)\n* [Confluent Connector Documentation](https://docs.confluent.io/current/connect/index.html)\n* [IBM Event Streams Connectors](https://ibm.github.io/event-streams/connecting/connectors/) or [the list of supported connectors](https://ibm.github.io/event-streams/connectors/)\n* [MongoDB Connector for Apache Kafka](https://github.com/mongodb/mongo-kafka)\n","frontmatter":{"title":"Kafka Connect","description":"Kafka Connect"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/kafka-connect/index.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}