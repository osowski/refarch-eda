{"componentChunkName":"component---src-pages-technology-kafka-connect-index-mdx","path":"/technology/kafka-connect/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect","description":"Kafka Connect"},"relativePagePath":"/technology/kafka-connect/index.mdx","titleType":"append","MdxNode":{"id":"142caa9e-3e4f-5b85-b28c-9cc11ee37e15","children":[],"parent":"ee24c705-6de5-53fb-a81a-0bcfe8db608b","internal":{"content":"---\ntitle: Kafka Connect\ndescription: Kafka Connect\n---\n\n[Kafka connect](https://kafka.apache.org/documentation/#connect) is an open source component for easily integrate external systems with Kafka. It works with any Kafka producer like IBM Event Streams and Red Hat AMQ Streams. It uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.\n\n![Kafka component](../images/kafka-components.png)\n\nThe general concepts are detailed in the [IBM Event streams product documentation](https://ibm.github.io/event-streams/connecting/connectors/). Here is a quick summary:\n\n* **connector** represents a logical job to move data from / to kafka  to / from external systems. A lot of [existing connectors](https://ibm.github.io/event-streams/connectors/) can be reused, or you can implement your owns.\n* **workers** are JVM running the connector. For production deployment workers run in cluster or \"distributed mode\".\n* **tasks**: each worker coordinates a set of tasks to copy data. Task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.\n\n![Connectors and tasks](../images/connector-tasks.png)\n\nWhen a connector is submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work.\n\n## Characteristics\n\n* Copy vast quantities of data from source to kafka: work at the datasource level. So when it is a database, it uses JDBC API for example.\n* Support streaming and batch.\n* Scale at the organization level, even if it can support a standalone, mono connector approach to start small, it is possible to run in parallel on distributed cluster.\n* Copy data, externalizing transformation in other framework.\n* Kafka Connect defines three models: data model, worker model and connector model.\n* Provide a REST interface to manage connectors and monitor jobs.\n\n## Installation\n\nThe  Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment.\n\nWe recommend reading the [IBM  event streams documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) for installing Kafka connect with IBM Event Streams or you can also leverage the [Strimzi Kafka connect operator](https://strimzi.io/docs/0.17.0/#kafka-connect-str).\n\nWith IBM Event Streams on premise deployment, the connectors setup is part of the user admin console toolbox:\n\n![Event Streams connector](../images/es-connectors.png)\n\n*Deploying connectors against an IBM Event Streams cluster, you need to have API key with permissions to produce and consume messages for all topics.*\n\nAs an extendable framework, kafka connect, can have new connector plugins. To deploy new connector, the kafka docker image defining the connector needs to be updated with the connector jar and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL.\n\nHere is the [list of supported connectors](https://ibm.github.io/event-streams/connectors/) for IBM Event Streams.\n\nWe will use this image to run the kafka connect in standalone mode or in [the distributed mode section](#distributed-mode).\n\n## Getting started with kafka connect standalone mode\n\nFor development and test purposes, we can use Kafka connect in standalone mode, but still connected to IBM Event Streams running on-premise or on-cloud.\n\n* From the [downloaded dockerfile](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/Dockerfile) (in the refarch-kc repository) we can build a new kafka connect environment image using the command:\n\n```shell\ndocker build -t ibmcase/kafkaconnect:0.0.1 .\n```\n\n* Start a container with kafka connector, to run a standalone connector: you need to use a worker configuration and one of the connector properties file under the `connectors` folder. Those files will be mounted under the `/opt/kafka/config` inside the container. Also, as we want to test sending the content of a file, we mount a local `data` folder to the `/home/data`:\n\n```shell\n# in the refarch-kc/docker/kafka-connect folder\ndocker run -ti  --rm -v $(pwd)/config:/opt/kafka/config -v $(pwd)/data:/home/data --entrypoint bash -p 8083:8083 ibmcase/kafkaconnect:0.0.1\n```\n\n**Note:** You need to modify those property files to set the API key for your own event streams cluster, and set any other properties.\n\n* Inside the container starts the standalone connector:\n\n```shell\ncd /opt/kafka\n./bin/connect-standalone.sh config/worker-standalone.properties config/file-source.properties config/file-sink.properties\n```\n\nThe  `file-source.properties` configures a file reader to source the `data/access_log.txt` file to the `clickstream` topic:\n\n```properties\nname=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/home/kafka-connect/access_log.txt\ntopic=clickstream\n```\n\nWhile the `config/file-sink.properties` defines a file sink stream to create a json file by getting messages from the `clickstream` topic. The file sink connector can read from multiple topics to aggregate the content in the same file.\n\nThe standalone connector worker configuration specifies where to connect, and what converters to use:\n\n```properties\nbootstrap.servers=....\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n\n# Local storage file for offset data\noffset.storage.file.filename=/tmp/connect.offsets\n```\n\nThe execution trace shows the producer id, and the consumer id, and the task for each connector:\n\n```log\nINFO Creating task local-file-source-0 (org.apache.kafka.connect.runtime.Worker:414)\n...\nINFO TaskConfig values:\n\ttask.class = class org.apache.kafka.connect.file.FileStreamSourceTask\n...\n INFO Creating connector local-file-sink of type FileStreamSink (org.apache.kafka.connect.runtime.Worker:246)\n INFO Creating task local-file-sink-0 (org.apache.kafka.connect.runtime.Worker:414)\n```\n\nTo validate the data are well published see the generated file under the `data` folder. As the Json converter was used, the message was wrapped into a json document with schema and payload.\n\n```json\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\"}\n```\n\n## Connecting to IBM Cloud Event Streams remote cluster\n\nTo connect to Event Streams on IBM Cloud the properties needs to define the broker adviser URLs and the API key that you get from the service crendentials.\n\nThis API key must provide permission to produce and consume messages for all topics, and also to create topics.\n\nWith Event streams on Cloud the [following document](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect) explains what properties to add to the worker and connectors configuration.\n\n```properties\nbootstrap.servers=broker-3-qnsdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprt...\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"98....\";\n\n```\n\nUsing the same `file source stream connector` to send records and a simple consumer console to trace the output like:\n\n```shell\ndocker run -ti  -v $(pwd)/config:/opt/kafka/config --entrypoint bash  ibmcase/kafkaconnect:0.0.1\n\nesuser@3245874dcdd3: cd /opt/kafka/bin/\nesuser@3245874dcdd3: ./kafka-console-consumer.sh --bootstrap-server eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443 --consumer.config /opt/kafka/config/console-consumer.properties --topic clickstream --from-beginning\"\n```\n\nThe console-consumer.properties specifies the SASL properties to connect to the remote broker using API key.\n\n## Distributed mode\n\nWhen running in distributed mode, the connectors need three topics as presented in the `create topics` table [here](https://ibm.github.io/event-streams/connecting/setting-up-connectors/).\n\n* **connect-configs**: This topic will store the connector and task configurations.\n* **connect-offsets**: This topic is used to store offsets for Kafka Connect.\n* **connect-status**: This topic will store status updates of connectors and tasks.\n\n* Using IBM Event Streams CLI, the topics are created via the commands like:\n\n```shell\n# log to the kubernetes cluster:\ncloudctl login -a https://icp-console.apps.green.ocp.csplab.local\n# initialize the event streams CLI plugin\ncloudctl es init\n# Create the Kafka topic\ncloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compact\ncloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compact\ncloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact\ncloudctl es topics\n```\n\n* When using a kafka cluster managed with Strimzi topic operator you can use the topic definitions in [the folder](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect):\n\n```shell\noc apply -f strimzi-connect-config-topic.yaml\noc apply -f strimzi-connect-offsets-topic.yaml\noc apply -f strimzi-connect-status-topic.yaml\n```\n\nThe connector configuration needs to specify some other properties as explained in the [kafka documentation](https://kafka.apache.org/documentation/#connectconfigs)):\n\n* group.id to specify the connect cluster name.\n* key and value converters.\n* replication factors and topic name for the three needed topics, if Kafka connect is enabled to create topics on the cluster.\n\nWhen using Event Streams as kafka cluster, add the `sasl` properties as described in the [product documentation](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect#distributed_worker).\n\nWith Event Streams as part of the Cloud Pak for integration, the administration console explains the steps to setup connectors, get distributed configuration and how to add connectors.\n\nSee [this properties file](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/config/connect-distributed.properties) as an example.\n\nTo start a Kafka connect in distributed mode locally, connected to Event Streams deployed on-premise use the following command (the entry point in the dockerfile use the connect-distributed mode script):\n\n```shell\ndocker run -v $(pwd)/config:/opt/kafka/config -p 8083:8083 ibmcase/kafkaconnect:0.0.1\n```\n\nTo illustrate the Kafka Connect distributed mode, we will add a source connector from a Mongo DB data source using [this connector](https://www.mongodb.com/kafka-connector).\n\n![Mongo source ](../images/kconnect-mongo.png)\n\nWhen using as a source, the connector publishes data changes from MongoDB into Kafka topics for streaming to consuming apps. Data is captured via Change Streams within the MongoDB cluster and published into Kafka topics. The installation of a connector is done by adding the jars from the connector into the plugin path (`/opt/connectors`) as defined in the connector properties. In the case of mongodb kafka connector the manual installation instructions are in [here](https://docs.mongodb.com/kafka-connector/current/kafka-installation/). The download page includes an uber jar.\n\nAs we run the kakfa connect as docker container, the approach is to build a new docker image based one of the Kafka image publicly available.\n\nTo define and start a connector, you do a POST to the REST API.\n\n## Verifying the connectors via the REST api\n\nThe documentation about the REST APIs for the distributed connector is in [this site](https://docs.confluent.io/current/connect/references/restapi.html).\n\nFor example the http://localhost:8083/connectors is the base URL when running locally.\n\n## Deploy the Kafka connect as a service within Openshift cluster\n\nWhen you use IBM Event Streams on Openshift, you can deploy the IBM kafka connector environment as Docker containers, and define the needed `connect-*` topics as explained in previous section. The product documentation describes how to do that.\n\nAnother approach is to use [Strimzi](https://strimzi.io/) operator.\n\nTo Be done!\n\n## Running with local kafka cluster\n\nWe are using a local kafka cluster started with docker-compose as defined in the compose file [here](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/backbone-compose.yml).\n\n* The docker network should be `kafkanet`, if not already created do the following\n\n```shell\ndocker network create kafkanet\n```\n\n* Start the kafka broker (bitnami distribution) and zookeeper node using the command below under the `refarch-kc/docker` folder:\n\n```shell\ndocker-compose -f backbone-compose.yml up -d\n```\n\n* Start a container with kafka code, to run a standalone connector: you need to use a worker configuration and a connector properties files. Those files will be mounted under the /home folder:\n\n```shell\ndocker run -ti  -rm --name kconnect -v $(pwd):/home --network kafkanet -p 8083:8083 bitnami/kafka:2 bash\n```\n\nNeed to map the port 8083, to access the REST APIs.\n\n* Inside the container starts the standalone connector:\n\n```shell\ncd /opt/bitnami/kafka\n./bin/connect-standalone.sh /home/kafka-connect/worker-standalone.properties /home/kafka-connect/file-source.properties\n```\n\nThe above file configures a file reader to source the `access_log.txt` file to the `clickstream` topic:\n\n```properties\nname=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/home/kafka-connect/access_log.txt\ntopic=clickstream\n```\n\nThe standalone connector worker configuration specifies where to connect, and what converters to use:\n\n```properties\nbootstrap.servers=kafka1:9092\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n\n# Local storage file for offset data\noffset.storage.file.filename=/tmp/connect.offsets\n```\n\nThe execution trace shows the producer id\n\n```log\nINFO [Producer clientId=connector-producer-local-file-source-0] Cluster ID: tj8y0hiZSYWHB9vLHGP1Ew (org.apache.kafka.clients.Metadata:261)\n```\n\nTo validate the data are well published run another container with the consumer console tool:\n\n```shell\ndocker run -ti  --name sinktrace --rm  --network kafkanet bitnami/kafka:2 bash -c \"\n/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic clickstream --from-beginning\"\n```\n\nAs the Json converter was used the trace show the message was wrapped into a json document with schema and payload.\n\n```json\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\"}\n```\n\n## Further Readings\n\n* [Apache Kafka connect documentation](https://kafka.apache.org/documentation/#connect)\n* [Confluent Connector Documentation](https://docs.confluent.io/current/connect/index.html)\n* [IBM Event Streams Connectors](https://ibm.github.io/event-streams/connecting/connectors/) or [the list of supported connectors](https://ibm.github.io/event-streams/connectors/)\n* [MongoDB Connector for Apache Kafka](https://github.com/mongodb/mongo-kafka)\n","type":"Mdx","contentDigest":"a19d121d9f9ccbaefa839b930ba941a3","counter":357,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Kafka Connect","description":"Kafka Connect"},"exports":{},"rawBody":"---\ntitle: Kafka Connect\ndescription: Kafka Connect\n---\n\n[Kafka connect](https://kafka.apache.org/documentation/#connect) is an open source component for easily integrate external systems with Kafka. It works with any Kafka producer like IBM Event Streams and Red Hat AMQ Streams. It uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.\n\n![Kafka component](../images/kafka-components.png)\n\nThe general concepts are detailed in the [IBM Event streams product documentation](https://ibm.github.io/event-streams/connecting/connectors/). Here is a quick summary:\n\n* **connector** represents a logical job to move data from / to kafka  to / from external systems. A lot of [existing connectors](https://ibm.github.io/event-streams/connectors/) can be reused, or you can implement your owns.\n* **workers** are JVM running the connector. For production deployment workers run in cluster or \"distributed mode\".\n* **tasks**: each worker coordinates a set of tasks to copy data. Task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.\n\n![Connectors and tasks](../images/connector-tasks.png)\n\nWhen a connector is submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work.\n\n## Characteristics\n\n* Copy vast quantities of data from source to kafka: work at the datasource level. So when it is a database, it uses JDBC API for example.\n* Support streaming and batch.\n* Scale at the organization level, even if it can support a standalone, mono connector approach to start small, it is possible to run in parallel on distributed cluster.\n* Copy data, externalizing transformation in other framework.\n* Kafka Connect defines three models: data model, worker model and connector model.\n* Provide a REST interface to manage connectors and monitor jobs.\n\n## Installation\n\nThe  Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment.\n\nWe recommend reading the [IBM  event streams documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) for installing Kafka connect with IBM Event Streams or you can also leverage the [Strimzi Kafka connect operator](https://strimzi.io/docs/0.17.0/#kafka-connect-str).\n\nWith IBM Event Streams on premise deployment, the connectors setup is part of the user admin console toolbox:\n\n![Event Streams connector](../images/es-connectors.png)\n\n*Deploying connectors against an IBM Event Streams cluster, you need to have API key with permissions to produce and consume messages for all topics.*\n\nAs an extendable framework, kafka connect, can have new connector plugins. To deploy new connector, the kafka docker image defining the connector needs to be updated with the connector jar and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL.\n\nHere is the [list of supported connectors](https://ibm.github.io/event-streams/connectors/) for IBM Event Streams.\n\nWe will use this image to run the kafka connect in standalone mode or in [the distributed mode section](#distributed-mode).\n\n## Getting started with kafka connect standalone mode\n\nFor development and test purposes, we can use Kafka connect in standalone mode, but still connected to IBM Event Streams running on-premise or on-cloud.\n\n* From the [downloaded dockerfile](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/Dockerfile) (in the refarch-kc repository) we can build a new kafka connect environment image using the command:\n\n```shell\ndocker build -t ibmcase/kafkaconnect:0.0.1 .\n```\n\n* Start a container with kafka connector, to run a standalone connector: you need to use a worker configuration and one of the connector properties file under the `connectors` folder. Those files will be mounted under the `/opt/kafka/config` inside the container. Also, as we want to test sending the content of a file, we mount a local `data` folder to the `/home/data`:\n\n```shell\n# in the refarch-kc/docker/kafka-connect folder\ndocker run -ti  --rm -v $(pwd)/config:/opt/kafka/config -v $(pwd)/data:/home/data --entrypoint bash -p 8083:8083 ibmcase/kafkaconnect:0.0.1\n```\n\n**Note:** You need to modify those property files to set the API key for your own event streams cluster, and set any other properties.\n\n* Inside the container starts the standalone connector:\n\n```shell\ncd /opt/kafka\n./bin/connect-standalone.sh config/worker-standalone.properties config/file-source.properties config/file-sink.properties\n```\n\nThe  `file-source.properties` configures a file reader to source the `data/access_log.txt` file to the `clickstream` topic:\n\n```properties\nname=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/home/kafka-connect/access_log.txt\ntopic=clickstream\n```\n\nWhile the `config/file-sink.properties` defines a file sink stream to create a json file by getting messages from the `clickstream` topic. The file sink connector can read from multiple topics to aggregate the content in the same file.\n\nThe standalone connector worker configuration specifies where to connect, and what converters to use:\n\n```properties\nbootstrap.servers=....\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n\n# Local storage file for offset data\noffset.storage.file.filename=/tmp/connect.offsets\n```\n\nThe execution trace shows the producer id, and the consumer id, and the task for each connector:\n\n```log\nINFO Creating task local-file-source-0 (org.apache.kafka.connect.runtime.Worker:414)\n...\nINFO TaskConfig values:\n\ttask.class = class org.apache.kafka.connect.file.FileStreamSourceTask\n...\n INFO Creating connector local-file-sink of type FileStreamSink (org.apache.kafka.connect.runtime.Worker:246)\n INFO Creating task local-file-sink-0 (org.apache.kafka.connect.runtime.Worker:414)\n```\n\nTo validate the data are well published see the generated file under the `data` folder. As the Json converter was used, the message was wrapped into a json document with schema and payload.\n\n```json\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\"}\n```\n\n## Connecting to IBM Cloud Event Streams remote cluster\n\nTo connect to Event Streams on IBM Cloud the properties needs to define the broker adviser URLs and the API key that you get from the service crendentials.\n\nThis API key must provide permission to produce and consume messages for all topics, and also to create topics.\n\nWith Event streams on Cloud the [following document](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect) explains what properties to add to the worker and connectors configuration.\n\n```properties\nbootstrap.servers=broker-3-qnsdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprt...\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"98....\";\n\n```\n\nUsing the same `file source stream connector` to send records and a simple consumer console to trace the output like:\n\n```shell\ndocker run -ti  -v $(pwd)/config:/opt/kafka/config --entrypoint bash  ibmcase/kafkaconnect:0.0.1\n\nesuser@3245874dcdd3: cd /opt/kafka/bin/\nesuser@3245874dcdd3: ./kafka-console-consumer.sh --bootstrap-server eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443 --consumer.config /opt/kafka/config/console-consumer.properties --topic clickstream --from-beginning\"\n```\n\nThe console-consumer.properties specifies the SASL properties to connect to the remote broker using API key.\n\n## Distributed mode\n\nWhen running in distributed mode, the connectors need three topics as presented in the `create topics` table [here](https://ibm.github.io/event-streams/connecting/setting-up-connectors/).\n\n* **connect-configs**: This topic will store the connector and task configurations.\n* **connect-offsets**: This topic is used to store offsets for Kafka Connect.\n* **connect-status**: This topic will store status updates of connectors and tasks.\n\n* Using IBM Event Streams CLI, the topics are created via the commands like:\n\n```shell\n# log to the kubernetes cluster:\ncloudctl login -a https://icp-console.apps.green.ocp.csplab.local\n# initialize the event streams CLI plugin\ncloudctl es init\n# Create the Kafka topic\ncloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compact\ncloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compact\ncloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact\ncloudctl es topics\n```\n\n* When using a kafka cluster managed with Strimzi topic operator you can use the topic definitions in [the folder](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect):\n\n```shell\noc apply -f strimzi-connect-config-topic.yaml\noc apply -f strimzi-connect-offsets-topic.yaml\noc apply -f strimzi-connect-status-topic.yaml\n```\n\nThe connector configuration needs to specify some other properties as explained in the [kafka documentation](https://kafka.apache.org/documentation/#connectconfigs)):\n\n* group.id to specify the connect cluster name.\n* key and value converters.\n* replication factors and topic name for the three needed topics, if Kafka connect is enabled to create topics on the cluster.\n\nWhen using Event Streams as kafka cluster, add the `sasl` properties as described in the [product documentation](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect#distributed_worker).\n\nWith Event Streams as part of the Cloud Pak for integration, the administration console explains the steps to setup connectors, get distributed configuration and how to add connectors.\n\nSee [this properties file](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/config/connect-distributed.properties) as an example.\n\nTo start a Kafka connect in distributed mode locally, connected to Event Streams deployed on-premise use the following command (the entry point in the dockerfile use the connect-distributed mode script):\n\n```shell\ndocker run -v $(pwd)/config:/opt/kafka/config -p 8083:8083 ibmcase/kafkaconnect:0.0.1\n```\n\nTo illustrate the Kafka Connect distributed mode, we will add a source connector from a Mongo DB data source using [this connector](https://www.mongodb.com/kafka-connector).\n\n![Mongo source ](../images/kconnect-mongo.png)\n\nWhen using as a source, the connector publishes data changes from MongoDB into Kafka topics for streaming to consuming apps. Data is captured via Change Streams within the MongoDB cluster and published into Kafka topics. The installation of a connector is done by adding the jars from the connector into the plugin path (`/opt/connectors`) as defined in the connector properties. In the case of mongodb kafka connector the manual installation instructions are in [here](https://docs.mongodb.com/kafka-connector/current/kafka-installation/). The download page includes an uber jar.\n\nAs we run the kakfa connect as docker container, the approach is to build a new docker image based one of the Kafka image publicly available.\n\nTo define and start a connector, you do a POST to the REST API.\n\n## Verifying the connectors via the REST api\n\nThe documentation about the REST APIs for the distributed connector is in [this site](https://docs.confluent.io/current/connect/references/restapi.html).\n\nFor example the http://localhost:8083/connectors is the base URL when running locally.\n\n## Deploy the Kafka connect as a service within Openshift cluster\n\nWhen you use IBM Event Streams on Openshift, you can deploy the IBM kafka connector environment as Docker containers, and define the needed `connect-*` topics as explained in previous section. The product documentation describes how to do that.\n\nAnother approach is to use [Strimzi](https://strimzi.io/) operator.\n\nTo Be done!\n\n## Running with local kafka cluster\n\nWe are using a local kafka cluster started with docker-compose as defined in the compose file [here](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/backbone-compose.yml).\n\n* The docker network should be `kafkanet`, if not already created do the following\n\n```shell\ndocker network create kafkanet\n```\n\n* Start the kafka broker (bitnami distribution) and zookeeper node using the command below under the `refarch-kc/docker` folder:\n\n```shell\ndocker-compose -f backbone-compose.yml up -d\n```\n\n* Start a container with kafka code, to run a standalone connector: you need to use a worker configuration and a connector properties files. Those files will be mounted under the /home folder:\n\n```shell\ndocker run -ti  -rm --name kconnect -v $(pwd):/home --network kafkanet -p 8083:8083 bitnami/kafka:2 bash\n```\n\nNeed to map the port 8083, to access the REST APIs.\n\n* Inside the container starts the standalone connector:\n\n```shell\ncd /opt/bitnami/kafka\n./bin/connect-standalone.sh /home/kafka-connect/worker-standalone.properties /home/kafka-connect/file-source.properties\n```\n\nThe above file configures a file reader to source the `access_log.txt` file to the `clickstream` topic:\n\n```properties\nname=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/home/kafka-connect/access_log.txt\ntopic=clickstream\n```\n\nThe standalone connector worker configuration specifies where to connect, and what converters to use:\n\n```properties\nbootstrap.servers=kafka1:9092\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n\n# Local storage file for offset data\noffset.storage.file.filename=/tmp/connect.offsets\n```\n\nThe execution trace shows the producer id\n\n```log\nINFO [Producer clientId=connector-producer-local-file-source-0] Cluster ID: tj8y0hiZSYWHB9vLHGP1Ew (org.apache.kafka.clients.Metadata:261)\n```\n\nTo validate the data are well published run another container with the consumer console tool:\n\n```shell\ndocker run -ti  --name sinktrace --rm  --network kafkanet bitnami/kafka:2 bash -c \"\n/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic clickstream --from-beginning\"\n```\n\nAs the Json converter was used the trace show the message was wrapped into a json document with schema and payload.\n\n```json\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \\\"POST /xmlrpc.php HTTP/1.0\\\" 200 370 \\\"-\\\" \\\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\\\"\"}\n```\n\n## Further Readings\n\n* [Apache Kafka connect documentation](https://kafka.apache.org/documentation/#connect)\n* [Confluent Connector Documentation](https://docs.confluent.io/current/connect/index.html)\n* [IBM Event Streams Connectors](https://ibm.github.io/event-streams/connecting/connectors/) or [the list of supported connectors](https://ibm.github.io/event-streams/connectors/)\n* [MongoDB Connector for Apache Kafka](https://github.com/mongodb/mongo-kafka)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/kafka-connect/index.mdx"}}}}