{"componentChunkName":"component---src-pages-technology-faq-index-mdx","path":"/technology/faq/","result":{"pageContext":{"frontmatter":{"title":"Kafka Frequently Asked Questions","description":"Kafka Frequently Asked Questions from different sources and meetings we have done"},"relativePagePath":"/technology/faq/index.mdx","titleType":"append","MdxNode":{"id":"601400ad-8d68-5864-9462-10be969eb7e1","children":[],"parent":"d4ab45a2-6af0-561f-b6eb-bf0d45fd984a","internal":{"content":"---\ntitle: Kafka Frequently Asked Questions\ndescription:  Kafka Frequently Asked Questions from different sources and meetings we have done\n---\n\n\n## Basic questions\n\n### What is Kafka?\n\n* pub/sub middleware to share data between applications\n* Open source, started in 2011 by Linkedin\n* based on append log to persist immutable records ordered by arrival.\n* support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput.\n* producer has no knowledge of consumer\n* records stay even after being consumed\n* durability with replication to avoid loosing data for high availability\n\n### What are the major components?\n\n* Topic, consumer, producer, brokers\n* Rich API to control the producer semantic, and consumer\n* Consumer groups\n* Kafka streams API to support data streaming with stateful operations and stream processing topology\n* Kafka connect for source and sink connection to external systems\n* Topic replication with Mirror Maker 2\n\n### Major use cases?\n\n* Modern data pipeline with buffering to data lake\n* Data hub, to continuously expose business entities to event-driven applications and microservices\n* Real time analytics with aggregate computation, and complex event processing\n* The communication layer for Event-driven, reactive microservice.\n\n### Why does Kafka use zookeeper?\n\nKafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... \n\nZookeeper is also used to manage offset commit, and to the leader selection process.\n\n### What is a replica?\n\nA lit of nodes responsible to participate into the data replication process for a given partition. \n\nIt is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records.\n\n### What are a leader and follower in Kafka?\n\nTopic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of **leader**. When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor.\n\nIf the leader fails, one of the followers needs to take over as the leader’s role. The leader election process involves zookeeper and assess which follower was the most in-synch with the leader.\n\nLeader is the end point for read and write operations on the partition. (Exception is the new feature to read from local follower).\n\nTo get the list of In-synch Replication for a given topic the following tool can be used:\n\n```shell\nkafka-topics.sh --bootstrap-server :9092 --describe --topic <topicname>\n```\n\n### What is Offset?\n\nA unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response.\n\nConsumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset.\n\n### What is a consumer group?\n\nIt groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group.\n\nConsumer group is specified via the `group.id` consumer's property, and when consumers subscribe to topic(s).\n\nThere is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The *group leader* is responsible to do the partition assignment.\n\nWhen using the [group.instance.id](https://kafka.apache.org/documentation/#consumerconfigs_group.instance.id) properties, consumer is treated as a static member, which means there will be no partition rebalance when consumer lefts a group for a short time period. When not set the group coordinator (a broker) will allocate ids to group members, and reallocation will occur. For Kafka Streams application it is recommended to use static membership.\n\nBrokers keep offsets until a [retention period](https://kafka.apache.org/documentation/#brokerconfigs_offsets.retention.minutes) within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires.\n\nWhen the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful.\n\nThe tool `kafka-consumer-group.sh` helps getting details of consumer group:\n\n```shell\n# Inside a Kafka broker container\nbin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose\n```\n\n### Support to multi-tenancy?\n\nMulti-tenant means multiple different groups of application can produce and consumer messages isolated from other. So by constructs, topics and brokers are multi-tenant.\nNow the control will be at the access control level policy, the use of service account, and naming convention on the topic name.\nConsumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, creation operations.\n\n## How client access Kafka cluster metadata?\n\nProvide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker.\n\n## How to get at most once delivery?\n\nSet producer acknowledge level (acks) property to 0 or 1.\n\n## How to support exactly once delivery?\n\nThe goal is to address that if a producer sends a message twice the system will send only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts.\n\nSee the section in the producer implementation considerations [note](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-producers-consumers/#how-to-support-exactly-once-delivery).\n\nThe consumer needs to always read from its last committed offset.\n\nAlso it is important to note that the Kafka Stream API supports exactly once semantics with the config: `processing.guarantee=exactly_once`. Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once.\n\nExactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing.\n\n## What is range partition assignment strategy?\n\nThere are multiple partition assignment strategy for a consumer, part of a consumer group , to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members.\n\nRange assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0\n\n## What is sticky assignor?\n\nThe CooperativeStickyAssignor helps supporting incremental cooperative rebalancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance.\n\nThe goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See [KIP 429 for details.](https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol) \n\n## How to get an homogeneous distribution of message to partitions?\n\nDesign the message key and hash coding for even distributed. Or implement a customer partitioner by implementing the [Partitioner](https://kafka.apache.org/24/javadoc/?org/apache/kafka/clients/producer/Partitioner.html) interface. \n\n## How to ensure efficient join between two topics?\n\nNeed to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: *partition = hash(key) % numPartitions*.\n\n## What is transaction in Kafka?\n\nProducer can use transaction begin, commit and rollback API while publishing events to a multi partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min  inflight record set to 1).  Either all messages are successfully written or none of them are.\n\nThere are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer.\n\nSee explanations [here](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-producers-consumers/#how-to-support-exactly-once-delivery).\n\nAnd the [KIP 98](https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging)\n\n## What is the high watermark?\n\nThe high watermark offset is the offset of the last message that was successfully copied to all of the log’s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages.\n\n## Retention time for topic what does it mean?\n\nThe message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: `retention.ms` and `retention.bytes`. Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to `cleanup.policy` topic's parameter.\n\nSee the Kafka documentation on [topic configuration parameters](https://kafka.apache.org/documentation/#topicconfigs).\n\nHere is a command to create a topic with specific retention properties:\n\n```shell\nbin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config  retention.ms=55000 --add-config  retention.byte=100000\n```\n\nBut there is also the `offsets.retention.minutes` property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: `auto.offset.reset=earliest`, the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to `latest`). When using `latest`, if the consumers are offline for more than the offsets retention time window, they will lose events.\n\n## What are the topic characteristics I need to define during requirements?\n\nThis is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy.\n\n* Number of brokers in the cluster\n* retention time and size\n* Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2\n* Type of data to transport to assess message size\n* Plan to use schema management to control change to the payload definition\n* volume per day with peak and average\n* Need to do geo replication to other Kafka cluster\n* Network filesystem used on the target Kubernetes cluster and current storage class\n\n## What are the impacts of having not enough resource for Kafka?\n\nThe table in this [Event Streams product documentation](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues.\n\n## What should we do for queue full exception or timeout exception on producer?\n\nThe brokers are running behind, so we need to add more brokers and redistribute partitions.\n\n## How to send large messages?\n\nWe can set some properties at the broker, topic, consumer and producer level:\n\n* Broker: consider the [message.max.bytes](https://kafka.apache.org/documentation/#brokerconfigs_message.max.bytes) and [replica.fetch.max.bytes](https://kafka.apache.org/documentation/#brokerconfigs_replica.fetch.max.bytes)\n* Consumer: [max.partition.fetch.bytes](https://kafka.apache.org/documentation/#consumerconfigs_max.partition.fetch.bytes). Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte\n\n## How to maximize throughput?\n\nFor producer if you want to maximize throughput over low latency, set [batch.size](https://kafka.apache.org/documentation/#producerconfigs_batch.size) and [linger.ms](https://kafka.apache.org/documentation/#producerconfigs_linger.ms) to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together.\n\n## Why Kafka Stream applications may impact cluster performance?\n\n* They may use internal hidden topics to persist their states for Ktable and GlobalKTable.\n* Process input and output topics\n\n## How message schema version is propagated?\n\nThe record includes a byte with the version number from the schema registry.\n\n## Consumers do not see message in topic, what happens?\n\nThe brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit from a producer.\n\nThe consumer has a communication issue, or fails, so the consumer group rebalance is underway.\n\n## How compression schema used is known by the consumer?\n\nThe record header includes such metadata. So it is possible to have different schema per record.\n\n## What does out-of-synch partition mean and occur?\n\nWith partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered “committed” when all ISRs for a partition wrote to their log. Only committed records are readable from consumer.\n\nSo out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected.\n\n## Security in Kafka\n\n* Encrypt data in transit between producer and Kafka brokers\n* Client authentication\n* Client authorization\n\n## How to protect data at rest?\n\n* Use encrypted file system for each brokers\n* Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the appeld log will be encrypted.\n\n## How to remove personal identifying information?\n\nFrom the source connector, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII.\n\n## How to handle variable workload with Kafka Connector source connector?\n\nIncrease and decrease the number of Kafka connect workers based upon current application load.\n\n## Competitors to Kafka\n\n* [NATS]()\n* [Redpanda](https://vectorized.io/) a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations.\n* [AWS Kinesis](https://jbcodeforce.github.io/architecture/aws/#kinesis)\n\t* Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used.\n\t* 24h to 7 days persistence\n\t* Number of shards are adaptable with throughput.\n\t* Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob.\n\t* restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, < 2MB per shard, 1000 write /s)\n\t* Server side encryption using master key managed by AWS KMS\n* GCP Pub/sub\n* Solace\n* Active MQ:\n\t* Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. \n\t* various messaging protocols including AMQP, STOMP, and MQTT\n\t* It maintains the delivery state of every message resulting in lower throughput.\n\t* Can apply JMS message selector to consumer specific message\n\t* Point to point or pub/sub, but servers push messages to consumer/subscribers\n\t* Performance of both queue and topic degrades as the number of consumers rises\n* Rabbit MQ:\n\t* Support queues, with messages removed once consumed\n\t* Add the concept of Exchange to route message to queues\n\t* Limited throughput, but can send large message\n\t* Support JMS, AMQP protocols, and participation to transaction\n\t* Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers.\n\n\n## Differences between Akka and Kafka?\n\n[Akka](https://akka.io/) is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. \nKafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture.\n\n[vert.x](https://vertx.io/) is another open source implementation of such internal messaging mechanism but supporting more language:  Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.\n\n\n## Run Kafka Test Container with TopologyTestDriver\n\nTopology Test Driver is used without kafka, so there is no real need to use test container. \n\n## Event streams resource requirements \n\nSee the [detailed tables](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) in the product documentation.\n\n## Security setting\n\nOn Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. \n\nIf no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication.\n\n\n* Mutual TLS authentication for internal communication looks like:\n\n```yaml\n- name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n      type: tls\n```\n\nTo connect any app (producer, consumer) we need a TLS user like:\n\n```yaml\npiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaUser\nmetadata:\n  name: tls-user\n  labels:\n    strimzi.io/cluster: vaccine-kafka\nspec:\n  authentication:\n    type: tls\n```\n\nThen the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password)\n\n```shell\noc describe secret tls-user\nData\n====\nca.crt:         1164 bytes\nuser.crt:       1009 bytes\nuser.key:       1704 bytes\nuser.p12:       2374 bytes\nuser.password:  12 bytes\n```\n\nFor Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to `/deployments/certs/user`. \n\n```shell\n%prod.kafka.security.protocol=SSL\n%prod.kafka.ssl.keystore.location=/deployments/certs/user/user.p12\n%prod.kafka.ssl.keystore.type=PKCS12\nquarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.from-secret=${KAFKA_USER:tls-user}\nquarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.with-key=user.password\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=${KAFKA_USER:tls-user}\n# To validate server side certificate we will mount it too with the following declaration\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\nquarkus.openshift.mounts.kafka-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.kafka-cert.secret-name=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n```\n\nFor the server side certificate, it will be in a truststore, which is mounted to  `/deployments/certs/server` and from a secret (this secret is created at the cluster level).\n\nAlso because we also use TLS for encryption we need:\n\n```\n%prod.kafka.ssl.protocol=TLSv1.2\n```\n\nMutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates.\n\n* SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections.\n    * The listener declaration:\n    ```yaml\n    - name: external\n    port: 9094\n    type: route\n    tls: true\n    authentication:\n      type: scram-sha-512\n    ```\n    * Need a scram-user:\n\n    ```yaml\n    apiVersion: kafka.strimzi.io/v1beta1\n    kind: KafkaUser\n    metadata:\n    name: scram-user\n    labels:\n        strimzi.io/cluster: vaccine-kafka\n    spec:\n    authentication:\n        type: scram-sha-512\n    ```\n\nThen the app properties need:\n\n```shell\nsecurity.protocol=SASL_SSL\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret=${KAFKA_USER:scram-user}\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key=password\n%prod.quarkus.openshift.mounts.kafka-cert.path=/deployments/certs/server\n%prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n```\n\n## Verify consumer connection\n\nHere is an example of TLS authentication for Event streams\n\n```\nConsumerConfig values: \n\tbootstrap.servers = [eda-dev-kafka-bootstrap.eventstreams.svc:9093]\n\tcheck.crcs = true\n\tclient.dns.lookup = default\n\tclient.id = cold-chain-agent-c2c11228-d876-4db2-a16a-ea7826e358d2-StreamThread-1-restore-consumer\n\tclient.rack = \n\tconnections.max.idle.ms = 540000\n\tdefault.api.timeout.ms = 60000\n\tenable.auto.commit = false\n\texclude.internal.topics = true\n\tfetch.max.bytes = 52428800\n\tfetch.max.wait.ms = 500\n\tfetch.min.bytes = 1\n\tgroup.id = null\n\tgroup.instance.id = null\n\theartbeat.interval.ms = 3000\n\tinterceptor.classes = []\n\tinternal.leave.group.on.close = false\n\tisolation.level = read_uncommitted\n\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n\tsasl.client.callback.handler.class = null\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.mechanism = GSSAPI\n\tsecurity.protocol = SSL\n\tsecurity.providers = null\n\tsend.buffer.bytes = 131072\n\tsession.timeout.ms = 10000\n\tssl.cipher.suites = null\n\tssl.enabled.protocols = [TLSv1.2]\n\tssl.endpoint.identification.algorithm = https\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.location = /deployments/certs/user/user.p12\n\tssl.keystore.password = [hidden]\n\tssl.keystore.type = PKCS12\n\tssl.protocol = TLSv1.2\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.location = /deployments/certs/server/ca.p12\n\tssl.truststore.password = [hidden]\n\tssl.truststore.type = PKCS12\n\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n```\n\n\n## Other FAQs\n\n* [IBM Event streams on Cloud FAQ](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-faqs) \n\n* [FAQ from Confluent](https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-HowareKafkabrokersdependonZookeeper?)","type":"Mdx","contentDigest":"4eef636274baf395904eb69f9ddd865a","owner":"gatsby-plugin-mdx","counter":702},"frontmatter":{"title":"Kafka Frequently Asked Questions","description":"Kafka Frequently Asked Questions from different sources and meetings we have done"},"exports":{},"rawBody":"---\ntitle: Kafka Frequently Asked Questions\ndescription:  Kafka Frequently Asked Questions from different sources and meetings we have done\n---\n\n\n## Basic questions\n\n### What is Kafka?\n\n* pub/sub middleware to share data between applications\n* Open source, started in 2011 by Linkedin\n* based on append log to persist immutable records ordered by arrival.\n* support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput.\n* producer has no knowledge of consumer\n* records stay even after being consumed\n* durability with replication to avoid loosing data for high availability\n\n### What are the major components?\n\n* Topic, consumer, producer, brokers\n* Rich API to control the producer semantic, and consumer\n* Consumer groups\n* Kafka streams API to support data streaming with stateful operations and stream processing topology\n* Kafka connect for source and sink connection to external systems\n* Topic replication with Mirror Maker 2\n\n### Major use cases?\n\n* Modern data pipeline with buffering to data lake\n* Data hub, to continuously expose business entities to event-driven applications and microservices\n* Real time analytics with aggregate computation, and complex event processing\n* The communication layer for Event-driven, reactive microservice.\n\n### Why does Kafka use zookeeper?\n\nKafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... \n\nZookeeper is also used to manage offset commit, and to the leader selection process.\n\n### What is a replica?\n\nA lit of nodes responsible to participate into the data replication process for a given partition. \n\nIt is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records.\n\n### What are a leader and follower in Kafka?\n\nTopic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of **leader**. When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor.\n\nIf the leader fails, one of the followers needs to take over as the leader’s role. The leader election process involves zookeeper and assess which follower was the most in-synch with the leader.\n\nLeader is the end point for read and write operations on the partition. (Exception is the new feature to read from local follower).\n\nTo get the list of In-synch Replication for a given topic the following tool can be used:\n\n```shell\nkafka-topics.sh --bootstrap-server :9092 --describe --topic <topicname>\n```\n\n### What is Offset?\n\nA unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response.\n\nConsumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset.\n\n### What is a consumer group?\n\nIt groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group.\n\nConsumer group is specified via the `group.id` consumer's property, and when consumers subscribe to topic(s).\n\nThere is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The *group leader* is responsible to do the partition assignment.\n\nWhen using the [group.instance.id](https://kafka.apache.org/documentation/#consumerconfigs_group.instance.id) properties, consumer is treated as a static member, which means there will be no partition rebalance when consumer lefts a group for a short time period. When not set the group coordinator (a broker) will allocate ids to group members, and reallocation will occur. For Kafka Streams application it is recommended to use static membership.\n\nBrokers keep offsets until a [retention period](https://kafka.apache.org/documentation/#brokerconfigs_offsets.retention.minutes) within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires.\n\nWhen the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful.\n\nThe tool `kafka-consumer-group.sh` helps getting details of consumer group:\n\n```shell\n# Inside a Kafka broker container\nbin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose\n```\n\n### Support to multi-tenancy?\n\nMulti-tenant means multiple different groups of application can produce and consumer messages isolated from other. So by constructs, topics and brokers are multi-tenant.\nNow the control will be at the access control level policy, the use of service account, and naming convention on the topic name.\nConsumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, creation operations.\n\n## How client access Kafka cluster metadata?\n\nProvide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker.\n\n## How to get at most once delivery?\n\nSet producer acknowledge level (acks) property to 0 or 1.\n\n## How to support exactly once delivery?\n\nThe goal is to address that if a producer sends a message twice the system will send only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts.\n\nSee the section in the producer implementation considerations [note](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-producers-consumers/#how-to-support-exactly-once-delivery).\n\nThe consumer needs to always read from its last committed offset.\n\nAlso it is important to note that the Kafka Stream API supports exactly once semantics with the config: `processing.guarantee=exactly_once`. Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once.\n\nExactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing.\n\n## What is range partition assignment strategy?\n\nThere are multiple partition assignment strategy for a consumer, part of a consumer group , to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members.\n\nRange assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0\n\n## What is sticky assignor?\n\nThe CooperativeStickyAssignor helps supporting incremental cooperative rebalancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance.\n\nThe goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See [KIP 429 for details.](https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol) \n\n## How to get an homogeneous distribution of message to partitions?\n\nDesign the message key and hash coding for even distributed. Or implement a customer partitioner by implementing the [Partitioner](https://kafka.apache.org/24/javadoc/?org/apache/kafka/clients/producer/Partitioner.html) interface. \n\n## How to ensure efficient join between two topics?\n\nNeed to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: *partition = hash(key) % numPartitions*.\n\n## What is transaction in Kafka?\n\nProducer can use transaction begin, commit and rollback API while publishing events to a multi partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min  inflight record set to 1).  Either all messages are successfully written or none of them are.\n\nThere are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer.\n\nSee explanations [here](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-producers-consumers/#how-to-support-exactly-once-delivery).\n\nAnd the [KIP 98](https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging)\n\n## What is the high watermark?\n\nThe high watermark offset is the offset of the last message that was successfully copied to all of the log’s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages.\n\n## Retention time for topic what does it mean?\n\nThe message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: `retention.ms` and `retention.bytes`. Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to `cleanup.policy` topic's parameter.\n\nSee the Kafka documentation on [topic configuration parameters](https://kafka.apache.org/documentation/#topicconfigs).\n\nHere is a command to create a topic with specific retention properties:\n\n```shell\nbin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config  retention.ms=55000 --add-config  retention.byte=100000\n```\n\nBut there is also the `offsets.retention.minutes` property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: `auto.offset.reset=earliest`, the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to `latest`). When using `latest`, if the consumers are offline for more than the offsets retention time window, they will lose events.\n\n## What are the topic characteristics I need to define during requirements?\n\nThis is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy.\n\n* Number of brokers in the cluster\n* retention time and size\n* Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2\n* Type of data to transport to assess message size\n* Plan to use schema management to control change to the payload definition\n* volume per day with peak and average\n* Need to do geo replication to other Kafka cluster\n* Network filesystem used on the target Kubernetes cluster and current storage class\n\n## What are the impacts of having not enough resource for Kafka?\n\nThe table in this [Event Streams product documentation](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues.\n\n## What should we do for queue full exception or timeout exception on producer?\n\nThe brokers are running behind, so we need to add more brokers and redistribute partitions.\n\n## How to send large messages?\n\nWe can set some properties at the broker, topic, consumer and producer level:\n\n* Broker: consider the [message.max.bytes](https://kafka.apache.org/documentation/#brokerconfigs_message.max.bytes) and [replica.fetch.max.bytes](https://kafka.apache.org/documentation/#brokerconfigs_replica.fetch.max.bytes)\n* Consumer: [max.partition.fetch.bytes](https://kafka.apache.org/documentation/#consumerconfigs_max.partition.fetch.bytes). Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte\n\n## How to maximize throughput?\n\nFor producer if you want to maximize throughput over low latency, set [batch.size](https://kafka.apache.org/documentation/#producerconfigs_batch.size) and [linger.ms](https://kafka.apache.org/documentation/#producerconfigs_linger.ms) to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together.\n\n## Why Kafka Stream applications may impact cluster performance?\n\n* They may use internal hidden topics to persist their states for Ktable and GlobalKTable.\n* Process input and output topics\n\n## How message schema version is propagated?\n\nThe record includes a byte with the version number from the schema registry.\n\n## Consumers do not see message in topic, what happens?\n\nThe brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit from a producer.\n\nThe consumer has a communication issue, or fails, so the consumer group rebalance is underway.\n\n## How compression schema used is known by the consumer?\n\nThe record header includes such metadata. So it is possible to have different schema per record.\n\n## What does out-of-synch partition mean and occur?\n\nWith partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered “committed” when all ISRs for a partition wrote to their log. Only committed records are readable from consumer.\n\nSo out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected.\n\n## Security in Kafka\n\n* Encrypt data in transit between producer and Kafka brokers\n* Client authentication\n* Client authorization\n\n## How to protect data at rest?\n\n* Use encrypted file system for each brokers\n* Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the appeld log will be encrypted.\n\n## How to remove personal identifying information?\n\nFrom the source connector, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII.\n\n## How to handle variable workload with Kafka Connector source connector?\n\nIncrease and decrease the number of Kafka connect workers based upon current application load.\n\n## Competitors to Kafka\n\n* [NATS]()\n* [Redpanda](https://vectorized.io/) a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations.\n* [AWS Kinesis](https://jbcodeforce.github.io/architecture/aws/#kinesis)\n\t* Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used.\n\t* 24h to 7 days persistence\n\t* Number of shards are adaptable with throughput.\n\t* Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob.\n\t* restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, < 2MB per shard, 1000 write /s)\n\t* Server side encryption using master key managed by AWS KMS\n* GCP Pub/sub\n* Solace\n* Active MQ:\n\t* Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. \n\t* various messaging protocols including AMQP, STOMP, and MQTT\n\t* It maintains the delivery state of every message resulting in lower throughput.\n\t* Can apply JMS message selector to consumer specific message\n\t* Point to point or pub/sub, but servers push messages to consumer/subscribers\n\t* Performance of both queue and topic degrades as the number of consumers rises\n* Rabbit MQ:\n\t* Support queues, with messages removed once consumed\n\t* Add the concept of Exchange to route message to queues\n\t* Limited throughput, but can send large message\n\t* Support JMS, AMQP protocols, and participation to transaction\n\t* Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers.\n\n\n## Differences between Akka and Kafka?\n\n[Akka](https://akka.io/) is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. \nKafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture.\n\n[vert.x](https://vertx.io/) is another open source implementation of such internal messaging mechanism but supporting more language:  Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.\n\n\n## Run Kafka Test Container with TopologyTestDriver\n\nTopology Test Driver is used without kafka, so there is no real need to use test container. \n\n## Event streams resource requirements \n\nSee the [detailed tables](https://ibm.github.io/event-streams/installing/prerequisites/#helm-resource-requirements) in the product documentation.\n\n## Security setting\n\nOn Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. \n\nIf no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication.\n\n\n* Mutual TLS authentication for internal communication looks like:\n\n```yaml\n- name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n      type: tls\n```\n\nTo connect any app (producer, consumer) we need a TLS user like:\n\n```yaml\npiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaUser\nmetadata:\n  name: tls-user\n  labels:\n    strimzi.io/cluster: vaccine-kafka\nspec:\n  authentication:\n    type: tls\n```\n\nThen the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password)\n\n```shell\noc describe secret tls-user\nData\n====\nca.crt:         1164 bytes\nuser.crt:       1009 bytes\nuser.key:       1704 bytes\nuser.p12:       2374 bytes\nuser.password:  12 bytes\n```\n\nFor Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to `/deployments/certs/user`. \n\n```shell\n%prod.kafka.security.protocol=SSL\n%prod.kafka.ssl.keystore.location=/deployments/certs/user/user.p12\n%prod.kafka.ssl.keystore.type=PKCS12\nquarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.from-secret=${KAFKA_USER:tls-user}\nquarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.with-key=user.password\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=${KAFKA_USER:tls-user}\n# To validate server side certificate we will mount it too with the following declaration\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\nquarkus.openshift.mounts.kafka-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.kafka-cert.secret-name=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n```\n\nFor the server side certificate, it will be in a truststore, which is mounted to  `/deployments/certs/server` and from a secret (this secret is created at the cluster level).\n\nAlso because we also use TLS for encryption we need:\n\n```\n%prod.kafka.ssl.protocol=TLSv1.2\n```\n\nMutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates.\n\n* SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections.\n    * The listener declaration:\n    ```yaml\n    - name: external\n    port: 9094\n    type: route\n    tls: true\n    authentication:\n      type: scram-sha-512\n    ```\n    * Need a scram-user:\n\n    ```yaml\n    apiVersion: kafka.strimzi.io/v1beta1\n    kind: KafkaUser\n    metadata:\n    name: scram-user\n    labels:\n        strimzi.io/cluster: vaccine-kafka\n    spec:\n    authentication:\n        type: scram-sha-512\n    ```\n\nThen the app properties need:\n\n```shell\nsecurity.protocol=SASL_SSL\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret=${KAFKA_USER:scram-user}\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key=password\n%prod.quarkus.openshift.mounts.kafka-cert.path=/deployments/certs/server\n%prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n```\n\n## Verify consumer connection\n\nHere is an example of TLS authentication for Event streams\n\n```\nConsumerConfig values: \n\tbootstrap.servers = [eda-dev-kafka-bootstrap.eventstreams.svc:9093]\n\tcheck.crcs = true\n\tclient.dns.lookup = default\n\tclient.id = cold-chain-agent-c2c11228-d876-4db2-a16a-ea7826e358d2-StreamThread-1-restore-consumer\n\tclient.rack = \n\tconnections.max.idle.ms = 540000\n\tdefault.api.timeout.ms = 60000\n\tenable.auto.commit = false\n\texclude.internal.topics = true\n\tfetch.max.bytes = 52428800\n\tfetch.max.wait.ms = 500\n\tfetch.min.bytes = 1\n\tgroup.id = null\n\tgroup.instance.id = null\n\theartbeat.interval.ms = 3000\n\tinterceptor.classes = []\n\tinternal.leave.group.on.close = false\n\tisolation.level = read_uncommitted\n\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n\tsasl.client.callback.handler.class = null\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.mechanism = GSSAPI\n\tsecurity.protocol = SSL\n\tsecurity.providers = null\n\tsend.buffer.bytes = 131072\n\tsession.timeout.ms = 10000\n\tssl.cipher.suites = null\n\tssl.enabled.protocols = [TLSv1.2]\n\tssl.endpoint.identification.algorithm = https\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.location = /deployments/certs/user/user.p12\n\tssl.keystore.password = [hidden]\n\tssl.keystore.type = PKCS12\n\tssl.protocol = TLSv1.2\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.location = /deployments/certs/server/ca.p12\n\tssl.truststore.password = [hidden]\n\tssl.truststore.type = PKCS12\n\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n```\n\n\n## Other FAQs\n\n* [IBM Event streams on Cloud FAQ](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-faqs) \n\n* [FAQ from Confluent](https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-HowareKafkabrokersdependonZookeeper?)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/faq/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}