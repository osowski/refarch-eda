{"componentChunkName":"component---src-pages-technology-kafka-mirrormaker-index-mdx","path":"/technology/kafka-mirrormaker/","result":{"pageContext":{"frontmatter":{"title":"Kafka Mirror Maker 2","description":"Kafka Mirror Maker 2"},"relativePagePath":"/technology/kafka-mirrormaker/index.mdx","titleType":"append","MdxNode":{"id":"6a0a26a8-1ae5-556d-b1f1-194d5080a1e2","children":[],"parent":"5daff97d-73d5-559c-8cb8-e95f0e85aa65","internal":{"content":"---\ntitle: Kafka Mirror Maker 2\ndescription: Kafka Mirror Maker 2\n---\n\nThis section introduces **Mirror Maker 2.0**, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - [KIP 382](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0).\n\n## General concepts\n\nAs [Mirror maker 2.0](https://strimzi.io/docs/master/#con-configuring-mirror-maker-deployment-configuration-kafka-mirror-maker) is using Kafka Connect framework, we recommend to review our summary of Kafka Connect [here](../kafka-connect/).\n\nThe figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.\n\n![Kafka Connect](../images/mm-k-connect.png)\n\nMirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. In distributed mode, MirrorMaker 2.0 creates the following topics on the target cluster:\n\n* mm2-configs.source.internal: This topic is used to store the connector and task configuration.\n* mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect.\n* mm2-status.source.internal: This topic is used to store status updates of connectors and tasks.\n* source.heartbeats\n* source.checkpoints.internal\n\nA typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol.  IBM Event Streams is a support, enterprise version of Apache Kafka by IBM.\n\n```properties\nclusters=source, target\nsource.bootstrap.servers=${KAFKA_SOURCE_BROKERS}\ntarget.bootstrap.servers=${KAFKA_TARGET_BROKERS}\ntarget.security.protocol=SASL_SSL\ntarget.ssl.protocol=TLSv1.2\ntarget.ssl.endpoint.identification.algorithm=https\ntarget.sasl.mechanism=PLAIN\ntarget.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY};\n# enable and configure individual replication flows\nsource->target.enabled=true\nsource->target.topics=products\ntasks.max=10\n```\n\n* Topics are configured to be replicated or not using a _whitelist_ and _blacklist_ concept\n* White listed topics are set with the `source->target.topics` attribute of the replication flow and uses [Java regular expression](https://www.vogella.com/tutorials/JavaRegularExpressions/article.html) syntax.\n* Blacklisted topics: by default the following pattern is applied:\n\n```properties\nblacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas]\n```\n\nWe can also define the _blacklist_ with the properties: `topics.blacklist`. Comma-separated lists and Java Regular Expressions are supported.\n\nInternally, `MirrorSourceConnector` and `MirrorCheckpointConnector` will create multiple Kafka tasks (up to the value of `tasks.max` property), and `MirrorHeartbeatConnector` creates an additional task. `MirrorSourceConnector` will have one task per topic-partition combination to replicate, while `MirrorCheckpointConnector` will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the `assign()` API, so there is no consumer group used while fetching data from source topics. There is no call to `commit()` either; rebalancing occurs only when there is a new topic created that matches the _whitelist_ pattern.\n\n## Scenarios\n\nThe following table summarizes the different scenarios we have tested for data replication using both the Strimzi Operator and IBM Event Streams:\n\n| Environment | Source                 | Target                 | Connect |\n|:-----------:|------------------------|------------------------|---------|\n| 1           | Local                  | Event Streams on Cloud | Local   |\n| 2           | Strimzi on OCP         | Event Streams on Cloud | OCP / ROKS |\n| 3           | Event Streams on Cloud | Local                  | Local   |\n| 4           | Event Streams on Cloud | Strimzi on OCP         | OCP / ROKS |\n| 5           | Event Streams on OCP   | Event Streams on Cloud | OCP / ROKS |\n\n\\*The connect column defines where the MirrorMaker 2 connect to.\n\n\n\nTo deploy MirrorMaker2 the tool, we can use the Strimzi Kafka latest docker image deployed on Openshift cluster (We address Strimzi deployment in [this note](../../technology/kafka-mirrormaker/)).\n\nTo define the clusters and topic configuration we use yaml files. One simple example to replicate from IBM Cloud Event streams to Kafka on premise is in the folder [deployments/strimzi/es-mirror-maker.properties](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/strimzi/es-mirror-maker.properties)\n\nUsing the same kafka image we can start a mirror maker container with:\n\n```properties\nclusters = source, target\nsource.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\nsource.security.protocol=SSL\nsource.ssl.truststore.password=password\nsource.ssl.truststore.location=/home/truststore.jks\ntarget.bootstrap.servers = kafka1:9092\n# enable and configure individual replication flows\nsource->target.enabled = true\nsource->target.topics = test\n```\n\n```bash\n./connect-mirror-maker.sh /home/strimzi.properties\n```\nWhen Mirror maker starts it will create some topics on source cluster to manage the offsets and topic metadata:\n\n```\nmm2-configs.target.internal                                   1            3\nmm2-offset-syncs.target.internal                              1            3\nmm2-offsets.target.internal                                   25           3\nmm2-status.target.internal                                    5            3\n```\n\nAnd on the target cluster:\n\n```\n__consumer_offsets\nheartbeats\nmm2-configs.source.internal\nmm2-offsets.source.internal\nmm2-status.source.internal\nsource.checkpoints.internal\nsource.heartbeats\nsource.test\n```\n\nThe `source.test` topic is the replicated `test` topic from the source cluster.\n\n![](../images/mm-k-connect.png)\n\n## Where to go next\n\nYou can read more about how to configure the different scenarios above, how they work, how to apply security in data replication, how to monitor Mirror Maker and much more detailed information on data replication [**here**](https://ibm-cloud-architecture.github.io/refarch-eda-data-consistency/)","type":"Mdx","contentDigest":"1be06885ab798ec352e8b4d3e4c34935","counter":353,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Kafka Mirror Maker 2","description":"Kafka Mirror Maker 2"},"exports":{},"rawBody":"---\ntitle: Kafka Mirror Maker 2\ndescription: Kafka Mirror Maker 2\n---\n\nThis section introduces **Mirror Maker 2.0**, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - [KIP 382](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0).\n\n## General concepts\n\nAs [Mirror maker 2.0](https://strimzi.io/docs/master/#con-configuring-mirror-maker-deployment-configuration-kafka-mirror-maker) is using Kafka Connect framework, we recommend to review our summary of Kafka Connect [here](../kafka-connect/).\n\nThe figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.\n\n![Kafka Connect](../images/mm-k-connect.png)\n\nMirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. In distributed mode, MirrorMaker 2.0 creates the following topics on the target cluster:\n\n* mm2-configs.source.internal: This topic is used to store the connector and task configuration.\n* mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect.\n* mm2-status.source.internal: This topic is used to store status updates of connectors and tasks.\n* source.heartbeats\n* source.checkpoints.internal\n\nA typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol.  IBM Event Streams is a support, enterprise version of Apache Kafka by IBM.\n\n```properties\nclusters=source, target\nsource.bootstrap.servers=${KAFKA_SOURCE_BROKERS}\ntarget.bootstrap.servers=${KAFKA_TARGET_BROKERS}\ntarget.security.protocol=SASL_SSL\ntarget.ssl.protocol=TLSv1.2\ntarget.ssl.endpoint.identification.algorithm=https\ntarget.sasl.mechanism=PLAIN\ntarget.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY};\n# enable and configure individual replication flows\nsource->target.enabled=true\nsource->target.topics=products\ntasks.max=10\n```\n\n* Topics are configured to be replicated or not using a _whitelist_ and _blacklist_ concept\n* White listed topics are set with the `source->target.topics` attribute of the replication flow and uses [Java regular expression](https://www.vogella.com/tutorials/JavaRegularExpressions/article.html) syntax.\n* Blacklisted topics: by default the following pattern is applied:\n\n```properties\nblacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas]\n```\n\nWe can also define the _blacklist_ with the properties: `topics.blacklist`. Comma-separated lists and Java Regular Expressions are supported.\n\nInternally, `MirrorSourceConnector` and `MirrorCheckpointConnector` will create multiple Kafka tasks (up to the value of `tasks.max` property), and `MirrorHeartbeatConnector` creates an additional task. `MirrorSourceConnector` will have one task per topic-partition combination to replicate, while `MirrorCheckpointConnector` will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the `assign()` API, so there is no consumer group used while fetching data from source topics. There is no call to `commit()` either; rebalancing occurs only when there is a new topic created that matches the _whitelist_ pattern.\n\n## Scenarios\n\nThe following table summarizes the different scenarios we have tested for data replication using both the Strimzi Operator and IBM Event Streams:\n\n| Environment | Source                 | Target                 | Connect |\n|:-----------:|------------------------|------------------------|---------|\n| 1           | Local                  | Event Streams on Cloud | Local   |\n| 2           | Strimzi on OCP         | Event Streams on Cloud | OCP / ROKS |\n| 3           | Event Streams on Cloud | Local                  | Local   |\n| 4           | Event Streams on Cloud | Strimzi on OCP         | OCP / ROKS |\n| 5           | Event Streams on OCP   | Event Streams on Cloud | OCP / ROKS |\n\n\\*The connect column defines where the MirrorMaker 2 connect to.\n\n\n\nTo deploy MirrorMaker2 the tool, we can use the Strimzi Kafka latest docker image deployed on Openshift cluster (We address Strimzi deployment in [this note](../../technology/kafka-mirrormaker/)).\n\nTo define the clusters and topic configuration we use yaml files. One simple example to replicate from IBM Cloud Event streams to Kafka on premise is in the folder [deployments/strimzi/es-mirror-maker.properties](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/strimzi/es-mirror-maker.properties)\n\nUsing the same kafka image we can start a mirror maker container with:\n\n```properties\nclusters = source, target\nsource.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\nsource.security.protocol=SSL\nsource.ssl.truststore.password=password\nsource.ssl.truststore.location=/home/truststore.jks\ntarget.bootstrap.servers = kafka1:9092\n# enable and configure individual replication flows\nsource->target.enabled = true\nsource->target.topics = test\n```\n\n```bash\n./connect-mirror-maker.sh /home/strimzi.properties\n```\nWhen Mirror maker starts it will create some topics on source cluster to manage the offsets and topic metadata:\n\n```\nmm2-configs.target.internal                                   1            3\nmm2-offset-syncs.target.internal                              1            3\nmm2-offsets.target.internal                                   25           3\nmm2-status.target.internal                                    5            3\n```\n\nAnd on the target cluster:\n\n```\n__consumer_offsets\nheartbeats\nmm2-configs.source.internal\nmm2-offsets.source.internal\nmm2-status.source.internal\nsource.checkpoints.internal\nsource.heartbeats\nsource.test\n```\n\nThe `source.test` topic is the replicated `test` topic from the source cluster.\n\n![](../images/mm-k-connect.png)\n\n## Where to go next\n\nYou can read more about how to configure the different scenarios above, how they work, how to apply security in data replication, how to monitor Mirror Maker and much more detailed information on data replication [**here**](https://ibm-cloud-architecture.github.io/refarch-eda-data-consistency/)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/kafka-mirrormaker/index.mdx"}}}}