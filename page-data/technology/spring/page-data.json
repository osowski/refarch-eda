{"componentChunkName":"component---src-pages-technology-spring-index-mdx","path":"/technology/spring/","result":{"pageContext":{"frontmatter":{"title":"Spring Cloud and Spring Cloud Stream","description":"Spring Cloud and Spring Cloud Stream quick techno overview"},"relativePagePath":"/technology/spring/index.mdx","titleType":"append","MdxNode":{"id":"28d6af36-0a2f-5135-b400-a7dcfe553228","children":[],"parent":"fd3134a4-eedc-5bc2-9bd2-5665635ac8a3","internal":{"content":"---\ntitle: Spring Cloud and Spring Cloud Stream\ndescription:  Spring Cloud and Spring Cloud Stream quick techno overview\n---\n\n**Audience**: Developers\n\n## Spring Cloud\n\n[Spring Cloud](https://spring.io/projects/spring-cloud) is based on Spring boot programming model but focusing on cloud native deployment and distributed computing. As other spring boot app it includes jetty or tomcat, health checks, metrics... It supports the following patterns:\n\n* Distributed/versioned [configuration](https://spring.io/projects/spring-cloud-config): externalize config in distributed system with config server.\n* Service registration and discovery: uses Netflix Eureka, Apache Zookeeper or Consul to keep service information. \n* Routing: supports HTTP (Open Feign or  Netflix Ribbon for load balancing) and messaging (RabbitMQ and Kafka)\n* Service-to-service calls: Sptring Cloud Gateway and Netflix Zuul is used\n* Load balancing\n* Circuit Breakers: based on Netflix Hystrix: if the request fails for n time, the circuit open.   \n* Global locks\n* Leadership election and cluster state\n* Distributed messaging\n\nIt also supports pipelines for ci/cd and contract testing for interface validation. \n\n### Getting started\n\nUse [start.spring.io](https://start.spring.io/) to create the application starting code using Kafka, Actuator, Cloud Stream or add the Spring Cloud BOM to your maven `pom.xml` file. See [the Adding Spring Cloud To An Existing Spring Boot Application section.](https://spring.io/projects/spring-cloud)\n\nAs most of the microservices expose REST resource, we may need to add the starter web:\n\n```xml\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n```\n\nWe also need to install the [Spring Cloud CLI](https://cloud.spring.io/spring-cloud-cli/).\n\nThen add the Spring cloud starter as dependency. When using config server, we need to add the config client. \n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-conflig-client</artifactId>\n</dependency>\n```\n\nFor centralized tracing uses, starter-sleuth, and zipkin.\n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-sleuth</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-zipkin</artifactId>\n</dependency>\n```\n\nFor service discovery add netflix-eureka-client.\n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-eureka</artifactId>\n</dependency>\n```\n\n\nUsing the Spring Cloud CLI we can get the service registry, config server, central tracing started in one command:\n\n```shell\nspring cloud eureka configserver zipkin\n```\n\n### Spring Cloud config\n\nUse the concept of Config Server you have a central place to manage external properties for applications across all environments.  As an application moves through the deployment pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run when they migrate. \n\n```java\n  @Value(\"${config.in.topic}\")\n  String topicName = \"orders\";\n```\n\nThe value of the `config.in.topic` comes from local configuration or remote config server. The config server will serve content from a git. See [this sample](https://github.com/spring-cloud-samples/configserver) for such server.\n\n\n\n## Spring Cloud Stream\n\n[Spring Cloud Stream](https://spring.io/projects/spring-cloud-stream) is a framework for building highly scalable event-driven microservices connected with shared messaging systems. It unifies lots of popular messaging platforms behind one easy to use API including RabbitMQ, Apache Kafka, Amazon Kinesis, Google PubSub, Solace PubSub+, Azure Event Hubs, and Apache RocketMQ. \n\nSpring Cloud Stream is an abstraction that uses the following important concepts to supporet middleware encapsulation: **destination binders** (integration with messaging systems like Kafka or RabbitMQ), **destination bindings** (bridge code to external systems) and **message** (canonical data model to communicate between producer and consumer). \n\nAs other Spring boot application, it uses extrernal properties to manage most of the configuration of the binders and binding.\n\n[Spring Cloud Stream Applications](https://spring.io/projects/spring-cloud-stream-applications) are standalone executable applications that communicate over messaging middleware such as Apache Kafka and RabbitMQ. The app is using uber-jars to get the minimal required library and code The following diagram illustrates those concepts for a Spring cloud app:\n\n![](./images/spring-stream-app.png)\n\n**Attention** Spring Cloud Stream is not Kafka Streams or Kafka API, it is similar but it represents another abstraction. From a Kafka developer's point of view, it does not seem relevant, as why not using Kafka API and Kafka Streams API, but this is a way to encapsulate any middleware supporting pub/sub and queueing. It may be more comparable to Microprofile reactive messaging specifications and APIs, but not compatible with it. For example binding can be compared to channel of the microprofile reactive messaging constructs.\n\nSo the development decision will be around middleware abstraction and the way to simplify going from one middleware to another. Now with Kafka, because of its long retention time, it means we can have any type of consumers to read the messages at any time. Those consumers may use Kafka API (Python app or nodejs apps), in this case using the Kafka API within the Spring boot application is a better approach, as the way the abstraction is used may not be fully compatible to any Kafka consumer types.\n\nWith Kafka based application the best practice is also to define the message structure, using Avro or Protbuf, and use schema registry to ensure compatibility management between applications. To support that Spring Cloud Stream support using native (to the middleware) serialization, which in the case of Kafka could be any serdes APIs or avro API. We will cover that [in later section](#avro-serialization).\n\n\n### Example of Kafka binding\n\nThe [order service spring cloud template](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/spring-cloud-stream) is a simple example of order service that exposes CRUD operations on the Order entity via a controller. Instead of writing to a database, this service immediately generates a message to Kafka and then the repository class consumes the message to get the data to write to the database. This is a simple way to implement 'transaction' by using the Append log of Kafka partition as a transaction log.\n\n![](./images/spring-orderms-app.png)\n\nThe way to generate code from a POST or an internal processing is to use [StreamBridge](https://github.com/spring-cloud/spring-cloud-stream/blob/master/spring-cloud-stream/src/main/java/org/springframework/cloud/stream/function/StreamBridge.java), which exposes a send function to send the record.\n\n```java\n    @Autowired\n\tprivate StreamBridge streamBridge;\n\n    public Order processANewOrder(Order order) {\n        order.status = OrderStatus.OPEN;\n        order.orderID = UUID.randomUUID().toString();\n        order.creationDate = LocalDate.now();\n        streamBridge.send(BINDING_NAME, order);\n        return order;\n    }\n```\n\nAs a good practice is to send a Kafka Record with a Key, which is specialy needed when sending messages to a multi partition topic: The messages with the same key will always go to the same partition. If the partition key is not present, messages will be partitioned in round-robin fashion. Spring Cloud Stream is little bit confusing as it created two concepts for partitioning: the partitionKey and the message key. The partition key is the way to support the same mechanism as Kafka is doing but for other middleware. So for Kafka we do not need to use partitionKey, but then it is important to use the message key construct. As Kafka is evolving on the partition allocation, it is recommended to do not interfere with Kafka mechanims and use the following approach:\n\n\n* Provide the message key as a SpEL expression property for example in the header: \n\n    ```properties\n    spring.cloud.stream.bindings.<binding-name>.producer.message-key-expression: headers['messageKey']\n    ```\n\n* Then in your application, when publishing the message, add a header called `kafka_messagekey` with the value set from the attribute to use as key. Spring Cloud Stream will use the value for this header to assign it to the Kafka record Key:\n\n    ```java\n     Message<Order> toSend = MessageBuilder.withPayload(order)\n            .setHeader(KafkaHeaders.MESSAGE_KEY, order.customerID.getBytes())\n            .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n        streamBridge.send(BINDING_NAME, toSend);\n    ```\n\nYou can also build composite key with a special java bean class for that and use instance of this class as key.\n\n    ```java\n     CustomerCompanyKey cck = new CustomerCompanyKey(order.customerID,customer.company);\n     Message<Order> toSend = MessageBuilder.withPayload(order)\n            .setHeader(KafkaHeaders.MESSAGE_KEY, cck)\n            .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n        streamBridge.send(BINDING_NAME, toSend);\n    ```\n\nThe following screen shot illustrates that all records with the same \"customerID\" are in the same partition:\n\n![](./images/orders-topic.png)\n\nIf you want to use the partition key as an alternate way to do partition allocation using Spring Cloud Stream strategy then use a partitionKey:\n\n```properties\nspring.cloud.stream.bindings.<binding-name>.producer.partition-key-expression: headers['partitionKey']\n```\n\nand then in the code:\n\n```java\nMessage<Order> toSend = MessageBuilder.withPayload(order)\n            .setHeader(\"partitionKey\", order.customerID.getBytes())\n            .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n        streamBridge.send(BINDING_NAME, toSend);\n```\n\n### Consuming message\n\nWith the last release of Spring Cloud Stream, consumers are single beans of type `Function`, `Consumer` or `Supplier`. Here is an example of consumer only.\n\n```java\n    @Bean\n    public Consumer<Message<Order>> consumeOrderEvent(){\n        return msg -> saveOrder(msg.getPayload());\n    }\n```\n\nFor thew binding configuration the name of the method gives the name of the binding:\n\n```yaml\nspring.cloud.stream:\n  bindings:\n    consumeOrderEvent-in-0:\n      destination: orders\n      contentType: application/json\n      group: orderms-grp\n      useNativeDecoding: true\n  kafka:\n    bindings:\n      consumeOrderEvent-in-0:\n        consumer:\n          ackMode: MANUAL\n          configuration:\n            value.deserializer: ibm.eda.demo.infrastructure.events.OrderDeserializer\n```\n\nThe deserialization is declared in a specific class:\n\n```java\npackage ibm.eda.demo.infrastructure.events;\n\nimport org.springframework.kafka.support.serializer.JsonDeserializer;\n\npublic class OrderDeserializer extends JsonDeserializer<Order> {\n    \n}\n```\n\nIn this example above as the goal is to save to the database, we should not auto commit the offset reading. So the following settings are needed on the consumer side:\n\n```yaml\nspring.cloud.stream.kafka:\n    bindings:\n      consumeOrderEvent-in-0:\n        consumer:\n          autoCommitOffset: false\n          startOffset: latest\n          ackMode: MANUAL\n```\n\nAnd the consumer code is now looking at the acknowledge header property if present or not and perform manual acknowledge once the save operation is successful.\n\n```java\n @Bean\n    public Consumer<Message<Order>> consumeOrderEvent(){\n        return msg -> {\n            Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class);\n            saveOrder(msg.getPayload());\n            if (acknowledgment != null) {\n                acknowledgment.acknowledge();\n            }\n        };\n```\n\n### Kafka spring cloud stream app basic\n\nThe approach to develop such application includes the following steps:\n\n* A spring boot application, with REST spring web starter\n* Define a resource and a controller for the REST API.\n* Define inbound and/or outbound binding to communicate to underlying middleware\n* Add method to process incoming message, taking into account the underlying middleware and serialization. For example with Kafka, most of the consumers may not auto commit the read offset but control the commit by using manual commit. \n* Add logic to produce message using middleware \n\n\nTo add a consumer from a Kafka topic for example, we can add a function that will process the message, and declare it as a Bean. \n\n```java\n @Bean\n    public Consumer<Message<CloudEvent>> consumeCloudEventEvent(){\n        return msg -> {\n            Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class);\n            saveOrder((Order)msg.getPayload().getData());\n            if (acknowledgment != null) {\n                System.out.println(\"Acknowledgment provided\");\n                acknowledgment.acknowledge();\n            }\n        };\n    }\n```\nThis previous code is also illustrating manual offset commit.\n\nThen we add configuration to link to the binders queue or topic:\n\n```yaml\n    consumeOrderEvent-in-0:\n        consumer:\n          autoCommitOffset: false\n          startOffset: latest\n          ackMode: MANUAL\n          configuration:\n            value.deserializer: ibm.eda.demo.infrastructure.events.CloudEventDeserializer\n```\n\n### Avro serialization\n\n\n```yaml\nproducer:\n        useNativeEncoding: true\n```","type":"Mdx","contentDigest":"8b83aeb8b3ba7d6cd609ca73a81ac6dd","owner":"gatsby-plugin-mdx","counter":712},"frontmatter":{"title":"Spring Cloud and Spring Cloud Stream","description":"Spring Cloud and Spring Cloud Stream quick techno overview"},"exports":{},"rawBody":"---\ntitle: Spring Cloud and Spring Cloud Stream\ndescription:  Spring Cloud and Spring Cloud Stream quick techno overview\n---\n\n**Audience**: Developers\n\n## Spring Cloud\n\n[Spring Cloud](https://spring.io/projects/spring-cloud) is based on Spring boot programming model but focusing on cloud native deployment and distributed computing. As other spring boot app it includes jetty or tomcat, health checks, metrics... It supports the following patterns:\n\n* Distributed/versioned [configuration](https://spring.io/projects/spring-cloud-config): externalize config in distributed system with config server.\n* Service registration and discovery: uses Netflix Eureka, Apache Zookeeper or Consul to keep service information. \n* Routing: supports HTTP (Open Feign or  Netflix Ribbon for load balancing) and messaging (RabbitMQ and Kafka)\n* Service-to-service calls: Sptring Cloud Gateway and Netflix Zuul is used\n* Load balancing\n* Circuit Breakers: based on Netflix Hystrix: if the request fails for n time, the circuit open.   \n* Global locks\n* Leadership election and cluster state\n* Distributed messaging\n\nIt also supports pipelines for ci/cd and contract testing for interface validation. \n\n### Getting started\n\nUse [start.spring.io](https://start.spring.io/) to create the application starting code using Kafka, Actuator, Cloud Stream or add the Spring Cloud BOM to your maven `pom.xml` file. See [the Adding Spring Cloud To An Existing Spring Boot Application section.](https://spring.io/projects/spring-cloud)\n\nAs most of the microservices expose REST resource, we may need to add the starter web:\n\n```xml\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n```\n\nWe also need to install the [Spring Cloud CLI](https://cloud.spring.io/spring-cloud-cli/).\n\nThen add the Spring cloud starter as dependency. When using config server, we need to add the config client. \n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-conflig-client</artifactId>\n</dependency>\n```\n\nFor centralized tracing uses, starter-sleuth, and zipkin.\n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-sleuth</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-zipkin</artifactId>\n</dependency>\n```\n\nFor service discovery add netflix-eureka-client.\n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-eureka</artifactId>\n</dependency>\n```\n\n\nUsing the Spring Cloud CLI we can get the service registry, config server, central tracing started in one command:\n\n```shell\nspring cloud eureka configserver zipkin\n```\n\n### Spring Cloud config\n\nUse the concept of Config Server you have a central place to manage external properties for applications across all environments.  As an application moves through the deployment pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run when they migrate. \n\n```java\n  @Value(\"${config.in.topic}\")\n  String topicName = \"orders\";\n```\n\nThe value of the `config.in.topic` comes from local configuration or remote config server. The config server will serve content from a git. See [this sample](https://github.com/spring-cloud-samples/configserver) for such server.\n\n\n\n## Spring Cloud Stream\n\n[Spring Cloud Stream](https://spring.io/projects/spring-cloud-stream) is a framework for building highly scalable event-driven microservices connected with shared messaging systems. It unifies lots of popular messaging platforms behind one easy to use API including RabbitMQ, Apache Kafka, Amazon Kinesis, Google PubSub, Solace PubSub+, Azure Event Hubs, and Apache RocketMQ. \n\nSpring Cloud Stream is an abstraction that uses the following important concepts to supporet middleware encapsulation: **destination binders** (integration with messaging systems like Kafka or RabbitMQ), **destination bindings** (bridge code to external systems) and **message** (canonical data model to communicate between producer and consumer). \n\nAs other Spring boot application, it uses extrernal properties to manage most of the configuration of the binders and binding.\n\n[Spring Cloud Stream Applications](https://spring.io/projects/spring-cloud-stream-applications) are standalone executable applications that communicate over messaging middleware such as Apache Kafka and RabbitMQ. The app is using uber-jars to get the minimal required library and code The following diagram illustrates those concepts for a Spring cloud app:\n\n![](./images/spring-stream-app.png)\n\n**Attention** Spring Cloud Stream is not Kafka Streams or Kafka API, it is similar but it represents another abstraction. From a Kafka developer's point of view, it does not seem relevant, as why not using Kafka API and Kafka Streams API, but this is a way to encapsulate any middleware supporting pub/sub and queueing. It may be more comparable to Microprofile reactive messaging specifications and APIs, but not compatible with it. For example binding can be compared to channel of the microprofile reactive messaging constructs.\n\nSo the development decision will be around middleware abstraction and the way to simplify going from one middleware to another. Now with Kafka, because of its long retention time, it means we can have any type of consumers to read the messages at any time. Those consumers may use Kafka API (Python app or nodejs apps), in this case using the Kafka API within the Spring boot application is a better approach, as the way the abstraction is used may not be fully compatible to any Kafka consumer types.\n\nWith Kafka based application the best practice is also to define the message structure, using Avro or Protbuf, and use schema registry to ensure compatibility management between applications. To support that Spring Cloud Stream support using native (to the middleware) serialization, which in the case of Kafka could be any serdes APIs or avro API. We will cover that [in later section](#avro-serialization).\n\n\n### Example of Kafka binding\n\nThe [order service spring cloud template](https://github.com/ibm-cloud-architecture/eda-quickstarts/tree/main/spring-cloud-stream) is a simple example of order service that exposes CRUD operations on the Order entity via a controller. Instead of writing to a database, this service immediately generates a message to Kafka and then the repository class consumes the message to get the data to write to the database. This is a simple way to implement 'transaction' by using the Append log of Kafka partition as a transaction log.\n\n![](./images/spring-orderms-app.png)\n\nThe way to generate code from a POST or an internal processing is to use [StreamBridge](https://github.com/spring-cloud/spring-cloud-stream/blob/master/spring-cloud-stream/src/main/java/org/springframework/cloud/stream/function/StreamBridge.java), which exposes a send function to send the record.\n\n```java\n    @Autowired\n\tprivate StreamBridge streamBridge;\n\n    public Order processANewOrder(Order order) {\n        order.status = OrderStatus.OPEN;\n        order.orderID = UUID.randomUUID().toString();\n        order.creationDate = LocalDate.now();\n        streamBridge.send(BINDING_NAME, order);\n        return order;\n    }\n```\n\nAs a good practice is to send a Kafka Record with a Key, which is specialy needed when sending messages to a multi partition topic: The messages with the same key will always go to the same partition. If the partition key is not present, messages will be partitioned in round-robin fashion. Spring Cloud Stream is little bit confusing as it created two concepts for partitioning: the partitionKey and the message key. The partition key is the way to support the same mechanism as Kafka is doing but for other middleware. So for Kafka we do not need to use partitionKey, but then it is important to use the message key construct. As Kafka is evolving on the partition allocation, it is recommended to do not interfere with Kafka mechanims and use the following approach:\n\n\n* Provide the message key as a SpEL expression property for example in the header: \n\n    ```properties\n    spring.cloud.stream.bindings.<binding-name>.producer.message-key-expression: headers['messageKey']\n    ```\n\n* Then in your application, when publishing the message, add a header called `kafka_messagekey` with the value set from the attribute to use as key. Spring Cloud Stream will use the value for this header to assign it to the Kafka record Key:\n\n    ```java\n     Message<Order> toSend = MessageBuilder.withPayload(order)\n            .setHeader(KafkaHeaders.MESSAGE_KEY, order.customerID.getBytes())\n            .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n        streamBridge.send(BINDING_NAME, toSend);\n    ```\n\nYou can also build composite key with a special java bean class for that and use instance of this class as key.\n\n    ```java\n     CustomerCompanyKey cck = new CustomerCompanyKey(order.customerID,customer.company);\n     Message<Order> toSend = MessageBuilder.withPayload(order)\n            .setHeader(KafkaHeaders.MESSAGE_KEY, cck)\n            .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n        streamBridge.send(BINDING_NAME, toSend);\n    ```\n\nThe following screen shot illustrates that all records with the same \"customerID\" are in the same partition:\n\n![](./images/orders-topic.png)\n\nIf you want to use the partition key as an alternate way to do partition allocation using Spring Cloud Stream strategy then use a partitionKey:\n\n```properties\nspring.cloud.stream.bindings.<binding-name>.producer.partition-key-expression: headers['partitionKey']\n```\n\nand then in the code:\n\n```java\nMessage<Order> toSend = MessageBuilder.withPayload(order)\n            .setHeader(\"partitionKey\", order.customerID.getBytes())\n            .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n        streamBridge.send(BINDING_NAME, toSend);\n```\n\n### Consuming message\n\nWith the last release of Spring Cloud Stream, consumers are single beans of type `Function`, `Consumer` or `Supplier`. Here is an example of consumer only.\n\n```java\n    @Bean\n    public Consumer<Message<Order>> consumeOrderEvent(){\n        return msg -> saveOrder(msg.getPayload());\n    }\n```\n\nFor thew binding configuration the name of the method gives the name of the binding:\n\n```yaml\nspring.cloud.stream:\n  bindings:\n    consumeOrderEvent-in-0:\n      destination: orders\n      contentType: application/json\n      group: orderms-grp\n      useNativeDecoding: true\n  kafka:\n    bindings:\n      consumeOrderEvent-in-0:\n        consumer:\n          ackMode: MANUAL\n          configuration:\n            value.deserializer: ibm.eda.demo.infrastructure.events.OrderDeserializer\n```\n\nThe deserialization is declared in a specific class:\n\n```java\npackage ibm.eda.demo.infrastructure.events;\n\nimport org.springframework.kafka.support.serializer.JsonDeserializer;\n\npublic class OrderDeserializer extends JsonDeserializer<Order> {\n    \n}\n```\n\nIn this example above as the goal is to save to the database, we should not auto commit the offset reading. So the following settings are needed on the consumer side:\n\n```yaml\nspring.cloud.stream.kafka:\n    bindings:\n      consumeOrderEvent-in-0:\n        consumer:\n          autoCommitOffset: false\n          startOffset: latest\n          ackMode: MANUAL\n```\n\nAnd the consumer code is now looking at the acknowledge header property if present or not and perform manual acknowledge once the save operation is successful.\n\n```java\n @Bean\n    public Consumer<Message<Order>> consumeOrderEvent(){\n        return msg -> {\n            Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class);\n            saveOrder(msg.getPayload());\n            if (acknowledgment != null) {\n                acknowledgment.acknowledge();\n            }\n        };\n```\n\n### Kafka spring cloud stream app basic\n\nThe approach to develop such application includes the following steps:\n\n* A spring boot application, with REST spring web starter\n* Define a resource and a controller for the REST API.\n* Define inbound and/or outbound binding to communicate to underlying middleware\n* Add method to process incoming message, taking into account the underlying middleware and serialization. For example with Kafka, most of the consumers may not auto commit the read offset but control the commit by using manual commit. \n* Add logic to produce message using middleware \n\n\nTo add a consumer from a Kafka topic for example, we can add a function that will process the message, and declare it as a Bean. \n\n```java\n @Bean\n    public Consumer<Message<CloudEvent>> consumeCloudEventEvent(){\n        return msg -> {\n            Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class);\n            saveOrder((Order)msg.getPayload().getData());\n            if (acknowledgment != null) {\n                System.out.println(\"Acknowledgment provided\");\n                acknowledgment.acknowledge();\n            }\n        };\n    }\n```\nThis previous code is also illustrating manual offset commit.\n\nThen we add configuration to link to the binders queue or topic:\n\n```yaml\n    consumeOrderEvent-in-0:\n        consumer:\n          autoCommitOffset: false\n          startOffset: latest\n          ackMode: MANUAL\n          configuration:\n            value.deserializer: ibm.eda.demo.infrastructure.events.CloudEventDeserializer\n```\n\n### Avro serialization\n\n\n```yaml\nproducer:\n        useNativeEncoding: true\n```","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/spring/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}