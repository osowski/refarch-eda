{"componentChunkName":"component---src-pages-technology-index-mdx","path":"/technology/","result":{"pageContext":{"frontmatter":{"title":"Getting a starting environment to develop EDA solution","description":"Getting a starting environment to develop EDA solution"},"relativePagePath":"/technology/index.mdx","titleType":"append","MdxNode":{"id":"b5106d05-d9cb-5245-95b0-545e5ca39a54","children":[],"parent":"c0b225a2-6711-5588-ba92-ad0e408b9831","internal":{"content":"---\ntitle: Getting a starting environment to develop EDA solution\ndescription:  Getting a starting environment to develop EDA solution\n---\n\nWe are presenting in this note how to get development and deployment environments to start developing Event Driven microservice solution. We assume OpensShift deployment and at first Java as the main programming language.\n\n## Infrastructure for dev Integration test\n\n* OpenShift\n   * [OpenShift Container Platform Installation Documentation](https://docs.openshift.com/container-platform/4.5/welcome/index.html) \n   * OpenShift Container Platform is flexible and can be installed in a number of different environments - onprem, cloud, and hybrid environments.\n   * OpenShift v4.4.x and newer is required for CP4I2020.2.1 (and subsequently Event Streams v10.0) and CP4Apps v4.2.x\n* Cloud Pak Operators\n    * Add [IBM Common Services operators](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/install_online_catalog_sources.html) \n    * Add the IBM operators to the list of installable operators. Same product note as above.\n    * Get [entitlement key](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/entitlement_key.html)\n* Cloud Pak for Integration\n    * Install Cloud Pak for Integration operator using the Operator hub catalog\n    * Install Cloud Pak for Integration platform navigator from the operator hub catalog\n    * The previous steps should have installed the common services if they were not installed before. So get the admin password via the `platform-auth-idp-credentials` secret in the `ibm-common-services` project. ` oc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 --decode`\n    * More information on [CP4I Installation](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/install.html)\n* Cloud Pak for Application\n  * You can either install Cloud Pak for Applications via the [CLI](https://www.ibm.com/support/knowledgecenter/SSCSJL_4.1.x/install-icpa-cli.html) or the [cloudctl CASE launcher](https://www.ibm.com/support/knowledgecenter/en/SSCSJL_4.2.x/install-icpa-case.html)\n  * Like the Cloud Pak for Integration you will need a key for the entitled registry.\n  \n\n### Deploying Event Streams\n\n- The instructions are in the [product documentation](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/install_event_streams.html), and are very simple using the IBM Event Streams operator. Select minimal configuration with persistence. \n\n- Here is an example of the yaml. Note that there are a few sample yamls that come after you install the Event Streams Operator. This yaml is for Event Streams v10.0 -\n\n```yaml\napiVersion: eventstreams.ibm.com/v1beta1\nkind: EventStreams\nmetadata:\n  name: minimal-prod\n  namespace: cp4i\nspec:\n  version: 10.0.0\n  license:\n    accept: false\n    use: CloudPakForIntegrationProduction\n  adminApi: {}\n  adminUI: {}\n  collector: {}\n  restProducer: {}\n  schemaRegistry:\n    storage:\n      type: persistent-claim\n      size: 100Mi\n      class: enter-storage-class-name-here\n  strimziOverrides:\n    kafka:\n      replicas: 3\n      authorization:\n        type: runas\n      config:\n        inter.broker.protocol.version: '2.5'\n        interceptor.class.names: com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor\n        log.cleaner.threads: 6\n        log.message.format.version: '2.5'\n        num.io.threads: 24\n        num.network.threads: 9\n        num.replica.fetchers: 3\n        offsets.topic.replication.factor: 3\n      listeners:\n        external:\n          type: route\n          authentication:\n            type: scram-sha-512\n        tls:\n          authentication:\n            type: tls\n      metrics: {}\n      storage:\n        type: persistent-claim\n        size: 4Gi\n        class: enter-storage-class-name-here\n    zookeeper:\n      replicas: 3\n      metrics: {}\n      storage:\n        type: persistent-claim\n        size: 2Gi\n        class: enter-storage-class-name-here\n```\n\n### Deploy Strimzi\n\n* [Strimzi](https://strimzi.io/) is a very powerful and useful distributed Kafka deployment built for use with Kubernetes and OpenShift.\n* On OpenShift Container Platform v4.0.x to 4.3.x as well as pre-Event Streams v10 the Strimzi Operator will serve most of our needs. It can serve as the base so that we can utilize technologies like KafkaConnect, KafkaConnectS2i, KafkaConnector, Mirror Maker2 and other Custom Resources.\n* You can install it via [Operator Hub](https://strimzi.io/blog/2019/03/06/strimzi-and-operator-lifecycle-manager/)\n* You can also install the Strimzi and it's [Cluster Operator](https://strimzi.io/docs/operators/master/deploying.html#cluster-operator-str) by applying a yaml file through CLI.\n\n## Java Developer Environment\n\n* Go with [Quarkus](http://quarkus.io) so all being set with maven plugin.\n* You can scaffold your application through the [Quarkus Web UI](https://code.quarkus.io/) which will allow you to pick and choose your project dependencies. You may also do it through the CLI like so: \n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.8.1.Final:create \\\n-DprojectGroupId=ibm.garage \\\n-DprojectArtifactId=your-application \\\n-Dextensions=\"kafka,kafka-streams,quarkus-kafka-streams\"\n  ```\n* If you already have your project created and you know the name of an extension you want ot add, you can do it through the CLI like the following - \n```shell\n./mvnw quarkus:add-extension -Dextensions=\"kafka\"\n```\n  \n* Here's a very simple Quarkus Producer application utilizing MicroProfile Reactive Messaging that sends messages to Event Stream v10 and newer: \n\n**Producer.java**\n\n```java\npackage com.ibm.garage.infrastructure;\n\nimport io.reactivex.Flowable;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\nimport javax.enterprise.context.ApplicationScoped;\nimport java.util.Random;\nimport java.util.concurrent.TimeUnit;\n\n\n/**\n * This class produces a message every 5 seconds.\n * The Kafka configuration is specified in the application.properties file.\n*/\n@ApplicationScoped\npublic class Producer {\n    private Random random = new Random();\n\n    @Outgoing(\"mock-producer\")      \n    public Flowable<KafkaRecord<Integer, String>> generate() {\n        return Flowable.interval(5, TimeUnit.SECONDS)    \n                .onBackpressureDrop()\n                .map(tick -> {      \n                    return KafkaRecord.of(random.nextInt(100), String.valueOf(random.nextInt(100)));\n                });\n    }                  \n}\n\n```\n\n\n* Now we have a simple Quarkus consumer, also using MicroProfile Reactive Messaging and printing the message.\n\n**Consumer.java**\n\n\n```java\npackage com.ibm.garage.infrastructure;\n\nimport io.quarkus.runtime.ShutdownEvent;\nimport io.quarkus.runtime.StartupEvent;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.event.Observes;\n\n@ApplicationScoped\npublic class Consumer {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(Consumer.class);\n\n    @Incoming(\"mock-consumer\")\n    public void consumingMessage(String incomingMessage) {\n        LOGGER.info(\"Message received from topic = {}\", incomingMessage);\n        //System.out.println(incomingMessage);\n    }\n    \n```\n\n* Quarkus does it's configuration via an `application.properties` file within the `src/main/resources/` path. A sample properties file.\n\n```properties\nquarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=${CERT_PASSWORD}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.type=PKCS12\n\n\n# mock message producer configuration\nmp.messaging.outgoing.mock-producer.connector=smallrye-kafka\nmp.messaging.outgoing.mock-producer.topic=${TOPIC_NAME}\nmp.messaging.outgoing.mock-producer.value.serializer=org.apache.kafka.common.serialization.StringSerializer\nmp.messaging.outgoing.mock-producer.key.serializer=org.apache.kafka.common.serialization.IntegerSerializer\n\n# Kafka Streams consumer configuration\nmp.messaging.incoming.mock-consumer.connector=smallrye-kafka\nmp.messaging.incoming.mock-consumer.topic=${TOPIC_NAME}\nmp.messaging.incoming.mock-consumer.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nmp.messaging.incoming.mock-consumer.key.deerializer=org.apache.kafka.common.serialization.IntegerDeserializer\n```\n* There are a few environment variables we need to pass to our properties before we can run it. Replace the values which can be retrieved from the Event Streams on CP4I UI.\n\n```shell\nexport BOOTSTRAP_SERVERS=your-external-bootstrap-address:xxxx \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-password \\\nexport TOPIC_NAME=your-topic \\\nexport CERT_LOCATION=/your-path-to-cert/es-cert.p12 \\\nexport CERT_PASSWORD=your-cert-password\n```\n\n* To run your applications run the following which will allow hot-reloading (if that's a functionality you need)\n```shell\n./mvnw quarkus:dev\n```\n\n\n* Kafka Strimzi image for docker and docker-compose to get up and running quickly. We have different docker composes files for you to start with:\n    * One Broker, one Zookeeper, kafka 2.5\n    * TODO\n\n## Python Developer Environment\n\n* There are a few Python packages but the [Confluent Kafka Python](https://github.com/confluentinc/confluent-kafka-python) package can serve our needs.\n* For Python environments you can use the Confluent Python package and install the dependency with pip.\n\n`pip install confluent-kafka`\n\n* The following is a very simple Python Producer that will serve as the scaffold:\n\n**KafkaProducer.py**\n\n```python\nimport json, os\nfrom confluent_kafka import KafkaError, Producer\n\nclass KafkaProducer:\n\n    def __init__(self, groupID = \"KafkaProducer\"):\n        # Get the producer configuration\n        self.producer_conf = self.getProducerConfiguration(groupID)\n        # Create the producer\n        self.producer = Producer(self.producer_conf)\n\n    def getProducerConfiguration(self,groupID):\n        try:\n            options ={\n                    'bootstrap.servers': os.environ['KAFKA_BROKERS'],\n                    'group.id': groupID\n            }\n            if (os.getenv('KAFKA_PASSWORD','') != ''):\n                # Set security protocol common to ES on prem and on IBM Cloud\n                options['security.protocol'] = 'SASL_SSL'\n                # Depending on the Kafka User, we will know whether we are talking to ES on prem or on IBM Cloud\n                # If we are connecting to ES on IBM Cloud, the SASL mechanism is plain\n                if (os.getenv('KAFKA_USER','') == 'token'):\n                    options['sasl.mechanisms'] = 'PLAIN'\n                # If we are connecting to ES on OCP, the SASL mechanism is scram-sha-512\n                else:\n                    options['sasl.mechanisms'] = 'SCRAM-SHA-512'\n                # Set the SASL username and password\n                options['sasl.username'] = os.getenv('KAFKA_USER','')\n                options['sasl.password'] = os.getenv('KAFKA_PASSWORD','')\n            # If we are talking to ES on prem, it uses an SSL self-signed certificate.\n            # Therefore, we need the CA public certificate for the SSL connection to happen.\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))):\n                options['ssl.ca.location'] = os.getenv('KAFKA_CERT','/certs/es-cert.pem')\n            \n            # Print out the producer configuration\n            self.printProducerConfiguration(options)\n\n            return options\n\n        except KeyError as error:\n            print('[KafkaProducer] - [ERROR] - A required environment variable does not exist: ' + error)\n            exit(1)\n\n    def printProducerConfiguration(self,options):\n        # Printing out producer config for debugging purposes        \n        print(\"[KafkaProducer] - This is the configuration for the producer:\")\n        print(\"[KafkaProducer] - -------------------------------------------\")\n        print('[KafkaProducer] - Bootstrap Server:      {}'.format(options['bootstrap.servers']))\n        if (os.getenv('KAFKA_PASSWORD','') != ''):\n            # Obfuscate password\n            if (len(options['sasl.password']) > 3):\n                obfuscated_password = options['sasl.password'][0] + \"*****\" + options['sasl.password'][len(options['sasl.password'])-1]\n            else:\n                obfuscated_password = \"*******\"\n            print('[KafkaProducer] - Security Protocol:     {}'.format(options['security.protocol']))\n            print('[KafkaProducer] - SASL Mechanism:        {}'.format(options['sasl.mechanisms']))\n            print('[KafkaProducer] - SASL Username:         {}'.format(options['sasl.username']))\n            print('[KafkaProducer] - SASL Password:         {}'.format(obfuscated_password))\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))): \n                print('[KafkaProducer] - SSL CA Location:       {}'.format(options['ssl.ca.location']))\n        print(\"[KafkaProducer] - -------------------------------------------\")\n\n    def delivery_report(self,err, msg):\n        \"\"\" Called once for each message produced to indicate delivery result. Triggered by poll() or flush(). \"\"\"\n        if err is not None:\n            print('[KafkaProducer] - [ERROR] - Message delivery failed: {}'.format(err))\n        else:\n            print('[KafkaProducer] - Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n\n    def publishEvent(self, topicName, eventToSend, keyName):\n        # Print the event to send\n        dataStr = json.dumps(eventToSend)\n        # Produce the message\n        self.producer.produce(topicName,key=eventToSend[keyName],value=dataStr.encode('utf-8'), callback=self.delivery_report)\n        # Flush\n        self.producer.flush()\n```\n* Now that we have the Producer we need to actually send Events.\n\n**ProducePlainMessage.py**\n\n```python\nimport argparse\nfrom KafkaProducer import KafkaProducer\n\nif __name__ == '__main__':\n\n    # Parse arguments\n    parser = argparse.ArgumentParser(description=\"Message Producer Example\")\n    parser.add_argument('-t', dest=\"topic\", required=True, help=\"Topic name\")\n    args = parser.parse_args()\n    \n    # Create the event to be sent\n    event = {\"eventKey\" : \"1\", \"message\" : \"This is a test message\"}\n    \n    # Print it out\n    print(\"--- Event to be published: ---\")\n    print(event)\n    print(\"----------------------------------------\")\n    \n    # Create the Kafka Producer\n    kafka_producer = KafkaProducer()\n    # Publish the event\n    kafka_producer.publishEvent(args.topic,event,\"eventKey\")\n```\n\n* The following is a simple Python consumer\n\n**KafkaConsumer.py**\n\n```python\nimport json,os\nfrom confluent_kafka import Consumer, KafkaError\n\n\nclass KafkaConsumer:\n\n    def __init__(self, topic_name = \"kafka-producer\", groupID = 'KafkaConsumer', autocommit = True):\n        # Get the consumer configuration\n        self.consumer_conf = self.getConsumerConfiguration(groupID, autocommit)\n        # Create the Avro consumer\n        self.consumer = Consumer(self.consumer_conf)\n        # Subscribe to the topic\n        self.consumer.subscribe([topic_name])\n\n    def getConsumerConfiguration(self, groupID, autocommit):\n        try:\n            options ={\n                    'bootstrap.servers': os.environ['KAFKA_BROKERS'],\n                    'group.id': groupID,\n                    'auto.offset.reset': \"earliest\",\n                    'enable.auto.commit': autocommit,\n            }\n            if (os.getenv('KAFKA_PASSWORD','') != ''):\n                # Set security protocol common to ES on prem and on IBM Cloud\n                options['security.protocol'] = 'SASL_SSL'\n                # Depending on the Kafka User, we will know whether we are talking to ES on prem or on IBM Cloud\n                # If we are connecting to ES on IBM Cloud, the SASL mechanism is plain\n                if (os.getenv('KAFKA_USER','') == 'token'):\n                    options['sasl.mechanisms'] = 'PLAIN'\n                # If we are connecting to ES on OCP, the SASL mechanism is scram-sha-512\n                else:\n                    options['sasl.mechanisms'] = 'SCRAM-SHA-512'\n                # Set the SASL username and password\n                options['sasl.username'] = os.getenv('KAFKA_USER','')\n                options['sasl.password'] = os.getenv('KAFKA_PASSWORD','')\n            # If we are talking to ES on prem, it uses an SSL self-signed certificate.\n            # Therefore, we need the CA public certificate for the SSL connection to happen.\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))):\n                options['ssl.ca.location'] = os.getenv('KAFKA_CERT','/certs/es-cert.pem')\n\n            # Print out the producer configuration\n            self.printConsumerConfiguration(options)\n\n            return options\n\n        except KeyError as error:\n            print('[KafkaConsumer] - [ERROR] - A required environment variable does not exist: ' + error)\n            exit(1)\n    \n    def printConsumerConfiguration(self,options):\n        # Printing out consumer config for debugging purposes        \n        print(\"[KafkaConsumer] - This is the configuration for the consumer:\")\n        print(\"[KafkaConsumer] - -------------------------------------------\")\n        print('[KafkaConsumer] - Bootstrap Server:      {}'.format(options['bootstrap.servers']))\n        if (os.getenv('KAFKA_PASSWORD','') != ''):\n            # Obfuscate password\n            if (len(options['sasl.password']) > 3):\n                obfuscated_password = options['sasl.password'][0] + \"*****\" + options['sasl.password'][len(options['sasl.password'])-1]\n            else:\n                obfuscated_password = \"*******\"\n            print('[KafkaConsumer] - Security Protocol:     {}'.format(options['security.protocol']))\n            print('[KafkaConsumer] - SASL Mechanism:        {}'.format(options['sasl.mechanisms']))\n            print('[KafkaConsumer] - SASL Username:         {}'.format(options['sasl.username']))\n            print('[KafkaConsumer] - SASL Password:         {}'.format(obfuscated_password))\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))): \n                print('[KafkaConsumer] - SSL CA Location:       {}'.format(options['ssl.ca.location']))\n        print('[KafkaConsumer] - Offset Reset:          {}'.format(options['auto.offset.reset']))\n        print('[KafkaConsumer] - Autocommit:            {}'.format(options['enable.auto.commit']))\n        print(\"[KafkaConsumer] - -------------------------------------------\")\n    \n    # Prints out and returns the decoded events received by the consumer\n    def traceResponse(self, msg):\n        print('[KafkaConsumer] - Next Message consumed from {} partition: [{}] at offset: {}\\n\\tkey: {}\\n\\tvalue: {}'\n                    .format(msg.topic(), msg.partition(), msg.offset(), msg.key().decode('utf-8'), msg.value().decode('utf-8')))\n\n    # Polls for next event\n    def pollNextEvent(self):\n        # Poll for messages\n        msg = self.consumer.poll(timeout=10.0)\n        # Validate the returned message\n        if msg is None:\n            print(\"[KafkaConsumer] - [INFO] - No new messages on the topic\")\n        elif msg.error():\n            if (\"PARTITION_EOF\" in msg.error()):\n                print(\"[KafkaConsumer] - [INFO] - End of partition\")\n            else:\n                print(\"[KafkaConsumer] - [ERROR] - Consumer error: {}\".format(msg.error()))\n        else:\n            # Print the message\n            self.traceResponse(msg)\n    \n    def close(self):\n        self.consumer.close()\n```\n\n* Now for the plain Kafka Python consumer\n\n**ConsumePlainMessage.py**\n\n```python\nimport argparse\nfrom KafkaConsumer import KafkaConsumer\n\n####################### MAIN #######################\nif __name__ == '__main__':\n    \n    # Parse arguments\n    parser = argparse.ArgumentParser(description=\"Message Consumer Example\")\n    parser.add_argument('-t', dest=\"topic\", required=True, help=\"Topic name\")\n    args = parser.parse_args()\n\n    # Create a Kafka Consumer\n    kafka_consumer = KafkaConsumer(args.topic)\n    # Poll for next message\n    kafka_consumer.pollNextEvent()\n    # Close the consumer\n    kafka_consumer.close()\n```\n\n\n* In `KafkaProducer.py` as well as `KafkaConsumer.py` we will need to provide environment variables so that our producer can parse it and actually connect to an Event Streams intance.\n\n```shell\nexport KAFKA_BROKERS=your-brokers \\\nexport KAFKA_USER=your-scram-username \\\nexport KAFKA_PASSWORD=your-scram-password \\\nexport KAFKA_CERT=your-cert-path\n\n```\n\n* To run your Producer and send a simple message - \n```shell\npython ProducePlainMessage.py -t <your-topic-name>\n```\n\n* To consume\n```shell\npython ConsumePlainMessage.py -t <your-topic-name>\n```\n\n\n## Node.js Developer Environment\n\n* The following code is based off this application [here](https://chrisphillips-cminion.github.io/eventstreams/2019/08/22/NodeJS-IBMES-101.html)\n\n* We will make use of the [node-rdkafka](https://github.com/Blizzard/node-rdkafka) library for our purposes. Therefore we will need to install the dependency for the project first:\n\n```shell\nnpm install node-rdkafka --save\n```\n\n* Here is a simple Node.js producer application that will produce 20 messages to the Kafka topic.\n\n**producer.js**\n\n```javascript\nvar Kafka = require('node-rdkafka');\n\nvar kafka_options = {\n    //'debug': 'all',\n    'metadata.broker.list': process.env.KAFKA_BROKERS,\n    'security.protocol': 'sasl_ssl',\n    'sasl.mechanisms': 'SCRAM-SHA-512',\n    'sasl.username': process.env.SCRAM_USERNAME,\n    'sasl.password': process.env.SCRAM_PASSWORD,\n    'ssl.ca.location': process.env.PEM_PATH,\n    'log.connection.close' : false,\n    'client.id': 'ES-NodeJS-101'\n};\n\nvar topicName = process.env.TOPIC_NAME\n\nproducer = new Kafka.Producer(kafka_options);\nproducer.setPollInterval(100);\n// Register listener for debug information; only invoked if debug option set in kafka_options\nproducer.on('event.log', function(log) {\n    console.log(log);\n});\n// Register error listener\nproducer.on('event.error', function(err) {\n    console.error('Error from producer:' + JSON.stringify(err));\n});\n\n// Register delivery report listener\nproducer.on('delivery-report', function(err, dr) {\n    if (err) {\n        console.error('Delivery report: Failed sending message ' + dr.value);\n        console.error(err);\n        // We could retry sending the message\n    } else {\n        console.log('Message produced, partition: ' + dr.partition + ' offset: ' + dr.offset);\n    }\n});\n\nvar sendMessageToTopic = function() {\n    console.log('The producer has connected.');\n    const genMessage = i => new Buffer.from(`Kafka example, message number ${i}`);\n    console.log('Producer is now sending 20 messages to the Kafka Topic');\n    const maxMessages = 20;\n    for (var i = 0; i < maxMessages; i++) {\n        producer.produce(topicName, -1, genMessage(i), i);\n    }\n}\n\n// Register callback invoked when producer has connected\nproducer.on('ready', function() {\n\n    sendMessageToTopic();\n\n    // request metadata for all topics\n    producer.getMetadata({\n        timeout: 10000\n    },\n    function(err, metadata) {\n        if (err) {\n            console.error('Error getting metadata: ' + JSON.stringify(err));\n            shutdown(-1);\n        } else {\n            console.log('Producer obtained metadata: ' + JSON.stringify(metadata));\n            var topicsByName = metadata.topics.filter(function(t) {\n                return t.name === topicName;\n            });\n            if (topicsByName.length === 0) {\n                console.error('ERROR - Topic ' + topicName + ' does not exist. Exiting');\n                shutdown(-1);\n            }\n        }\n    });\n    var counter = 0;\n});\n\nproducer.connect();\n\n```\n\n* Below is a simple Node.js consumer:\n\n**consumer.js**\n\n```javascript\nvar Kafka = require('node-rdkafka');\n\nvar kafka_options = {\n    //'debug': 'all',\n    'metadata.broker.list': process.env.KAFKA_BROKERS,\n    'security.protocol': 'sasl_ssl',\n    'sasl.mechanisms': 'SCRAM-SHA-512',\n    'sasl.username': process.env.SCRAM_USERNAME,\n    'sasl.password': process.env.SCRAM_PASSWORD,\n    'ssl.ca.location': process.env.PEM_PATH,\n    'log.connection.close' : false,\n    'group.id': 'ES-NodeJS-101-consumer'\n};\n\n\n\nvar topicName = process.env.TOPIC_NAME\n\nconst consumer = new Kafka.KafkaConsumer(kafka_options, {\n    \"auto.offset.reset\": \"beginning\"\n  });\n\n\nconsumer.on('event.log', function(log) {\n    console.log(log);\n});\n\n// Register error listener\nconsumer.on('event.error', function(err) {\n    console.error('Error from consumer:' + JSON.stringify(err));\n});\n\nvar consumedMessages = []\n// Register callback to be invoked when consumer has connected\nconsumer.on('ready', function() {\n    console.log('The consumer has connected.');\n\n    // request metadata for one topic\n    consumer.getMetadata({\n        topic: topicName,\n        timeout: 10000\n    },\n    function(err, metadata) {\n        if (err) {\n            console.error('Error getting metadata: ' + JSON.stringify(err));\n            shutdown(-1);\n        } else {\n            console.log('Consumer obtained metadata: ' + JSON.stringify(metadata));\n\n        }\n    });\n\n    consumer.subscribe([topicName]);\n\n    consumerLoop = setInterval(function () {\n        if (consumer.isConnected()) {\n            // The consume(num, cb) method can take a callback to process messages.\n            // In this sample code we use the \".on('data')\" event listener instead,\n            // for illustrative purposes.\n            consumer.consume(10);\n        }    \n\n        if (consumedMessages.length === 0) {\n            console.log('No messages consumed');\n        } else {\n            for (var i = 0; i < consumedMessages.length; i++) {\n                var m = consumedMessages[i];\n                console.log('Message consumed: topic=' + m.topic + ', partition=' + m.partition + ', offset=' + m.offset + ', key=' + m.key + ', value=' + m.value.toString());\n            }\n            consumedMessages = [];\n        }\n    }, 2000);\n});\n\n// Register a listener to process received messages\nconsumer.on('data', function(m) {\n    consumedMessages.push(m);\n});\n\nconsumer.connect()\n```\n* Like the Python environments, we will need a SCRAM Username, password, and the PEM Certificate. We will need a few environment variables to provide to our application so that it can connect to Event Streams.\n\n```shell\nexport KAFKA_BROKERS=your-external-bootstrap-server-address \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-password \\\nexport TOPIC_NAME=your-topic-name \\\nexport PEM_PATH=/path-to-your-pem-certificate/es-cert.pem\n```\n\n* To run these applications after exporting the necessary environment variables:\n```shell\nnode producer.js\nnode consumer.js\n```\n\n\n# Golang Developer Environment\n\n* Similar to the Python Developer environment, we can leverage the [Confluent-kafka-go](https://github.com/confluentinc/confluent-kafka-go) library.\n* If using Go/Golang v 1.13 and newer you can get it using Go Modules by importing via github.\n  - `import \"github.com/confluentinc/confluent-kafka-go/kafka\"`\n  - `go build ./...`\n* Otherwise if you cannot use Go modules you can manually install it\n  - `go get -u gopkg.in/confluentinc/confluent-kafka-go.v1/kafka`\n  - `import \"gopkg.in/confluentinc/confluent-kafka-go.v1/kafka\"`\n  - Note that our sample producer and consumer uses this option.\n  \n\n* Below is a simple Go Kafka producer that sends 7 Test messages.\n\n**producer.go**\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"gopkg.in/confluentinc/confluent-kafka-go.v1/kafka\"\n)\n\nfunc main() {\n\n\tkafkaBrokers := os.Getenv(\"KAFKA_BROKERS\")\n\tscramUsername := os.Getenv(\"SCRAM_USERNAME\")\n\tscramPassword := os.Getenv(\"SCRAM_PASSWORD\")\n\tpemPath := os.Getenv(\"PEM_PATH\")\n\ttopic := os.Getenv(\"TOPIC_NAME\")\n\n\n\tp, err := kafka.NewProducer(&kafka.ConfigMap {\n\t\t\"bootstrap.servers\": kafkaBrokers,\n\t\t\"security.protocol\": \"SASL_SSL\",\n\t\t\"sasl.mechanism\": \"SCRAM-SHA-512\",\n\t\t\"group.id\": \"golang-kafka-producer\",\n\t\t\"sasl.username\": scramUsername,\n\t\t\"sasl.password\": scramPassword,\n\t\t\"ssl.ca.location\": pemPath})\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tdefer p.Close()\n\n\t// Delivery report handler for produced messages\n\tgo func() {\n\t\tfor e := range p.Events() {\n\t\t\tswitch ev := e.(type) {\n\t\t\tcase *kafka.Message:\n\t\t\t\tif ev.TopicPartition.Error != nil {\n\t\t\t\t\tfmt.Printf(\"Delivery failed: %v\\n\", ev.TopicPartition)\n\t\t\t\t} else {\n\t\t\t\t\tfmt.Printf(\"Delivered message to %v\\n\", ev.TopicPartition)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Produce messages to topic (asynchronously)\n\tfor _, word := range []string{\"TEST\", \"TEST1\", \"TEST2\", \"TEST3\", \"TEST4\", \"TEST5\", \"TEST6\"} {\n\t\tp.Produce(&kafka.Message{\n\t\t\tTopicPartition: kafka.TopicPartition{Topic: &topic, Partition: kafka.PartitionAny},\n\t\t\tValue: []byte(word),\n\t\t}, nil)\n\t}\n\n\t// Wait for message deliveries before shutting down\n\tp.Flush(15 * 1000)\n}\n```\n\n* Also we have a simple consumer application:\n\n**consumer.go**\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"gopkg.in/confluentinc/confluent-kafka-go.v1/kafka\"\n)\n\nfunc main() {\n\n\tkafkaBrokers := os.Getenv(\"KAFKA_BROKERS\")\n\tscramUsername := os.Getenv(\"SCRAM_USERNAME\")\n\tscramPassword := os.Getenv(\"SCRAM_PASSWORD\")\n\tpemPath := os.Getenv(\"PEM_PATH\")\n\ttopic := os.Getenv(\"TOPIC_NAME\")\n\n\tc, err := kafka.NewConsumer(&kafka.ConfigMap{\n\t\t\"bootstrap.servers\": kafkaBrokers,\n\t\t\"security.protocol\": \"SASL_SSL\",\n\t\t\"sasl.mechanism\": \"SCRAM-SHA-512\",\n\t\t\"sasl.username\": scramUsername,\n\t\t\"sasl.password\": scramPassword,\n\t\t\"ssl.ca.location\": pemPath,\n\t\t\"group.id\": \"golang-kafka-consumer\",\n\t\t\"auto.offset.reset\": \"earliest\"})\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tc.SubscribeTopics([]string{topic, \"^aRegex.*[Tt]opic\"}, nil)\n\n\tfor {\n\t\tmsg, err := c.ReadMessage(-1)\n\t\tif err == nil {\n\t\t\tfmt.Printf(\"Message on %s: %s\\n\", msg.TopicPartition, string(msg.Value))\n\t\t} else {\n\t\t\t// The client will automatically try to recover from all errors.\n\t\t\tfmt.Printf(\"Consumer error: %v (%v)\\n\", err, msg)\n\t\t}\n\t}\n\n\tc.Close()\n}\n```\n\n* Like all the previous developer environments (Java, Python and Node.js) we will need to set environment variables for our application. Replace the values below with your own.\n\n```shell\nexport KAFKA_BROKERS=your-external-bootstrap-address:443 \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-pasword \\\nexport PEM_PATH=/path-to-pem-cert/es-cert.pem \\\nexport TOPIC_NAME=your-topic\n```\n\n* Now you can test the applications.\n\n```shell\ngo run consumer.go\ngo run producer.go\n```\n","type":"Mdx","contentDigest":"92083830c1100422335923034a5b7428","owner":"gatsby-plugin-mdx","counter":662},"frontmatter":{"title":"Getting a starting environment to develop EDA solution","description":"Getting a starting environment to develop EDA solution"},"exports":{},"rawBody":"---\ntitle: Getting a starting environment to develop EDA solution\ndescription:  Getting a starting environment to develop EDA solution\n---\n\nWe are presenting in this note how to get development and deployment environments to start developing Event Driven microservice solution. We assume OpensShift deployment and at first Java as the main programming language.\n\n## Infrastructure for dev Integration test\n\n* OpenShift\n   * [OpenShift Container Platform Installation Documentation](https://docs.openshift.com/container-platform/4.5/welcome/index.html) \n   * OpenShift Container Platform is flexible and can be installed in a number of different environments - onprem, cloud, and hybrid environments.\n   * OpenShift v4.4.x and newer is required for CP4I2020.2.1 (and subsequently Event Streams v10.0) and CP4Apps v4.2.x\n* Cloud Pak Operators\n    * Add [IBM Common Services operators](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/install_online_catalog_sources.html) \n    * Add the IBM operators to the list of installable operators. Same product note as above.\n    * Get [entitlement key](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/entitlement_key.html)\n* Cloud Pak for Integration\n    * Install Cloud Pak for Integration operator using the Operator hub catalog\n    * Install Cloud Pak for Integration platform navigator from the operator hub catalog\n    * The previous steps should have installed the common services if they were not installed before. So get the admin password via the `platform-auth-idp-credentials` secret in the `ibm-common-services` project. ` oc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 --decode`\n    * More information on [CP4I Installation](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/install.html)\n* Cloud Pak for Application\n  * You can either install Cloud Pak for Applications via the [CLI](https://www.ibm.com/support/knowledgecenter/SSCSJL_4.1.x/install-icpa-cli.html) or the [cloudctl CASE launcher](https://www.ibm.com/support/knowledgecenter/en/SSCSJL_4.2.x/install-icpa-case.html)\n  * Like the Cloud Pak for Integration you will need a key for the entitled registry.\n  \n\n### Deploying Event Streams\n\n- The instructions are in the [product documentation](https://www.ibm.com/support/knowledgecenter/SSGT7J_20.2/install/install_event_streams.html), and are very simple using the IBM Event Streams operator. Select minimal configuration with persistence. \n\n- Here is an example of the yaml. Note that there are a few sample yamls that come after you install the Event Streams Operator. This yaml is for Event Streams v10.0 -\n\n```yaml\napiVersion: eventstreams.ibm.com/v1beta1\nkind: EventStreams\nmetadata:\n  name: minimal-prod\n  namespace: cp4i\nspec:\n  version: 10.0.0\n  license:\n    accept: false\n    use: CloudPakForIntegrationProduction\n  adminApi: {}\n  adminUI: {}\n  collector: {}\n  restProducer: {}\n  schemaRegistry:\n    storage:\n      type: persistent-claim\n      size: 100Mi\n      class: enter-storage-class-name-here\n  strimziOverrides:\n    kafka:\n      replicas: 3\n      authorization:\n        type: runas\n      config:\n        inter.broker.protocol.version: '2.5'\n        interceptor.class.names: com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor\n        log.cleaner.threads: 6\n        log.message.format.version: '2.5'\n        num.io.threads: 24\n        num.network.threads: 9\n        num.replica.fetchers: 3\n        offsets.topic.replication.factor: 3\n      listeners:\n        external:\n          type: route\n          authentication:\n            type: scram-sha-512\n        tls:\n          authentication:\n            type: tls\n      metrics: {}\n      storage:\n        type: persistent-claim\n        size: 4Gi\n        class: enter-storage-class-name-here\n    zookeeper:\n      replicas: 3\n      metrics: {}\n      storage:\n        type: persistent-claim\n        size: 2Gi\n        class: enter-storage-class-name-here\n```\n\n### Deploy Strimzi\n\n* [Strimzi](https://strimzi.io/) is a very powerful and useful distributed Kafka deployment built for use with Kubernetes and OpenShift.\n* On OpenShift Container Platform v4.0.x to 4.3.x as well as pre-Event Streams v10 the Strimzi Operator will serve most of our needs. It can serve as the base so that we can utilize technologies like KafkaConnect, KafkaConnectS2i, KafkaConnector, Mirror Maker2 and other Custom Resources.\n* You can install it via [Operator Hub](https://strimzi.io/blog/2019/03/06/strimzi-and-operator-lifecycle-manager/)\n* You can also install the Strimzi and it's [Cluster Operator](https://strimzi.io/docs/operators/master/deploying.html#cluster-operator-str) by applying a yaml file through CLI.\n\n## Java Developer Environment\n\n* Go with [Quarkus](http://quarkus.io) so all being set with maven plugin.\n* You can scaffold your application through the [Quarkus Web UI](https://code.quarkus.io/) which will allow you to pick and choose your project dependencies. You may also do it through the CLI like so: \n```shell\nmvn io.quarkus:quarkus-maven-plugin:1.8.1.Final:create \\\n-DprojectGroupId=ibm.garage \\\n-DprojectArtifactId=your-application \\\n-Dextensions=\"kafka,kafka-streams,quarkus-kafka-streams\"\n  ```\n* If you already have your project created and you know the name of an extension you want ot add, you can do it through the CLI like the following - \n```shell\n./mvnw quarkus:add-extension -Dextensions=\"kafka\"\n```\n  \n* Here's a very simple Quarkus Producer application utilizing MicroProfile Reactive Messaging that sends messages to Event Stream v10 and newer: \n\n**Producer.java**\n\n```java\npackage com.ibm.garage.infrastructure;\n\nimport io.reactivex.Flowable;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\nimport javax.enterprise.context.ApplicationScoped;\nimport java.util.Random;\nimport java.util.concurrent.TimeUnit;\n\n\n/**\n * This class produces a message every 5 seconds.\n * The Kafka configuration is specified in the application.properties file.\n*/\n@ApplicationScoped\npublic class Producer {\n    private Random random = new Random();\n\n    @Outgoing(\"mock-producer\")      \n    public Flowable<KafkaRecord<Integer, String>> generate() {\n        return Flowable.interval(5, TimeUnit.SECONDS)    \n                .onBackpressureDrop()\n                .map(tick -> {      \n                    return KafkaRecord.of(random.nextInt(100), String.valueOf(random.nextInt(100)));\n                });\n    }                  \n}\n\n```\n\n\n* Now we have a simple Quarkus consumer, also using MicroProfile Reactive Messaging and printing the message.\n\n**Consumer.java**\n\n\n```java\npackage com.ibm.garage.infrastructure;\n\nimport io.quarkus.runtime.ShutdownEvent;\nimport io.quarkus.runtime.StartupEvent;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.event.Observes;\n\n@ApplicationScoped\npublic class Consumer {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(Consumer.class);\n\n    @Incoming(\"mock-consumer\")\n    public void consumingMessage(String incomingMessage) {\n        LOGGER.info(\"Message received from topic = {}\", incomingMessage);\n        //System.out.println(incomingMessage);\n    }\n    \n```\n\n* Quarkus does it's configuration via an `application.properties` file within the `src/main/resources/` path. A sample properties file.\n\n```properties\nquarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=${CERT_PASSWORD}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.type=PKCS12\n\n\n# mock message producer configuration\nmp.messaging.outgoing.mock-producer.connector=smallrye-kafka\nmp.messaging.outgoing.mock-producer.topic=${TOPIC_NAME}\nmp.messaging.outgoing.mock-producer.value.serializer=org.apache.kafka.common.serialization.StringSerializer\nmp.messaging.outgoing.mock-producer.key.serializer=org.apache.kafka.common.serialization.IntegerSerializer\n\n# Kafka Streams consumer configuration\nmp.messaging.incoming.mock-consumer.connector=smallrye-kafka\nmp.messaging.incoming.mock-consumer.topic=${TOPIC_NAME}\nmp.messaging.incoming.mock-consumer.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nmp.messaging.incoming.mock-consumer.key.deerializer=org.apache.kafka.common.serialization.IntegerDeserializer\n```\n* There are a few environment variables we need to pass to our properties before we can run it. Replace the values which can be retrieved from the Event Streams on CP4I UI.\n\n```shell\nexport BOOTSTRAP_SERVERS=your-external-bootstrap-address:xxxx \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-password \\\nexport TOPIC_NAME=your-topic \\\nexport CERT_LOCATION=/your-path-to-cert/es-cert.p12 \\\nexport CERT_PASSWORD=your-cert-password\n```\n\n* To run your applications run the following which will allow hot-reloading (if that's a functionality you need)\n```shell\n./mvnw quarkus:dev\n```\n\n\n* Kafka Strimzi image for docker and docker-compose to get up and running quickly. We have different docker composes files for you to start with:\n    * One Broker, one Zookeeper, kafka 2.5\n    * TODO\n\n## Python Developer Environment\n\n* There are a few Python packages but the [Confluent Kafka Python](https://github.com/confluentinc/confluent-kafka-python) package can serve our needs.\n* For Python environments you can use the Confluent Python package and install the dependency with pip.\n\n`pip install confluent-kafka`\n\n* The following is a very simple Python Producer that will serve as the scaffold:\n\n**KafkaProducer.py**\n\n```python\nimport json, os\nfrom confluent_kafka import KafkaError, Producer\n\nclass KafkaProducer:\n\n    def __init__(self, groupID = \"KafkaProducer\"):\n        # Get the producer configuration\n        self.producer_conf = self.getProducerConfiguration(groupID)\n        # Create the producer\n        self.producer = Producer(self.producer_conf)\n\n    def getProducerConfiguration(self,groupID):\n        try:\n            options ={\n                    'bootstrap.servers': os.environ['KAFKA_BROKERS'],\n                    'group.id': groupID\n            }\n            if (os.getenv('KAFKA_PASSWORD','') != ''):\n                # Set security protocol common to ES on prem and on IBM Cloud\n                options['security.protocol'] = 'SASL_SSL'\n                # Depending on the Kafka User, we will know whether we are talking to ES on prem or on IBM Cloud\n                # If we are connecting to ES on IBM Cloud, the SASL mechanism is plain\n                if (os.getenv('KAFKA_USER','') == 'token'):\n                    options['sasl.mechanisms'] = 'PLAIN'\n                # If we are connecting to ES on OCP, the SASL mechanism is scram-sha-512\n                else:\n                    options['sasl.mechanisms'] = 'SCRAM-SHA-512'\n                # Set the SASL username and password\n                options['sasl.username'] = os.getenv('KAFKA_USER','')\n                options['sasl.password'] = os.getenv('KAFKA_PASSWORD','')\n            # If we are talking to ES on prem, it uses an SSL self-signed certificate.\n            # Therefore, we need the CA public certificate for the SSL connection to happen.\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))):\n                options['ssl.ca.location'] = os.getenv('KAFKA_CERT','/certs/es-cert.pem')\n            \n            # Print out the producer configuration\n            self.printProducerConfiguration(options)\n\n            return options\n\n        except KeyError as error:\n            print('[KafkaProducer] - [ERROR] - A required environment variable does not exist: ' + error)\n            exit(1)\n\n    def printProducerConfiguration(self,options):\n        # Printing out producer config for debugging purposes        \n        print(\"[KafkaProducer] - This is the configuration for the producer:\")\n        print(\"[KafkaProducer] - -------------------------------------------\")\n        print('[KafkaProducer] - Bootstrap Server:      {}'.format(options['bootstrap.servers']))\n        if (os.getenv('KAFKA_PASSWORD','') != ''):\n            # Obfuscate password\n            if (len(options['sasl.password']) > 3):\n                obfuscated_password = options['sasl.password'][0] + \"*****\" + options['sasl.password'][len(options['sasl.password'])-1]\n            else:\n                obfuscated_password = \"*******\"\n            print('[KafkaProducer] - Security Protocol:     {}'.format(options['security.protocol']))\n            print('[KafkaProducer] - SASL Mechanism:        {}'.format(options['sasl.mechanisms']))\n            print('[KafkaProducer] - SASL Username:         {}'.format(options['sasl.username']))\n            print('[KafkaProducer] - SASL Password:         {}'.format(obfuscated_password))\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))): \n                print('[KafkaProducer] - SSL CA Location:       {}'.format(options['ssl.ca.location']))\n        print(\"[KafkaProducer] - -------------------------------------------\")\n\n    def delivery_report(self,err, msg):\n        \"\"\" Called once for each message produced to indicate delivery result. Triggered by poll() or flush(). \"\"\"\n        if err is not None:\n            print('[KafkaProducer] - [ERROR] - Message delivery failed: {}'.format(err))\n        else:\n            print('[KafkaProducer] - Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n\n    def publishEvent(self, topicName, eventToSend, keyName):\n        # Print the event to send\n        dataStr = json.dumps(eventToSend)\n        # Produce the message\n        self.producer.produce(topicName,key=eventToSend[keyName],value=dataStr.encode('utf-8'), callback=self.delivery_report)\n        # Flush\n        self.producer.flush()\n```\n* Now that we have the Producer we need to actually send Events.\n\n**ProducePlainMessage.py**\n\n```python\nimport argparse\nfrom KafkaProducer import KafkaProducer\n\nif __name__ == '__main__':\n\n    # Parse arguments\n    parser = argparse.ArgumentParser(description=\"Message Producer Example\")\n    parser.add_argument('-t', dest=\"topic\", required=True, help=\"Topic name\")\n    args = parser.parse_args()\n    \n    # Create the event to be sent\n    event = {\"eventKey\" : \"1\", \"message\" : \"This is a test message\"}\n    \n    # Print it out\n    print(\"--- Event to be published: ---\")\n    print(event)\n    print(\"----------------------------------------\")\n    \n    # Create the Kafka Producer\n    kafka_producer = KafkaProducer()\n    # Publish the event\n    kafka_producer.publishEvent(args.topic,event,\"eventKey\")\n```\n\n* The following is a simple Python consumer\n\n**KafkaConsumer.py**\n\n```python\nimport json,os\nfrom confluent_kafka import Consumer, KafkaError\n\n\nclass KafkaConsumer:\n\n    def __init__(self, topic_name = \"kafka-producer\", groupID = 'KafkaConsumer', autocommit = True):\n        # Get the consumer configuration\n        self.consumer_conf = self.getConsumerConfiguration(groupID, autocommit)\n        # Create the Avro consumer\n        self.consumer = Consumer(self.consumer_conf)\n        # Subscribe to the topic\n        self.consumer.subscribe([topic_name])\n\n    def getConsumerConfiguration(self, groupID, autocommit):\n        try:\n            options ={\n                    'bootstrap.servers': os.environ['KAFKA_BROKERS'],\n                    'group.id': groupID,\n                    'auto.offset.reset': \"earliest\",\n                    'enable.auto.commit': autocommit,\n            }\n            if (os.getenv('KAFKA_PASSWORD','') != ''):\n                # Set security protocol common to ES on prem and on IBM Cloud\n                options['security.protocol'] = 'SASL_SSL'\n                # Depending on the Kafka User, we will know whether we are talking to ES on prem or on IBM Cloud\n                # If we are connecting to ES on IBM Cloud, the SASL mechanism is plain\n                if (os.getenv('KAFKA_USER','') == 'token'):\n                    options['sasl.mechanisms'] = 'PLAIN'\n                # If we are connecting to ES on OCP, the SASL mechanism is scram-sha-512\n                else:\n                    options['sasl.mechanisms'] = 'SCRAM-SHA-512'\n                # Set the SASL username and password\n                options['sasl.username'] = os.getenv('KAFKA_USER','')\n                options['sasl.password'] = os.getenv('KAFKA_PASSWORD','')\n            # If we are talking to ES on prem, it uses an SSL self-signed certificate.\n            # Therefore, we need the CA public certificate for the SSL connection to happen.\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))):\n                options['ssl.ca.location'] = os.getenv('KAFKA_CERT','/certs/es-cert.pem')\n\n            # Print out the producer configuration\n            self.printConsumerConfiguration(options)\n\n            return options\n\n        except KeyError as error:\n            print('[KafkaConsumer] - [ERROR] - A required environment variable does not exist: ' + error)\n            exit(1)\n    \n    def printConsumerConfiguration(self,options):\n        # Printing out consumer config for debugging purposes        \n        print(\"[KafkaConsumer] - This is the configuration for the consumer:\")\n        print(\"[KafkaConsumer] - -------------------------------------------\")\n        print('[KafkaConsumer] - Bootstrap Server:      {}'.format(options['bootstrap.servers']))\n        if (os.getenv('KAFKA_PASSWORD','') != ''):\n            # Obfuscate password\n            if (len(options['sasl.password']) > 3):\n                obfuscated_password = options['sasl.password'][0] + \"*****\" + options['sasl.password'][len(options['sasl.password'])-1]\n            else:\n                obfuscated_password = \"*******\"\n            print('[KafkaConsumer] - Security Protocol:     {}'.format(options['security.protocol']))\n            print('[KafkaConsumer] - SASL Mechanism:        {}'.format(options['sasl.mechanisms']))\n            print('[KafkaConsumer] - SASL Username:         {}'.format(options['sasl.username']))\n            print('[KafkaConsumer] - SASL Password:         {}'.format(obfuscated_password))\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))): \n                print('[KafkaConsumer] - SSL CA Location:       {}'.format(options['ssl.ca.location']))\n        print('[KafkaConsumer] - Offset Reset:          {}'.format(options['auto.offset.reset']))\n        print('[KafkaConsumer] - Autocommit:            {}'.format(options['enable.auto.commit']))\n        print(\"[KafkaConsumer] - -------------------------------------------\")\n    \n    # Prints out and returns the decoded events received by the consumer\n    def traceResponse(self, msg):\n        print('[KafkaConsumer] - Next Message consumed from {} partition: [{}] at offset: {}\\n\\tkey: {}\\n\\tvalue: {}'\n                    .format(msg.topic(), msg.partition(), msg.offset(), msg.key().decode('utf-8'), msg.value().decode('utf-8')))\n\n    # Polls for next event\n    def pollNextEvent(self):\n        # Poll for messages\n        msg = self.consumer.poll(timeout=10.0)\n        # Validate the returned message\n        if msg is None:\n            print(\"[KafkaConsumer] - [INFO] - No new messages on the topic\")\n        elif msg.error():\n            if (\"PARTITION_EOF\" in msg.error()):\n                print(\"[KafkaConsumer] - [INFO] - End of partition\")\n            else:\n                print(\"[KafkaConsumer] - [ERROR] - Consumer error: {}\".format(msg.error()))\n        else:\n            # Print the message\n            self.traceResponse(msg)\n    \n    def close(self):\n        self.consumer.close()\n```\n\n* Now for the plain Kafka Python consumer\n\n**ConsumePlainMessage.py**\n\n```python\nimport argparse\nfrom KafkaConsumer import KafkaConsumer\n\n####################### MAIN #######################\nif __name__ == '__main__':\n    \n    # Parse arguments\n    parser = argparse.ArgumentParser(description=\"Message Consumer Example\")\n    parser.add_argument('-t', dest=\"topic\", required=True, help=\"Topic name\")\n    args = parser.parse_args()\n\n    # Create a Kafka Consumer\n    kafka_consumer = KafkaConsumer(args.topic)\n    # Poll for next message\n    kafka_consumer.pollNextEvent()\n    # Close the consumer\n    kafka_consumer.close()\n```\n\n\n* In `KafkaProducer.py` as well as `KafkaConsumer.py` we will need to provide environment variables so that our producer can parse it and actually connect to an Event Streams intance.\n\n```shell\nexport KAFKA_BROKERS=your-brokers \\\nexport KAFKA_USER=your-scram-username \\\nexport KAFKA_PASSWORD=your-scram-password \\\nexport KAFKA_CERT=your-cert-path\n\n```\n\n* To run your Producer and send a simple message - \n```shell\npython ProducePlainMessage.py -t <your-topic-name>\n```\n\n* To consume\n```shell\npython ConsumePlainMessage.py -t <your-topic-name>\n```\n\n\n## Node.js Developer Environment\n\n* The following code is based off this application [here](https://chrisphillips-cminion.github.io/eventstreams/2019/08/22/NodeJS-IBMES-101.html)\n\n* We will make use of the [node-rdkafka](https://github.com/Blizzard/node-rdkafka) library for our purposes. Therefore we will need to install the dependency for the project first:\n\n```shell\nnpm install node-rdkafka --save\n```\n\n* Here is a simple Node.js producer application that will produce 20 messages to the Kafka topic.\n\n**producer.js**\n\n```javascript\nvar Kafka = require('node-rdkafka');\n\nvar kafka_options = {\n    //'debug': 'all',\n    'metadata.broker.list': process.env.KAFKA_BROKERS,\n    'security.protocol': 'sasl_ssl',\n    'sasl.mechanisms': 'SCRAM-SHA-512',\n    'sasl.username': process.env.SCRAM_USERNAME,\n    'sasl.password': process.env.SCRAM_PASSWORD,\n    'ssl.ca.location': process.env.PEM_PATH,\n    'log.connection.close' : false,\n    'client.id': 'ES-NodeJS-101'\n};\n\nvar topicName = process.env.TOPIC_NAME\n\nproducer = new Kafka.Producer(kafka_options);\nproducer.setPollInterval(100);\n// Register listener for debug information; only invoked if debug option set in kafka_options\nproducer.on('event.log', function(log) {\n    console.log(log);\n});\n// Register error listener\nproducer.on('event.error', function(err) {\n    console.error('Error from producer:' + JSON.stringify(err));\n});\n\n// Register delivery report listener\nproducer.on('delivery-report', function(err, dr) {\n    if (err) {\n        console.error('Delivery report: Failed sending message ' + dr.value);\n        console.error(err);\n        // We could retry sending the message\n    } else {\n        console.log('Message produced, partition: ' + dr.partition + ' offset: ' + dr.offset);\n    }\n});\n\nvar sendMessageToTopic = function() {\n    console.log('The producer has connected.');\n    const genMessage = i => new Buffer.from(`Kafka example, message number ${i}`);\n    console.log('Producer is now sending 20 messages to the Kafka Topic');\n    const maxMessages = 20;\n    for (var i = 0; i < maxMessages; i++) {\n        producer.produce(topicName, -1, genMessage(i), i);\n    }\n}\n\n// Register callback invoked when producer has connected\nproducer.on('ready', function() {\n\n    sendMessageToTopic();\n\n    // request metadata for all topics\n    producer.getMetadata({\n        timeout: 10000\n    },\n    function(err, metadata) {\n        if (err) {\n            console.error('Error getting metadata: ' + JSON.stringify(err));\n            shutdown(-1);\n        } else {\n            console.log('Producer obtained metadata: ' + JSON.stringify(metadata));\n            var topicsByName = metadata.topics.filter(function(t) {\n                return t.name === topicName;\n            });\n            if (topicsByName.length === 0) {\n                console.error('ERROR - Topic ' + topicName + ' does not exist. Exiting');\n                shutdown(-1);\n            }\n        }\n    });\n    var counter = 0;\n});\n\nproducer.connect();\n\n```\n\n* Below is a simple Node.js consumer:\n\n**consumer.js**\n\n```javascript\nvar Kafka = require('node-rdkafka');\n\nvar kafka_options = {\n    //'debug': 'all',\n    'metadata.broker.list': process.env.KAFKA_BROKERS,\n    'security.protocol': 'sasl_ssl',\n    'sasl.mechanisms': 'SCRAM-SHA-512',\n    'sasl.username': process.env.SCRAM_USERNAME,\n    'sasl.password': process.env.SCRAM_PASSWORD,\n    'ssl.ca.location': process.env.PEM_PATH,\n    'log.connection.close' : false,\n    'group.id': 'ES-NodeJS-101-consumer'\n};\n\n\n\nvar topicName = process.env.TOPIC_NAME\n\nconst consumer = new Kafka.KafkaConsumer(kafka_options, {\n    \"auto.offset.reset\": \"beginning\"\n  });\n\n\nconsumer.on('event.log', function(log) {\n    console.log(log);\n});\n\n// Register error listener\nconsumer.on('event.error', function(err) {\n    console.error('Error from consumer:' + JSON.stringify(err));\n});\n\nvar consumedMessages = []\n// Register callback to be invoked when consumer has connected\nconsumer.on('ready', function() {\n    console.log('The consumer has connected.');\n\n    // request metadata for one topic\n    consumer.getMetadata({\n        topic: topicName,\n        timeout: 10000\n    },\n    function(err, metadata) {\n        if (err) {\n            console.error('Error getting metadata: ' + JSON.stringify(err));\n            shutdown(-1);\n        } else {\n            console.log('Consumer obtained metadata: ' + JSON.stringify(metadata));\n\n        }\n    });\n\n    consumer.subscribe([topicName]);\n\n    consumerLoop = setInterval(function () {\n        if (consumer.isConnected()) {\n            // The consume(num, cb) method can take a callback to process messages.\n            // In this sample code we use the \".on('data')\" event listener instead,\n            // for illustrative purposes.\n            consumer.consume(10);\n        }    \n\n        if (consumedMessages.length === 0) {\n            console.log('No messages consumed');\n        } else {\n            for (var i = 0; i < consumedMessages.length; i++) {\n                var m = consumedMessages[i];\n                console.log('Message consumed: topic=' + m.topic + ', partition=' + m.partition + ', offset=' + m.offset + ', key=' + m.key + ', value=' + m.value.toString());\n            }\n            consumedMessages = [];\n        }\n    }, 2000);\n});\n\n// Register a listener to process received messages\nconsumer.on('data', function(m) {\n    consumedMessages.push(m);\n});\n\nconsumer.connect()\n```\n* Like the Python environments, we will need a SCRAM Username, password, and the PEM Certificate. We will need a few environment variables to provide to our application so that it can connect to Event Streams.\n\n```shell\nexport KAFKA_BROKERS=your-external-bootstrap-server-address \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-password \\\nexport TOPIC_NAME=your-topic-name \\\nexport PEM_PATH=/path-to-your-pem-certificate/es-cert.pem\n```\n\n* To run these applications after exporting the necessary environment variables:\n```shell\nnode producer.js\nnode consumer.js\n```\n\n\n# Golang Developer Environment\n\n* Similar to the Python Developer environment, we can leverage the [Confluent-kafka-go](https://github.com/confluentinc/confluent-kafka-go) library.\n* If using Go/Golang v 1.13 and newer you can get it using Go Modules by importing via github.\n  - `import \"github.com/confluentinc/confluent-kafka-go/kafka\"`\n  - `go build ./...`\n* Otherwise if you cannot use Go modules you can manually install it\n  - `go get -u gopkg.in/confluentinc/confluent-kafka-go.v1/kafka`\n  - `import \"gopkg.in/confluentinc/confluent-kafka-go.v1/kafka\"`\n  - Note that our sample producer and consumer uses this option.\n  \n\n* Below is a simple Go Kafka producer that sends 7 Test messages.\n\n**producer.go**\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"gopkg.in/confluentinc/confluent-kafka-go.v1/kafka\"\n)\n\nfunc main() {\n\n\tkafkaBrokers := os.Getenv(\"KAFKA_BROKERS\")\n\tscramUsername := os.Getenv(\"SCRAM_USERNAME\")\n\tscramPassword := os.Getenv(\"SCRAM_PASSWORD\")\n\tpemPath := os.Getenv(\"PEM_PATH\")\n\ttopic := os.Getenv(\"TOPIC_NAME\")\n\n\n\tp, err := kafka.NewProducer(&kafka.ConfigMap {\n\t\t\"bootstrap.servers\": kafkaBrokers,\n\t\t\"security.protocol\": \"SASL_SSL\",\n\t\t\"sasl.mechanism\": \"SCRAM-SHA-512\",\n\t\t\"group.id\": \"golang-kafka-producer\",\n\t\t\"sasl.username\": scramUsername,\n\t\t\"sasl.password\": scramPassword,\n\t\t\"ssl.ca.location\": pemPath})\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tdefer p.Close()\n\n\t// Delivery report handler for produced messages\n\tgo func() {\n\t\tfor e := range p.Events() {\n\t\t\tswitch ev := e.(type) {\n\t\t\tcase *kafka.Message:\n\t\t\t\tif ev.TopicPartition.Error != nil {\n\t\t\t\t\tfmt.Printf(\"Delivery failed: %v\\n\", ev.TopicPartition)\n\t\t\t\t} else {\n\t\t\t\t\tfmt.Printf(\"Delivered message to %v\\n\", ev.TopicPartition)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Produce messages to topic (asynchronously)\n\tfor _, word := range []string{\"TEST\", \"TEST1\", \"TEST2\", \"TEST3\", \"TEST4\", \"TEST5\", \"TEST6\"} {\n\t\tp.Produce(&kafka.Message{\n\t\t\tTopicPartition: kafka.TopicPartition{Topic: &topic, Partition: kafka.PartitionAny},\n\t\t\tValue: []byte(word),\n\t\t}, nil)\n\t}\n\n\t// Wait for message deliveries before shutting down\n\tp.Flush(15 * 1000)\n}\n```\n\n* Also we have a simple consumer application:\n\n**consumer.go**\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"gopkg.in/confluentinc/confluent-kafka-go.v1/kafka\"\n)\n\nfunc main() {\n\n\tkafkaBrokers := os.Getenv(\"KAFKA_BROKERS\")\n\tscramUsername := os.Getenv(\"SCRAM_USERNAME\")\n\tscramPassword := os.Getenv(\"SCRAM_PASSWORD\")\n\tpemPath := os.Getenv(\"PEM_PATH\")\n\ttopic := os.Getenv(\"TOPIC_NAME\")\n\n\tc, err := kafka.NewConsumer(&kafka.ConfigMap{\n\t\t\"bootstrap.servers\": kafkaBrokers,\n\t\t\"security.protocol\": \"SASL_SSL\",\n\t\t\"sasl.mechanism\": \"SCRAM-SHA-512\",\n\t\t\"sasl.username\": scramUsername,\n\t\t\"sasl.password\": scramPassword,\n\t\t\"ssl.ca.location\": pemPath,\n\t\t\"group.id\": \"golang-kafka-consumer\",\n\t\t\"auto.offset.reset\": \"earliest\"})\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tc.SubscribeTopics([]string{topic, \"^aRegex.*[Tt]opic\"}, nil)\n\n\tfor {\n\t\tmsg, err := c.ReadMessage(-1)\n\t\tif err == nil {\n\t\t\tfmt.Printf(\"Message on %s: %s\\n\", msg.TopicPartition, string(msg.Value))\n\t\t} else {\n\t\t\t// The client will automatically try to recover from all errors.\n\t\t\tfmt.Printf(\"Consumer error: %v (%v)\\n\", err, msg)\n\t\t}\n\t}\n\n\tc.Close()\n}\n```\n\n* Like all the previous developer environments (Java, Python and Node.js) we will need to set environment variables for our application. Replace the values below with your own.\n\n```shell\nexport KAFKA_BROKERS=your-external-bootstrap-address:443 \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-pasword \\\nexport PEM_PATH=/path-to-pem-cert/es-cert.pem \\\nexport TOPIC_NAME=your-topic\n```\n\n* Now you can test the applications.\n\n```shell\ngo run consumer.go\ngo run producer.go\n```\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}