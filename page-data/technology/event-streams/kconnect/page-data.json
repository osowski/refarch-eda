{"componentChunkName":"component---src-pages-technology-event-streams-kconnect-mdx","path":"/technology/event-streams/kconnect/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect with Event Streams on Cloud","description":"A set of labs and reference for working with Kafka Connect with Event streams on cloud"},"relativePagePath":"/technology/event-streams/kconnect.mdx","titleType":"append","MdxNode":{"id":"aaf1acf0-7d50-5efa-b7a3-168216bd13a3","children":[],"parent":"0a0dd534-190b-5f9a-879f-99c632a91deb","internal":{"content":"---\ntitle: Kafka Connect with Event Streams on Cloud\ndescription: A set of labs and reference for working with Kafka Connect with Event streams on cloud\n---\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>General pre-requisites</AnchorLink>\n  <AnchorLink>Scenario Setup</AnchorLink>\n  <AnchorLink>Scenario 1: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source</AnchorLink>\n  <AnchorLink>Scenario 2: Event Streams on Cloud to MQ on premise via MQ connector sink</AnchorLink>\n  <AnchorLink>Scenario 3: Event Streams on Cloud to DB2 on premise via JDBC Sink connector</AnchorLink>\n  <AnchorLink>Scenario 4: Run the solution component on Kubernetes as a Service</AnchorLink> \n</AnchorLinks>\n\n## Overview\n\nThis lab will address multiple scenarios that aim to build an end to end data pipeline, as depicted by the following figure, using Event Streams on Cloud:\n\n![1](./images/kconnect-overview.png)\n\nThe end to end scenario is address a classical business use case where stores are sending their transactions to a central messaging platform, based on queues, and with the adoption of loosely coupled microservice, real time analytics and complex event processing, Kafka is added to the legacy environment. Adopting Kafka connect let integrate with existing applications without any changes. For example the scenario illustrate JDBC Sink connector to save to existing data base.\n\nThis lab is about Kafka Connect running on Kubernetes and accessing public cloud Event Streams instance.\n\nFor this lab the input messaging is RabbitMQ, the output messaging is MQ, the database is DB2 on Cloud.\n\n<InlineNotification kind=\"info\">You need to decide what your 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a Kubernetes cluster. We propose to run the components with docker compose for the scenario 1,2,3 and do a deployment of the components on Kubernetes as a service on scenario 4. If you do not want to build all the components, we have each of them available in dockerhub and the docker compose file should run automatically.\n</InlineNotification>\n\n## General pre-requisites\n\nWe need the following IBM Cloud services created and tools to run the lab. We try to use docker images as much as possible to do not impact your local laptop.\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* Event Streams instance, may be reuse the one created in [this lab](./es-cloud/).\n* [DB2 instance](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started).\n* [IBM Kubernetes Service](https://cloud.ibm.com/docs/containers?topic=containers-cs_cluster_tutorial#cs_cluster_tutorial).\n* [IBM Cloud CLI](https://cloud.ibm.com/docs/cli?topic=cli-getting-started).\n* If you want to run locally you need to get [docker compose](https://docs.docker.com/compose/).\n* [git CLI](https://git-scm.com/downloads).\n* [maven](https://maven.apache.org/install.html).\n\nFor the on-premise environment, we will not use VMs, but simply run some of the components on IBM Kubernetes Service platform or Openshift. The point is that the workload is packaged as container images and can run anywhere.\n\n## Scenario setup\n\n1. Login to the cloud via CLI: `ibmcloud login`\n1. Initialize the Event Streams CLI and select the target Event Streams cluster: `ibmcloud es init`\n1. If not done before using UI or from previous work add the following topics:\n    * `ibmcloud es topic-create connect-configs`\n    * `ibmcloud es topic-create connect-offsets`\n    * `ibmcloud es topic-create connect-status`\n    * `ibmcloud es topic-create inventory`\n    * `ibmcloud es topic-create items`\n1. Clone the lab repository: `git clone https://github.com/jbcodeforce/eda-kconnect-lab && cd eda-kconnect-lab`.\n1. Prepare the script to set the environment variables used by all the components of the solution, like the Kafka broker, by first renaming the `scripts/setenv-TMP.sh` in [this repository](https://github.com/jbcodeforce/eda-kconnect-lab) to `scripts/setenv.sh` and then modify the KAFKA_BROKERS and KAFKA_APIKEY with the respecting value of the Event Streams credentials.\n1. Prepare the Kafka Connect environment, as we need to use three connectors. Therefore we need to clone the source, build and get the jars file in the connectors. In fact we have developed scripts to automate those tedious steps:\n\n  * Under the `kconnect` folder run `./setupConnectors.sh` script and get the three connectors downloaded and built.\n  * Build a docker image for the connector: this is also done by running the script: `./createOrStartKconnect.sh build`.\n\n<InlineNotification kind=\"info\">You need to decide what 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a Kubernetes cluster. We propose to run the components with docker compose for the scenario 1,2,3 and do a deployment of the components on Kubernetes as a service on scenario 4.\n</InlineNotification>\n\n## Scenario 1: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source\n\n### Deploy RabbitMQ on IKS\n\n### Configure the kafka connector for Rabbitmq source \n\n## Scenario 2: Event Streams on Cloud to MQ on premise via MQ connector sink\n\n## Scenario 3: Event Streams on Cloud to DB2 on premise via JDBC Sink connector\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from the `inventory topic` and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n### Pre-requisites\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for cRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) under the `inventory-app` folder of [this repository](https://github.com/jbcodeforce/eda-kconnect-lab). At the application starts stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/import.sql) from `inventory-app/src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n  \n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n### Run the Kafka Connector in distributed mode\n\nThe docker image built in the [setup](#scenario-setup) has the configuration for kafka connect distributed cluster, we need in this scenario to start connect and upload the DB2 Sink connector definition. To start it, run the script `./createOrStartKconnect.sh start` under `kconnect` folder.\n\n### Upload the DB2 sink definition\n\nRename the file `db2-sink-config-TMPL.json` as `db2-sink-config.json` and modify the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format`.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh url-kafka-connect` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhodt:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n### Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```\nProduce to the topic inventory\n[KafkaProducer] - This is the configuration for the producer:\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProducerInventory', 'acks': 0, 'request.timeout.ms': 10000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5'}\nsending -> {'storeName': 'LA02', 'itemCode': 'IT09', 'id': 0, 'timestamp': 1591211295.617515}\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'PT02', 'itemCode': 'IT00', 'id': 1, 'timestamp': 1591211296.7727954}\n[KafkaProducer] - Message delivered to inventory [0]\n\n```\n\n### Verify records are uploaded into the Inventory database\n\nUsing the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n\n## Scenario 4: Run the solution component on Kubernetes as a Service","type":"Mdx","contentDigest":"31e5c7728bb00210a6b293764dd34bf4","counter":373,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Kafka Connect with Event Streams on Cloud","description":"A set of labs and reference for working with Kafka Connect with Event streams on cloud"},"exports":{},"rawBody":"---\ntitle: Kafka Connect with Event Streams on Cloud\ndescription: A set of labs and reference for working with Kafka Connect with Event streams on cloud\n---\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>General pre-requisites</AnchorLink>\n  <AnchorLink>Scenario Setup</AnchorLink>\n  <AnchorLink>Scenario 1: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source</AnchorLink>\n  <AnchorLink>Scenario 2: Event Streams on Cloud to MQ on premise via MQ connector sink</AnchorLink>\n  <AnchorLink>Scenario 3: Event Streams on Cloud to DB2 on premise via JDBC Sink connector</AnchorLink>\n  <AnchorLink>Scenario 4: Run the solution component on Kubernetes as a Service</AnchorLink> \n</AnchorLinks>\n\n## Overview\n\nThis lab will address multiple scenarios that aim to build an end to end data pipeline, as depicted by the following figure, using Event Streams on Cloud:\n\n![1](./images/kconnect-overview.png)\n\nThe end to end scenario is address a classical business use case where stores are sending their transactions to a central messaging platform, based on queues, and with the adoption of loosely coupled microservice, real time analytics and complex event processing, Kafka is added to the legacy environment. Adopting Kafka connect let integrate with existing applications without any changes. For example the scenario illustrate JDBC Sink connector to save to existing data base.\n\nThis lab is about Kafka Connect running on Kubernetes and accessing public cloud Event Streams instance.\n\nFor this lab the input messaging is RabbitMQ, the output messaging is MQ, the database is DB2 on Cloud.\n\n<InlineNotification kind=\"info\">You need to decide what your 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a Kubernetes cluster. We propose to run the components with docker compose for the scenario 1,2,3 and do a deployment of the components on Kubernetes as a service on scenario 4. If you do not want to build all the components, we have each of them available in dockerhub and the docker compose file should run automatically.\n</InlineNotification>\n\n## General pre-requisites\n\nWe need the following IBM Cloud services created and tools to run the lab. We try to use docker images as much as possible to do not impact your local laptop.\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* Event Streams instance, may be reuse the one created in [this lab](./es-cloud/).\n* [DB2 instance](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started).\n* [IBM Kubernetes Service](https://cloud.ibm.com/docs/containers?topic=containers-cs_cluster_tutorial#cs_cluster_tutorial).\n* [IBM Cloud CLI](https://cloud.ibm.com/docs/cli?topic=cli-getting-started).\n* If you want to run locally you need to get [docker compose](https://docs.docker.com/compose/).\n* [git CLI](https://git-scm.com/downloads).\n* [maven](https://maven.apache.org/install.html).\n\nFor the on-premise environment, we will not use VMs, but simply run some of the components on IBM Kubernetes Service platform or Openshift. The point is that the workload is packaged as container images and can run anywhere.\n\n## Scenario setup\n\n1. Login to the cloud via CLI: `ibmcloud login`\n1. Initialize the Event Streams CLI and select the target Event Streams cluster: `ibmcloud es init`\n1. If not done before using UI or from previous work add the following topics:\n    * `ibmcloud es topic-create connect-configs`\n    * `ibmcloud es topic-create connect-offsets`\n    * `ibmcloud es topic-create connect-status`\n    * `ibmcloud es topic-create inventory`\n    * `ibmcloud es topic-create items`\n1. Clone the lab repository: `git clone https://github.com/jbcodeforce/eda-kconnect-lab && cd eda-kconnect-lab`.\n1. Prepare the script to set the environment variables used by all the components of the solution, like the Kafka broker, by first renaming the `scripts/setenv-TMP.sh` in [this repository](https://github.com/jbcodeforce/eda-kconnect-lab) to `scripts/setenv.sh` and then modify the KAFKA_BROKERS and KAFKA_APIKEY with the respecting value of the Event Streams credentials.\n1. Prepare the Kafka Connect environment, as we need to use three connectors. Therefore we need to clone the source, build and get the jars file in the connectors. In fact we have developed scripts to automate those tedious steps:\n\n  * Under the `kconnect` folder run `./setupConnectors.sh` script and get the three connectors downloaded and built.\n  * Build a docker image for the connector: this is also done by running the script: `./createOrStartKconnect.sh build`.\n\n<InlineNotification kind=\"info\">You need to decide what 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a Kubernetes cluster. We propose to run the components with docker compose for the scenario 1,2,3 and do a deployment of the components on Kubernetes as a service on scenario 4.\n</InlineNotification>\n\n## Scenario 1: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source\n\n### Deploy RabbitMQ on IKS\n\n### Configure the kafka connector for Rabbitmq source \n\n## Scenario 2: Event Streams on Cloud to MQ on premise via MQ connector sink\n\n## Scenario 3: Event Streams on Cloud to DB2 on premise via JDBC Sink connector\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from the `inventory topic` and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n### Pre-requisites\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for cRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) under the `inventory-app` folder of [this repository](https://github.com/jbcodeforce/eda-kconnect-lab). At the application starts stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/import.sql) from `inventory-app/src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n  \n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n### Run the Kafka Connector in distributed mode\n\nThe docker image built in the [setup](#scenario-setup) has the configuration for kafka connect distributed cluster, we need in this scenario to start connect and upload the DB2 Sink connector definition. To start it, run the script `./createOrStartKconnect.sh start` under `kconnect` folder.\n\n### Upload the DB2 sink definition\n\nRename the file `db2-sink-config-TMPL.json` as `db2-sink-config.json` and modify the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format`.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh url-kafka-connect` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhodt:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n### Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```\nProduce to the topic inventory\n[KafkaProducer] - This is the configuration for the producer:\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProducerInventory', 'acks': 0, 'request.timeout.ms': 10000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5'}\nsending -> {'storeName': 'LA02', 'itemCode': 'IT09', 'id': 0, 'timestamp': 1591211295.617515}\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'PT02', 'itemCode': 'IT00', 'id': 1, 'timestamp': 1591211296.7727954}\n[KafkaProducer] - Message delivered to inventory [0]\n\n```\n\n### Verify records are uploaded into the Inventory database\n\nUsing the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n\n## Scenario 4: Run the solution component on Kubernetes as a Service","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/event-streams/kconnect.mdx"}}}}