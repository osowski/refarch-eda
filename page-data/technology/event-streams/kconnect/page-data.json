{"componentChunkName":"component---src-pages-technology-event-streams-kconnect-mdx","path":"/technology/event-streams/kconnect/","result":{"pageContext":{"frontmatter":{"title":"Kafka Connect with Event Streams on Cloud","description":"A set of labs and reference for working with Kafka Connect with Event streams on cloud"},"relativePagePath":"/technology/event-streams/kconnect.mdx","titleType":"append","MdxNode":{"id":"aaf1acf0-7d50-5efa-b7a3-168216bd13a3","children":[],"parent":"0a0dd534-190b-5f9a-879f-99c632a91deb","internal":{"content":"---\ntitle: Kafka Connect with Event Streams on Cloud\ndescription: A set of labs and reference for working with Kafka Connect with Event streams on cloud\n---\n  \n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Solution anatomy</AnchorLink>\n  <AnchorLink>General pre-requisites</AnchorLink>\n  <AnchorLink>Scenario setup</AnchorLink>\n  <AnchorLink>Scenario 1: Event Streams on Cloud to MQ on premise via MQ connector sink</AnchorLink>\n  <AnchorLink>Scenario 2: Deploying Kafka Connector MQ Sink to OpenShift</AnchorLink>\n  <AnchorLink>Scenario 3: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source</AnchorLink>\n  <AnchorLink>Scenario 4: Event Streams on Cloud to DB2 on premise via JDBC Sink connector</AnchorLink>\n  <AnchorLink>Scenario 5: Run the solution components end to end on Kubernetes</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis lab will address multiple scenarios that aim to build an end to end data pipeline, as depicted by the following figure, using Event Streams on Cloud. At the high level Kafka connect is used to integrate external systems into the Kafka ecosystem. For example external system can inject message to queue manager, from which a first Kafka source connector will get the message to a Kafka topic, which then will be processed by a series of event driven microservices down to a final topic, that will be use by Sink connectors.\n\n![1](./images/kconnect-overview.png)\n\nTo support this lab we are reusing a classical business use case where stores are sending their transactions to a central messaging platform, based on queues, and with the adoption of loosely coupled microservice, real time analytics and complex event processing, Kafka is added to the legacy environment. Adopting Kafka connect lets integrate with existing applications without any changes.\n\n## Solution anatomy\n\nThe lab is divided into scenarios that can be combined to support the real time inventory data pipeline as illustrated in the figure below:\n\n![2](./images/kconnect-scenario-components.png)\n\n1. The store application, is a Quarkus based app, generating item sales to RabbitMQ `items` queue. The code of this application is under the `store-sale-producer` folder, in the [lab repository](https://github.com/jbcodeforce/eda-kconnect-lab/). We will address how to get this code in the pre-requisite section.\n\n    * RabbitMQ runs in docker image started locally via docker compose. The messages are in the `items` queue.\n    * The lab, focusing on the injection to Kafka, is documented in the [scenario 2](#scenario-2:-rabbitmq-on-premise-to-event-streams-on-cloud-via-rabbitmq-connector-source).\n    * [A] The Sink connector description is in the [kconnect](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/kconnect) folder.\n    * Kafka backbone is Event Streams on Cloud.\n1. The inventory MS is a Kafka Stream application, done with Reactive Messaging and Kafka Stream API. The folder is\nFor example the scenario illustrate JDBC Sink connector to save to existing data base.\n1. The mock up Inventory mainframe application is not implemented and we will use the MQ tools to view the message in the `inventory` queue\n\n    * The MQ Sink connector [B] configuration is defined in the [kconnect](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/kconnect) folder.\n    * MQ broker runs in docker container started with docker-compose\n    * The lab scenario is [the number 1](#scenario-1:-event-streams-on-cloud-to-mq-on-premise-via-mq-connector-sink)\n1. The Inventory Application, using DB2 as datasource is a quarkus app using hibernate with panache, defined in the [nventory-app](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) folder\n\n    * The JDBC Sink connector [C] configuration is defined in the [kconnect](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/kconnect) folder.\n    * The [scenario 3](#scenario-3:-event-streams-on-cloud-to-db2-on-premise-via-jdbc-sink-connector) lab goes over how the Kafka Connect JDBC sink works.\n1. The [scenario 4](#scenario-4:-run-the-solution-components-end-to-end-on-kubernetes) addresses the end to end solution, which is basically an end to end demonstration of a simple data pipeline for a real time view of an inventory solution.\n\n<InlineNotification kind=\"info\">You need to decide what your 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a premise cluster. We propose to run the components with docker compose for the scenario 1,2,3 and scenario 4 is for a deployment of the components on a kubernetes cluster that could run on premise or on any cloud provider using Openshift. If you do not want to build all the components, we have each of them available in docker hub and the docker compose file should run them automatically.\n</InlineNotification>\n\n## General pre-requisites\n\nWe need the following IBM Cloud services created and tools to run the lab. We try to use docker images as much as possible to do not impact your local laptop.\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* Event Streams instance, may be reuse the one created in [this lab](/technology/event-streams/es-cloud/).\n* [IBM Cloud CLI](https://cloud.ibm.com/docs/cli?topic=cli-getting-started).\n* If you want to run locally you need to get [docker compose](https://docs.docker.com/compose/).\n* [git CLI](https://git-scm.com/downloads).\n* [Maven](https://maven.apache.org/install.html).\n\nFor the on-premise environment, we will not use VMs, but simply run some of the components on IBM premise Service platform or Openshift. The point is that the workload is packaged as container images and can run anywhere.\n\n## Scenario setup\n\n1. Login to the cloud via CLI: `ibmcloud login`\n1. Initialize the Event Streams CLI and select the target Event Streams cluster: `ibmcloud es init`\n1. Define connect topics: When running in distributed mode, the connectors need three topics as presented in the `create topics` table [here](https://ibm.github.io/event-streams/connecting/setting-up-connectors/).\n\n    * **connect-configs**: This topic will store the connector and task configurations.\n    * **connect-offsets**: This topic is used to store offsets for Kafka Connect.\n    * **connect-status**: This topic will store status updates of connectors and tasks.\n\n    Using IBM Event Streams CLI, the topics are created via the commands like:\n\n    ```shell\n    # log to the kubernetes cluster:\n    ibmcloud login -a https://icp-console.apps.green.ocp.csplab.local\n    # initialize the event streams CLI plugin\n    ibmcloud es init\n    # Create the Kafka topics for Kafka connect\n    ibmcloud es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compact\n    ibmcloud es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compact\n    ibmcloud es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact\n    # Create the topic for the scenarios\n    ibmcloud es topic-create inventory\n    ibmcloud es topic-create items\n    ibmcloud es topics\n    ```\n1. Create API KEY with a manager-level access.\n1. Clone the lab repository: `git clone https://github.com/jbcodeforce/eda-kconnect-lab && cd eda-kconnect-lab`.\n1. Prepare the script to set the environment variables used by all the components of the solution, like the Kafka broker URLs and APIKEy.\n\n    * First rename the `scripts/setenv-TMP.sh` to `scripts/setenv.sh`\n    * Then modify the KAFKA_BROKERS and KAFKA_APIKEY with the respecting values as defined in the Event Streams credentials.\n\n  ```json\n  {\n    \"api_key\": \"bA ... Qp\",\n    \"apikey\": \"bA ... Qp\",\n    \"iam_apikey_description\": \"Auto-generated for key 4d ... c6\",\n    \"iam_apikey_name\": \"es-mgr-creds\",\n    \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n    \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/db ... f2::serviceid:ServiceId-7a ... 6d\",\n    \"instance_id\": \"29 ... 15\",\n    \"kafka_admin_url\": \"https://70 ... 1g.svc01.us-east.eventstreams.cloud.ibm.com\",\n    \"kafka_brokers_sasl\": [\n      \"broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\"\n    ],\n    \"kafka_http_url\": \"https://70 ... 1g.svc01.us-east.eventstreams.cloud.ibm.com\",\n    \"password\": \"bA ... Qp\",\n    \"user\": \"token\"\n  }\n  ```\n\n1. Prepare the Kafka Connect environment, as we need to use three connectors. Therefore we need to clone the source, build and get the jars file in the connectors. In fact we have developed scripts to automate those tedious steps:\n\n  * Under the `kconnect` folder run `./setupConnectors.sh` script and get the three connectors downloaded and built.\n  * Build a docker image for the connector: this is also done by running a second script: `./createOrStartKconnect.sh build`.\n\n<InlineNotification kind=\"info\">You need to decide what 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a premise cluster. We propose to run the components with docker compose for the scenario 1,2,3 and do a deployment of the components on premise as a service on scenario 4.\n</InlineNotification>\n\n## Scenario 1: Event Streams on Cloud to MQ on premise via MQ connector sink\n\nThis scenario uses the [IBM Kafka Connect sink connector for IBM MQ](https://github.com/ibm-messaging/kafka-connect-mq-sink) to pull streaming data into a local MQ queue.  In this example we are using IBM Event Streams on IBM Cloud as the Kafka data source and a dockerized instance of MQ as the destination. We could have used MQ broker as part of Cloud Pak for integration or [as a service in IBM Cloud](https://cloud.ibm.com/docs/mqcloud/index.html).\n\n### Pre-requisites\n\nWe assume that you have an instance of Event Streams already running on IBM Cloud with at least on manager-level credentials created.  The credentials will come in the form of a JSON document as seen in the previous section.\nYou will need the `kafka_brokers_sasl` and `password` atribute to configure the sink connector.\n\nThis scenario uses the `inventory` topic created in the Scenario Setup in previous section.\n\n### Create Local IBM MQ Instance\n\nHere we will use Docker to create a local MQ instance.  First create a data directory to mount in the container.\n\n`mkdir qm1data`\n\nThen create the container.\n\n```shell\ndocker run                     \\\n  --name mq                    \\\n  --detach                     \\\n  --publish 1414:1414          \\\n  --publish 9443:9443          \\\n  --publish 9157:9157          \\\n  --volume qm1data:/mnt/mqm    \\\n  --env LICENSE=accept         \\\n  --env MQ_QMGR_NAME=QM1       \\\n  --env MQ_APP_PASSWORD=admin  \\\n  --env MQ_ENABLE_METRICS=true \\\n  ibmcom/mq\n```\n\nYou should be able to log into the MQ server on port 9443 with default user `admin` and password `passw0rd`.\n\nConnect to the running MQ instance to create a Channel and Queue as described on the [Using IBM MQ with Kafka Connect](https://github.com/ibm-messaging/kafka-connect-mq-sink/blob/master/UsingMQwithKafkaConnect.md) page.\n\n```shell\ndocker exec -ti mq bash\nstrmqm QM1\nrunmqsc QM1\nDEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\nALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\nREFRESH SECURITY TYPE(CONNAUTH)\nDEFINE QLOCAL(INVENTORY)\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\nSET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\nEND\n```\n\nExit the session and continue on to create the MQ Connector Sink.\n\n### Create MQ Kafka Connector Sink\n\nThe MQ Connector Sink can be downloaded from [Github](https://github.com/ibm-messaging/kafka-connect-mq-sink).  The Github site includes exhaustive instructions and an abridged version follows.\n\nClone the repository with the following command:\n\n`git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git`\n\nChange directory into the kafka-connect-mq-sink directory:\n\n`cd kafka-connect-mq-sink`\n\nBuild the connector using Maven:\n\n`mvn clean package`\n\nNext, create a directory to contain the Kafka Connector configuration.\n\n`mkdir config && cd config`\n\nCreate a configuration file called `connect-distributed.properties` for Kafka Connect based on the template below.\n\n```properties\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Producer Side\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\nproducer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. T\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\nstatus.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\nSave this file in the `config` directory.\n\nNext, create a log4j configuration file named `connect-log4j.properties` based on the template below.\n\n```properties\nlog4j.rootLogger=DEBUG, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n\nlog4j.logger.org.apache.kafka=INFO\n```\n\nSave this file to the `config` directory as well.\n\nFinally, create a JSON configuraiton file for the MQ sink.  This can be stored anywhere but it can be conveniently created in the `config` directory.  We name this file `mq-sink.json`.\n\n```json\n{\n    \"name\": \"mq-sink\",\n    \"config\":\n    {\n        \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n        \"tasks.max\": \"1\",\n        \"topics\": \"inventory\",\n\n        \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n        \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n        \"mq.queue.manager\": \"QM1\",\n        \"mq.connection.name.list\": \"mq(1414)\",\n        \"mq.user.name\": \"admin\",\n        \"mq.password\": \"passw0rd\",\n        \"mq.user.authentication.mqcsp\": true,\n        \"mq.channel.name\": \"KAFKA.CHANNEL\",\n        \"mq.queue\": \"INVENTORY\",\n        \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n    }\n}\n```\n\nBack out one directory to the `kafka-connect-mq-sink` directory.\n\n`cd ..`\n\nBuild docker image\n`docker build -t kafkaconnect-with-mq-sink:1.3.0 .`\n\nFinally, run the Kafka Connect MQ Sink container.\n\n```\ndocker run                                 \\\n  --name mq-sink                           \\\n  --detach                                 \\\n  --volume $(pwd)/config:/opt/kafka/config \\\n  --publish 8083:8083                      \\\n  --link mq:mq                             \\\n  kafkaconnect-with-mq-sink:1.3.0\n```\n\nYou should now have a working MQ sink.\n\nAs an alternate approach, when you have a Kafka Connect isntance up and running, with the dependant jar files, it is possible to configure the connector with a POST operation like:\n\n```Shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./mq-sink.json\"\n\n# The response returns the metadata about the connector\n{\"name\":\"mq-sink\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"ibmmq(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink\"},\"tasks\":[{\"connector\":\"mq-sink\",\"task\":0}],\"type\":\"sink\"}\n```\n\nOnce the connector is up and running, we can use some tool to send inventory message. In the `integration-tests` folder we have some python code to produce message. If you have a python environment with kafka api you can use yours, or we have also provided a Dockerfile to prepare a local python environment, which will not impact yours.\n\n```shell\n# if you change the name of the image\ndocker build -t ibmcase/python37 .\n# ... then update the script ./startPython.sh\n./startPython.sh\n# Now in the new bash session you should see ProduceInventoryEvent.py,... start it by sending 2 events\npython ProduceInventoryEvent.py --size 2\n# Events are random but use stores and items known by the database downstream.\n sending -> {'storeName': 'NYC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 163, 'id': 1, 'timestamp': '23-Jun-2020 04:32:38'}\n# the following trace demonstrates Kafka received the message\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'SC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 178, 'id': 2, 'timestamp': '23-Jun-2020 04:32:38'}\n[KafkaProducer] - Message delivered to inventory [0]\n```\n\nIn the Kafka Connect trace we can see:\n\n```shell\nkconnect_1  | [2020-06-23 04:23:16,270] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 26: {inventory-0=OffsetAndMetadata{offset=44, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\nkconnect_1  | [2020-06-23 04:32:46,382] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 83: {inventory-0=OffsetAndMetadata{offset=48, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\n```\n\nAnd in the IBM MQ Console, under the Local Queue: Inventory we can see the messages:\n\n![](./images/ibmq-q-inventory.png)\n\nTo remove the connector do the following command. Do this specially if you go to scenario 2 next.\n\n```shell\ncurl -X DELETE http://localhost:8083/connectors/mq-sink\n```\n\n## Scenario 2: Deploying Kafka Connector MQ Sink to OpenShift\n\n### Prerequisites\n\nWe are assuming you already have an instance of IBM EventStreams running on IBM Cloud from previous scenarios.  Also, we assume you have a running instance of OpenShift with a project created to run the MQ Sink.  Finally, we assume you're familia with OpenShift and Kubernetes and will know how to work with the configuration files provided below.\n\n### MQ on OpenShift\n\nStrictly speaking you don't need to move the instance of MQ previously used onto OpenShift for the MQ Sink to work however the configuration to do so is provided.  Note that this is not a production configuration and is intended for POC purposes only.\n\nCreate a ConfigMap on OpenShift with the following definition:\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-config\n  namespace: mq-demo\ndata:\n    LICENSE: accept\n    MQ_QMGR_NAME: QM1\n    MQ_APP_PASSWORD: admin\n    MQ_ENABLE_METRICS: \"true\"\n```\n\nThis will make it easier to update the MQ configuration if needed without editing everything in the Pod definition.\n\nNext, create the MQ Pod with the following definition:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ibm-mq\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: ibm-mq\n      image: ibmcom/mq\n      ports:\n        - containerPort: 1414\n          protocol: TCP\n        - containerPort: 9443\n          protocol: TCP\n        - containerPort: 9157\n          protocol: TCP\n      envFrom:\n        - configMapRef:\n            name: mq-config\n```\n\nNext, define a Service to point to the MQ Pod.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mq-service\n  namespace: mq-demo\nspec:\n  selector:\n    app: mq-kafka-sink-demo-app\n  ports:\n    - name: mq-port\n      protocol: TCP\n      port: 1414\n      targetPort: 1414\n    - name: mq-portal\n      protocol: TCP\n      port: 9443\n      targetPort: 9443\n    - name: mq-dunno\n      protocol: TCP\n      port: 9157\n      targetPort: 9157\n```\n\nFinally, define a Route to be able to access the admin UI.\n\n```yaml\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: mq-route\n  namespace: mq-demo\nspec:\n  host: ibmmq.bnpp.apps.openshift.proxmox.lab\n  to:\n    kind: Service\n    name: mq-service\n    weight: 100\n  port:\n    targetPort: mq-portal\n  tls:\n    termination: passthrough\n    insecureEdgeTerminationPolicy: Redirect\n  wildcardPolicy: None\n```\n\nYou will want to connect to the container and run the setup commands as described in a previous scenario.  At this point, IBM MQ should be running and available on OpenShift.\n\nTo run the Kafka Connect MQ Sink on OpenShift or any container platform, you will need to build a container that has Kafka installed as well as the MQ Sink and proper configuration.  Typically we would use the Strimzi containerized Kafka solution to run on OpenShift, but in this case to illustrate all the components we are building a container from scratch.  The following is the Dockerfile:\n\n```Dockerfile\nFROM ubuntu:20.04\n\nADD https://mirrors.koehn.com/apache/kafka/2.5.0/kafka_2.12-2.5.0.tgz /tmp/\n\nRUN apt update                                                                                                     && \\\n    apt install -y curl git maven                                                                                  && \\\n    tar -C /opt -xvf /tmp/kafka_2.12-2.5.0.tgz                                                                     && \\\n    rm -f /tmp/kafka_2.12-2.5.0.tgz                                                                                && \\\n    ln -s /opt/kafka_2.12-2.5.0 /opt/kafka                                                                         && \\\n    mv -f /opt/kafka/config/connect-distributed.properties /opt/kafka/config/connect-distributed.properties.bak    && \\\n    cd /opt                                                                                                        && \\\n    git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git                                           && \\\n    cd /opt/kafka-connect-mq-sink                                                                                  && \\\n    mvn clean package                                                                                              && \\\n    ln -s /opt/kafka-connect-mq-sink/target/kafka-connect-mq-sink-1.3.0-jar-with-dependencies.jar /opt/kafka/libs/ && \\\n    mv -f /opt/kafka-connect-mq-sink/config/mq-sink.json /opt/kafka-connect-mq-sink/config/mq-sink.json.bak\n\nCOPY connect-distributed.properties /opt/kafka/config/connect-distributed.properties\nCOPY mq-sink-connector-config.json /opt/kafka-connect-mq-sink/config/mq-sink.json\nCOPY entrypoint.sh /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\nWe start with a vanilla Linux container, install the binary distribution of Kafka for Linux, clone the MQ Sink repository from Github, build the MQ Sink, and finally copy in some template files.  Finally the container runs a custom entrypoint script as shown below:\n\n```shell\n#!/bin/sh\n\nset -x\n\nsed -i \"s/KAFKA_BOOTSTRAP_SERVERS/${KAFKA_BOOTSTRAP_SERVERS}/g\" /opt/kafka/config/connect-distributed.properties\nsed -i \"s/KAFKA_API_KEY/${KAFKA_API_KEY}/g\"                     /opt/kafka/config/connect-distributed.properties\n\n\nsed -i \"s/KAFKA_TOPICS/${KAFKA_TOPICS}/g\"         /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE_MANAGER/${MQ_QUEUE_MANAGER}/g\" /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_HOST/${MQ_HOST}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PORT/${MQ_PORT}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_USER/${MQ_USER}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PASSWORD/${MQ_PASSWORD}/g\"           /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_CHANNEL/${MQ_CHANNEL}/g\"             /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE/${MQ_QUEUE}/g\"                 /opt/kafka-connect-mq-sink/config/mq-sink.json\n\n/opt/kafka/bin/connect-distributed.sh /opt/kafka/config/connect-distributed.properties &\n\nsleep 60\ncurl -X DELETE -H \"Content-Type: application/json\" http://localhost:8083/connectors/mq-sink-connector\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@/opt/kafka-connect-mq-sink/config/mq-sink.json\"\n\ntail -f /dev/null\n```\n\nAgain this is not a production-ready entrypoint container script; it's intended for POC purposes.  The script updates the template files copied into the container with values from the environment (either `--env` using Docker, or from a `ConfigMap` in OpenShift) and then starts Kafka Connect in distributed mode.  It pauses the script for 1 minute to let Kafka Connect start, then finally activates the MQ Sink by POSTing the MQ Sink configuration using `cURL`.\n\nThe two template configuration files that are copied into the container are shown below.\n\n### connect-distributed.properties:\n\n```shell\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Producer Side\nproducer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,\n# and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\n\n#status.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\n### mq-sink-connector-config.json:\n\n```json\n{\n  \"name\": \"mq-sink-connector\",\n  \"config\":\n  {\n      \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n      \"tasks.max\": \"1\",\n      \"topics\": \"KAFKA_TOPICS\",\n\n      \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n      \"mq.queue.manager\": \"MQ_QUEUE_MANAGER\",\n      \"mq.connection.name.list\": \"MQ_HOST(MQ_PORT)\",\n      \"mq.user.name\": \"MQ_USER\",\n      \"mq.password\": \"MQ_PASSWORD\",\n      \"mq.user.authentication.mqcsp\": true,\n      \"mq.channel.name\": \"MQ_CHANNEL\",\n      \"mq.queue\": \"MQ_QUEUE\",\n      \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n  }\n}\n```\n\nNotice in these two files there are several capitalized variables which are replaced by the entrypoint.sh script at container startup.\n\nTo deploy the container on OpenShift, we create a `ConfigMap` with information about the IBM EventStreams on IBM Cloud instance as well as the local instance of MQ on OpenShift.\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-kafka-sink-demo-config\n  namespace: mq-demo\ndata:\n  KAFKA_API_KEY: bA ... Qp\n  KAFKA_BOOTSTRAP_SERVERS: >-\n    broker-1- ... eventstreams.cloud.ibm.com:9093,broker-0- ... eventstreams.cloud.ibm.com:9093,broker-4- ... eventstreams.cloud.ibm.com:9093,broker-2- ... eventstreams.cloud.ibm.com:9093,broker-5- ... eventstreams.cloud.ibm.com:9093,broker-3- ... eventstreams.cloud.ibm.com:9093\n  KAFKA_TOPICS: inventory\n  MQ_HOST: mq-service\n  MQ_PORT: \"1414\"\n  MQ_USER: admin\n  MQ_QUEUE_MANAGER: QM1\n  MQ_PASSWORD: passw0rd\n  MQ_CHANNEL: KAFKA.CHANNEL\n  MQ_QUEUE: INVENTORY\n```\n\nFinally to deploy the MQ Sink container, we create a Pod definition on OpenShift:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mq-kafka-sink-demo\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: mq-kafka-sink-demo\n      image: registry/mq-kafka-sink-demo:0.0.1\n      envFrom:\n        - configMapRef:\n            name: mq-kafka-sink-demo-config\n```\n\nWith the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured.  You will see signs of success in the container output (via oc logs, or in the UI):\n\n```shell\n+ curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json\n...\n{\"name\":\"mq-sink-connector\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"mq-service(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink-connector\"},\"tasks\":[{\"connector\":\"mq-sink-connector\",\"task\":0}],\"type\":\"sink\"}\n...\n[2020-06-23 04:26:26,054] INFO Creating task mq-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:419)\n...[2020-06-23 04:26:26,449] INFO Connection to MQ established (com.ibm.eventstreams.connect.mqsink.JMSWriter:229)\n[2020-06-23 04:26:26,449] INFO WorkerSinkTask{id=mq-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)\n```\n\nYou should now have the Kafka Connector MQ Sink running on OpenShift.\n\n## Scenario 3: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source\n\nIn this scenario we are using the [IBM messaging github: source connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source). The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe [lab repository](https://github.com/jbcodeforce/eda-kconnect-lab) includes a docker compose file (`RabbitMQ-Kconnect-compose.yaml`) under the `infrastructure` folder to run Rabbit MQ and Kconnect together on your local computer.\n\nIn scenario 1, we were using the official IBM MQ Connector docker image, but in this scenario we have to define a Kafka connect image that includes the RabbitMQ and JDBC connectors. Which means we needs to get the different connector jar files, and build a docker image. Those steps were described in [previous section](#scenario-setup).\n\n### Pre-requisites\n\nTo send message to RabbitMQ, we have implemented a simple simulator, to send item sale messages for one of the stores. The code is under `store-sale-producer` folder and we have also uploaded the image into dockerhub. If you need to rebuild the image the following commands may be done:\n\n```shell\n./mvnw clean package -Dquarkus.container-image.build=true -Dquarkus.container-image.group=ibmcase -Dquarkus.container-image.tag=1.0.0\n```\n\n### Start the backend environment\n\nIn this section we start RabbitMQ, Kafka Connect and the Store Item Sale generator app. This app exposes a REST api to generate items sale operations that happen in a predefined set of stores.\n\nUnder the infrastructure folder use the command: `docker-compose -f RabbitMQ-Kconnect-compose.yaml up`.\n\nThe trace includes RabbitMQ, storeSaleGenerator_1 and Kafka connect logs. Here is a small extract of important messages:\n\n```shell\nrabbitmq_1            | 2020-06-17 06:12:58.293 [info] <0.9.0> Server startup complete; 4 plugins started.\nrabbitmq_1            |  * rabbitmq_management\nrabbitmq_1            |  * rabbitmq_web_dispatch\nrabbitmq_1            |  * rabbitmq_management_agent\nrabbitmq_1            |  * rabbitmq_amqp1_0\nrabbitmq_1            |  completed with 4 plugins.\n....\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,056 INFO  [io.quarkus] (main) Profile prod activated.\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,057 INFO  [io.quarkus] (main) Installed features: [cdi, mutiny, resteasy, resteasy-jsonb, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-amqp, vertx]\n```\n\n### Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:1567/](http://localhost:1567/) using the guest/guest login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nIf in the Admin tab you do not see **rabbit-user** listed do the following:\n\n* Add a new admin user: **rabbit-user/rabbit-pass** using the Admin tab. Enable the virtual host to be '/'.\n\n![](./images/rabbitmq-user.png)\n\n\nGo to the Queue tab and add `items` queue with default parameters:\n\n![8](./images/rabbitmq-item-queue.png)\n\n\nWith the following result\n\n![9](./images/rabbitmq-item-queue-2.png)\n\n\n### Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n### Generate sale messages\n\nThe Store application has an OpenAPI [http://localhost:8080/swagger-ui/](http://localhost:8080/swagger-ui/) definition to send messages:  `/stores/start/2` api:\n\n![5](./images/store-app-1.png)\n\nOr use `curl -X POST http://localhost:8080/stores/start/2`\n\nIn the trace you should see something like:\n\n```shell\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":0,\"storeName\":\"SF02\",\"itemCode\":\"IT07\",\"quantity\":7,\"price\":46.79320631709398}\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":1,\"storeName\":\"NYC01\",\"itemCode\":\"IT00\",\"quantity\":7,\"price\":0.7764381649099172}\n```\n### Verify messages arrived in Kafka items topic\n\nWe can use the Kafdrop tool to go to the `items` topic as illustrated below. The tool can be started via the `./startKafdrop.sh` command under the `infrastructure` folder after setting a `kakfa.properties` file as:\n\n```properties\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nssl.enabled.protocols=TLSv1.2\nssl.endpoint.identification.algorithm=HTTPS\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"APIKEY\";\n```\n\n![](./images/kafdrop.png)\n\n\n## Scenario 4: Event Streams on Cloud to DB2 on premise via JDBC Sink connector\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from the `inventory topic` and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n### Pre-requisites\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for cRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) under the `inventory-app` folder of [this repository](https://github.com/jbcodeforce/eda-kconnect-lab). At the application starts stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/import.sql) from `inventory-app/src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n\n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n### Run the Kafka Connector in distributed mode\n\nThe docker image built in the [setup](#scenario-setup) has the configuration for kafka connect distributed cluster, we need in this scenario to start the connector and upload the DB2 Sink connector definition. To start it, run the script `./createOrStartKconnect.sh start` under `kconnect` folder.\n\n### Upload the DB2 sink definition\n\nRename the file `db2-sink-config-TMPL.json` as `db2-sink-config.json` and modify the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format`.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh url-kafka-connect` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhodt:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n### Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```\nProduce to the topic inventory\n[KafkaProducer] - This is the configuration for the producer:\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProducerInventory', 'acks': 0, 'request.timeout.ms': 10000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5'}\nsending -> {'storeName': 'LA02', 'itemCode': 'IT09', 'id': 0, 'timestamp': 1591211295.617515}\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'PT02', 'itemCode': 'IT00', 'id': 1, 'timestamp': 1591211296.7727954}\n[KafkaProducer] - Message delivered to inventory [0]\n\n```\n\n### Verify records are uploaded into the Inventory database\n\nUsing the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n\n## Scenario 5: Run the solution components end to end on Kubernetes\n\nThis solution covers all the components of the data pipeline. It still uses DB2 and Event Streams on Cloud but deploy all the other component in OpenShift as part of IBM Kubernetes Service.\n\n### Pre-requisites\n\nCreate the following services in IBM Cloud:\n\n* [DB2 instance](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started).\n* [IBM Kubernetes Service](https://cloud.ibm.com/docs/containers?topic=containers-cs_cluster_tutorial#cs_cluster_tutorial).\n\n### Deployment\n\nTo be finished !\n\n1. Verify the Store Sale Simulator runs\n1. Verify the connectors `http://localhost:8083/connectors`\n","type":"Mdx","contentDigest":"19652aa172bd0d38c8f248d290b370bd","counter":568,"owner":"gatsby-plugin-mdx"},"exports":[],"rawBody":"---\ntitle: Kafka Connect with Event Streams on Cloud\ndescription: A set of labs and reference for working with Kafka Connect with Event streams on cloud\n---\n  \n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Solution anatomy</AnchorLink>\n  <AnchorLink>General pre-requisites</AnchorLink>\n  <AnchorLink>Scenario setup</AnchorLink>\n  <AnchorLink>Scenario 1: Event Streams on Cloud to MQ on premise via MQ connector sink</AnchorLink>\n  <AnchorLink>Scenario 2: Deploying Kafka Connector MQ Sink to OpenShift</AnchorLink>\n  <AnchorLink>Scenario 3: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source</AnchorLink>\n  <AnchorLink>Scenario 4: Event Streams on Cloud to DB2 on premise via JDBC Sink connector</AnchorLink>\n  <AnchorLink>Scenario 5: Run the solution components end to end on Kubernetes</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis lab will address multiple scenarios that aim to build an end to end data pipeline, as depicted by the following figure, using Event Streams on Cloud. At the high level Kafka connect is used to integrate external systems into the Kafka ecosystem. For example external system can inject message to queue manager, from which a first Kafka source connector will get the message to a Kafka topic, which then will be processed by a series of event driven microservices down to a final topic, that will be use by Sink connectors.\n\n![1](./images/kconnect-overview.png)\n\nTo support this lab we are reusing a classical business use case where stores are sending their transactions to a central messaging platform, based on queues, and with the adoption of loosely coupled microservice, real time analytics and complex event processing, Kafka is added to the legacy environment. Adopting Kafka connect lets integrate with existing applications without any changes.\n\n## Solution anatomy\n\nThe lab is divided into scenarios that can be combined to support the real time inventory data pipeline as illustrated in the figure below:\n\n![2](./images/kconnect-scenario-components.png)\n\n1. The store application, is a Quarkus based app, generating item sales to RabbitMQ `items` queue. The code of this application is under the `store-sale-producer` folder, in the [lab repository](https://github.com/jbcodeforce/eda-kconnect-lab/). We will address how to get this code in the pre-requisite section.\n\n    * RabbitMQ runs in docker image started locally via docker compose. The messages are in the `items` queue.\n    * The lab, focusing on the injection to Kafka, is documented in the [scenario 2](#scenario-2:-rabbitmq-on-premise-to-event-streams-on-cloud-via-rabbitmq-connector-source).\n    * [A] The Sink connector description is in the [kconnect](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/kconnect) folder.\n    * Kafka backbone is Event Streams on Cloud.\n1. The inventory MS is a Kafka Stream application, done with Reactive Messaging and Kafka Stream API. The folder is\nFor example the scenario illustrate JDBC Sink connector to save to existing data base.\n1. The mock up Inventory mainframe application is not implemented and we will use the MQ tools to view the message in the `inventory` queue\n\n    * The MQ Sink connector [B] configuration is defined in the [kconnect](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/kconnect) folder.\n    * MQ broker runs in docker container started with docker-compose\n    * The lab scenario is [the number 1](#scenario-1:-event-streams-on-cloud-to-mq-on-premise-via-mq-connector-sink)\n1. The Inventory Application, using DB2 as datasource is a quarkus app using hibernate with panache, defined in the [nventory-app](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) folder\n\n    * The JDBC Sink connector [C] configuration is defined in the [kconnect](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/kconnect) folder.\n    * The [scenario 3](#scenario-3:-event-streams-on-cloud-to-db2-on-premise-via-jdbc-sink-connector) lab goes over how the Kafka Connect JDBC sink works.\n1. The [scenario 4](#scenario-4:-run-the-solution-components-end-to-end-on-kubernetes) addresses the end to end solution, which is basically an end to end demonstration of a simple data pipeline for a real time view of an inventory solution.\n\n<InlineNotification kind=\"info\">You need to decide what your 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a premise cluster. We propose to run the components with docker compose for the scenario 1,2,3 and scenario 4 is for a deployment of the components on a kubernetes cluster that could run on premise or on any cloud provider using Openshift. If you do not want to build all the components, we have each of them available in docker hub and the docker compose file should run them automatically.\n</InlineNotification>\n\n## General pre-requisites\n\nWe need the following IBM Cloud services created and tools to run the lab. We try to use docker images as much as possible to do not impact your local laptop.\n\n* [Docker](https://docs.docker.com/) and docker compose to run the solution locally.\n* Event Streams instance, may be reuse the one created in [this lab](/technology/event-streams/es-cloud/).\n* [IBM Cloud CLI](https://cloud.ibm.com/docs/cli?topic=cli-getting-started).\n* If you want to run locally you need to get [docker compose](https://docs.docker.com/compose/).\n* [git CLI](https://git-scm.com/downloads).\n* [Maven](https://maven.apache.org/install.html).\n\nFor the on-premise environment, we will not use VMs, but simply run some of the components on IBM premise Service platform or Openshift. The point is that the workload is packaged as container images and can run anywhere.\n\n## Scenario setup\n\n1. Login to the cloud via CLI: `ibmcloud login`\n1. Initialize the Event Streams CLI and select the target Event Streams cluster: `ibmcloud es init`\n1. Define connect topics: When running in distributed mode, the connectors need three topics as presented in the `create topics` table [here](https://ibm.github.io/event-streams/connecting/setting-up-connectors/).\n\n    * **connect-configs**: This topic will store the connector and task configurations.\n    * **connect-offsets**: This topic is used to store offsets for Kafka Connect.\n    * **connect-status**: This topic will store status updates of connectors and tasks.\n\n    Using IBM Event Streams CLI, the topics are created via the commands like:\n\n    ```shell\n    # log to the kubernetes cluster:\n    ibmcloud login -a https://icp-console.apps.green.ocp.csplab.local\n    # initialize the event streams CLI plugin\n    ibmcloud es init\n    # Create the Kafka topics for Kafka connect\n    ibmcloud es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compact\n    ibmcloud es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compact\n    ibmcloud es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact\n    # Create the topic for the scenarios\n    ibmcloud es topic-create inventory\n    ibmcloud es topic-create items\n    ibmcloud es topics\n    ```\n1. Create API KEY with a manager-level access.\n1. Clone the lab repository: `git clone https://github.com/jbcodeforce/eda-kconnect-lab && cd eda-kconnect-lab`.\n1. Prepare the script to set the environment variables used by all the components of the solution, like the Kafka broker URLs and APIKEy.\n\n    * First rename the `scripts/setenv-TMP.sh` to `scripts/setenv.sh`\n    * Then modify the KAFKA_BROKERS and KAFKA_APIKEY with the respecting values as defined in the Event Streams credentials.\n\n  ```json\n  {\n    \"api_key\": \"bA ... Qp\",\n    \"apikey\": \"bA ... Qp\",\n    \"iam_apikey_description\": \"Auto-generated for key 4d ... c6\",\n    \"iam_apikey_name\": \"es-mgr-creds\",\n    \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n    \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/db ... f2::serviceid:ServiceId-7a ... 6d\",\n    \"instance_id\": \"29 ... 15\",\n    \"kafka_admin_url\": \"https://70 ... 1g.svc01.us-east.eventstreams.cloud.ibm.com\",\n    \"kafka_brokers_sasl\": [\n      \"broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\",\n      \"broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\"\n    ],\n    \"kafka_http_url\": \"https://70 ... 1g.svc01.us-east.eventstreams.cloud.ibm.com\",\n    \"password\": \"bA ... Qp\",\n    \"user\": \"token\"\n  }\n  ```\n\n1. Prepare the Kafka Connect environment, as we need to use three connectors. Therefore we need to clone the source, build and get the jars file in the connectors. In fact we have developed scripts to automate those tedious steps:\n\n  * Under the `kconnect` folder run `./setupConnectors.sh` script and get the three connectors downloaded and built.\n  * Build a docker image for the connector: this is also done by running a second script: `./createOrStartKconnect.sh build`.\n\n<InlineNotification kind=\"info\">You need to decide what 'on-premise' environment is for this lab. You can run with docker compose, or deploy on a premise cluster. We propose to run the components with docker compose for the scenario 1,2,3 and do a deployment of the components on premise as a service on scenario 4.\n</InlineNotification>\n\n## Scenario 1: Event Streams on Cloud to MQ on premise via MQ connector sink\n\nThis scenario uses the [IBM Kafka Connect sink connector for IBM MQ](https://github.com/ibm-messaging/kafka-connect-mq-sink) to pull streaming data into a local MQ queue.  In this example we are using IBM Event Streams on IBM Cloud as the Kafka data source and a dockerized instance of MQ as the destination. We could have used MQ broker as part of Cloud Pak for integration or [as a service in IBM Cloud](https://cloud.ibm.com/docs/mqcloud/index.html).\n\n### Pre-requisites\n\nWe assume that you have an instance of Event Streams already running on IBM Cloud with at least on manager-level credentials created.  The credentials will come in the form of a JSON document as seen in the previous section.\nYou will need the `kafka_brokers_sasl` and `password` atribute to configure the sink connector.\n\nThis scenario uses the `inventory` topic created in the Scenario Setup in previous section.\n\n### Create Local IBM MQ Instance\n\nHere we will use Docker to create a local MQ instance.  First create a data directory to mount in the container.\n\n`mkdir qm1data`\n\nThen create the container.\n\n```shell\ndocker run                     \\\n  --name mq                    \\\n  --detach                     \\\n  --publish 1414:1414          \\\n  --publish 9443:9443          \\\n  --publish 9157:9157          \\\n  --volume qm1data:/mnt/mqm    \\\n  --env LICENSE=accept         \\\n  --env MQ_QMGR_NAME=QM1       \\\n  --env MQ_APP_PASSWORD=admin  \\\n  --env MQ_ENABLE_METRICS=true \\\n  ibmcom/mq\n```\n\nYou should be able to log into the MQ server on port 9443 with default user `admin` and password `passw0rd`.\n\nConnect to the running MQ instance to create a Channel and Queue as described on the [Using IBM MQ with Kafka Connect](https://github.com/ibm-messaging/kafka-connect-mq-sink/blob/master/UsingMQwithKafkaConnect.md) page.\n\n```shell\ndocker exec -ti mq bash\nstrmqm QM1\nrunmqsc QM1\nDEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\nALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\nREFRESH SECURITY TYPE(CONNAUTH)\nDEFINE QLOCAL(INVENTORY)\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\nSET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\nEND\n```\n\nExit the session and continue on to create the MQ Connector Sink.\n\n### Create MQ Kafka Connector Sink\n\nThe MQ Connector Sink can be downloaded from [Github](https://github.com/ibm-messaging/kafka-connect-mq-sink).  The Github site includes exhaustive instructions and an abridged version follows.\n\nClone the repository with the following command:\n\n`git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git`\n\nChange directory into the kafka-connect-mq-sink directory:\n\n`cd kafka-connect-mq-sink`\n\nBuild the connector using Maven:\n\n`mvn clean package`\n\nNext, create a directory to contain the Kafka Connector configuration.\n\n`mkdir config && cd config`\n\nCreate a configuration file called `connect-distributed.properties` for Kafka Connect based on the template below.\n\n```properties\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\n\n# Producer Side\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"bA ... Qp\";\nproducer.bootstrap.servers=broker-1- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3- ... kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\n\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. T\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\nstatus.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\nSave this file in the `config` directory.\n\nNext, create a log4j configuration file named `connect-log4j.properties` based on the template below.\n\n```properties\nlog4j.rootLogger=DEBUG, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n\nlog4j.logger.org.apache.kafka=INFO\n```\n\nSave this file to the `config` directory as well.\n\nFinally, create a JSON configuraiton file for the MQ sink.  This can be stored anywhere but it can be conveniently created in the `config` directory.  We name this file `mq-sink.json`.\n\n```json\n{\n    \"name\": \"mq-sink\",\n    \"config\":\n    {\n        \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n        \"tasks.max\": \"1\",\n        \"topics\": \"inventory\",\n\n        \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n        \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n        \"mq.queue.manager\": \"QM1\",\n        \"mq.connection.name.list\": \"mq(1414)\",\n        \"mq.user.name\": \"admin\",\n        \"mq.password\": \"passw0rd\",\n        \"mq.user.authentication.mqcsp\": true,\n        \"mq.channel.name\": \"KAFKA.CHANNEL\",\n        \"mq.queue\": \"INVENTORY\",\n        \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n    }\n}\n```\n\nBack out one directory to the `kafka-connect-mq-sink` directory.\n\n`cd ..`\n\nBuild docker image\n`docker build -t kafkaconnect-with-mq-sink:1.3.0 .`\n\nFinally, run the Kafka Connect MQ Sink container.\n\n```\ndocker run                                 \\\n  --name mq-sink                           \\\n  --detach                                 \\\n  --volume $(pwd)/config:/opt/kafka/config \\\n  --publish 8083:8083                      \\\n  --link mq:mq                             \\\n  kafkaconnect-with-mq-sink:1.3.0\n```\n\nYou should now have a working MQ sink.\n\nAs an alternate approach, when you have a Kafka Connect isntance up and running, with the dependant jar files, it is possible to configure the connector with a POST operation like:\n\n```Shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./mq-sink.json\"\n\n# The response returns the metadata about the connector\n{\"name\":\"mq-sink\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"ibmmq(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink\"},\"tasks\":[{\"connector\":\"mq-sink\",\"task\":0}],\"type\":\"sink\"}\n```\n\nOnce the connector is up and running, we can use some tool to send inventory message. In the `integration-tests` folder we have some python code to produce message. If you have a python environment with kafka api you can use yours, or we have also provided a Dockerfile to prepare a local python environment, which will not impact yours.\n\n```shell\n# if you change the name of the image\ndocker build -t ibmcase/python37 .\n# ... then update the script ./startPython.sh\n./startPython.sh\n# Now in the new bash session you should see ProduceInventoryEvent.py,... start it by sending 2 events\npython ProduceInventoryEvent.py --size 2\n# Events are random but use stores and items known by the database downstream.\n sending -> {'storeName': 'NYC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 163, 'id': 1, 'timestamp': '23-Jun-2020 04:32:38'}\n# the following trace demonstrates Kafka received the message\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'SC01', 'itemCode': 'IT06', 'quantity': 15, 'price': 178, 'id': 2, 'timestamp': '23-Jun-2020 04:32:38'}\n[KafkaProducer] - Message delivered to inventory [0]\n```\n\nIn the Kafka Connect trace we can see:\n\n```shell\nkconnect_1  | [2020-06-23 04:23:16,270] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 26: {inventory-0=OffsetAndMetadata{offset=44, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\nkconnect_1  | [2020-06-23 04:32:46,382] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 83: {inventory-0=OffsetAndMetadata{offset=48, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:349)\n```\n\nAnd in the IBM MQ Console, under the Local Queue: Inventory we can see the messages:\n\n![](./images/ibmq-q-inventory.png)\n\nTo remove the connector do the following command. Do this specially if you go to scenario 2 next.\n\n```shell\ncurl -X DELETE http://localhost:8083/connectors/mq-sink\n```\n\n## Scenario 2: Deploying Kafka Connector MQ Sink to OpenShift\n\n### Prerequisites\n\nWe are assuming you already have an instance of IBM EventStreams running on IBM Cloud from previous scenarios.  Also, we assume you have a running instance of OpenShift with a project created to run the MQ Sink.  Finally, we assume you're familia with OpenShift and Kubernetes and will know how to work with the configuration files provided below.\n\n### MQ on OpenShift\n\nStrictly speaking you don't need to move the instance of MQ previously used onto OpenShift for the MQ Sink to work however the configuration to do so is provided.  Note that this is not a production configuration and is intended for POC purposes only.\n\nCreate a ConfigMap on OpenShift with the following definition:\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-config\n  namespace: mq-demo\ndata:\n    LICENSE: accept\n    MQ_QMGR_NAME: QM1\n    MQ_APP_PASSWORD: admin\n    MQ_ENABLE_METRICS: \"true\"\n```\n\nThis will make it easier to update the MQ configuration if needed without editing everything in the Pod definition.\n\nNext, create the MQ Pod with the following definition:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ibm-mq\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: ibm-mq\n      image: ibmcom/mq\n      ports:\n        - containerPort: 1414\n          protocol: TCP\n        - containerPort: 9443\n          protocol: TCP\n        - containerPort: 9157\n          protocol: TCP\n      envFrom:\n        - configMapRef:\n            name: mq-config\n```\n\nNext, define a Service to point to the MQ Pod.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mq-service\n  namespace: mq-demo\nspec:\n  selector:\n    app: mq-kafka-sink-demo-app\n  ports:\n    - name: mq-port\n      protocol: TCP\n      port: 1414\n      targetPort: 1414\n    - name: mq-portal\n      protocol: TCP\n      port: 9443\n      targetPort: 9443\n    - name: mq-dunno\n      protocol: TCP\n      port: 9157\n      targetPort: 9157\n```\n\nFinally, define a Route to be able to access the admin UI.\n\n```yaml\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: mq-route\n  namespace: mq-demo\nspec:\n  host: ibmmq.bnpp.apps.openshift.proxmox.lab\n  to:\n    kind: Service\n    name: mq-service\n    weight: 100\n  port:\n    targetPort: mq-portal\n  tls:\n    termination: passthrough\n    insecureEdgeTerminationPolicy: Redirect\n  wildcardPolicy: None\n```\n\nYou will want to connect to the container and run the setup commands as described in a previous scenario.  At this point, IBM MQ should be running and available on OpenShift.\n\nTo run the Kafka Connect MQ Sink on OpenShift or any container platform, you will need to build a container that has Kafka installed as well as the MQ Sink and proper configuration.  Typically we would use the Strimzi containerized Kafka solution to run on OpenShift, but in this case to illustrate all the components we are building a container from scratch.  The following is the Dockerfile:\n\n```Dockerfile\nFROM ubuntu:20.04\n\nADD https://mirrors.koehn.com/apache/kafka/2.5.0/kafka_2.12-2.5.0.tgz /tmp/\n\nRUN apt update                                                                                                     && \\\n    apt install -y curl git maven                                                                                  && \\\n    tar -C /opt -xvf /tmp/kafka_2.12-2.5.0.tgz                                                                     && \\\n    rm -f /tmp/kafka_2.12-2.5.0.tgz                                                                                && \\\n    ln -s /opt/kafka_2.12-2.5.0 /opt/kafka                                                                         && \\\n    mv -f /opt/kafka/config/connect-distributed.properties /opt/kafka/config/connect-distributed.properties.bak    && \\\n    cd /opt                                                                                                        && \\\n    git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git                                           && \\\n    cd /opt/kafka-connect-mq-sink                                                                                  && \\\n    mvn clean package                                                                                              && \\\n    ln -s /opt/kafka-connect-mq-sink/target/kafka-connect-mq-sink-1.3.0-jar-with-dependencies.jar /opt/kafka/libs/ && \\\n    mv -f /opt/kafka-connect-mq-sink/config/mq-sink.json /opt/kafka-connect-mq-sink/config/mq-sink.json.bak\n\nCOPY connect-distributed.properties /opt/kafka/config/connect-distributed.properties\nCOPY mq-sink-connector-config.json /opt/kafka-connect-mq-sink/config/mq-sink.json\nCOPY entrypoint.sh /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\nWe start with a vanilla Linux container, install the binary distribution of Kafka for Linux, clone the MQ Sink repository from Github, build the MQ Sink, and finally copy in some template files.  Finally the container runs a custom entrypoint script as shown below:\n\n```shell\n#!/bin/sh\n\nset -x\n\nsed -i \"s/KAFKA_BOOTSTRAP_SERVERS/${KAFKA_BOOTSTRAP_SERVERS}/g\" /opt/kafka/config/connect-distributed.properties\nsed -i \"s/KAFKA_API_KEY/${KAFKA_API_KEY}/g\"                     /opt/kafka/config/connect-distributed.properties\n\n\nsed -i \"s/KAFKA_TOPICS/${KAFKA_TOPICS}/g\"         /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE_MANAGER/${MQ_QUEUE_MANAGER}/g\" /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_HOST/${MQ_HOST}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PORT/${MQ_PORT}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_USER/${MQ_USER}/g\"                   /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_PASSWORD/${MQ_PASSWORD}/g\"           /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_CHANNEL/${MQ_CHANNEL}/g\"             /opt/kafka-connect-mq-sink/config/mq-sink.json\nsed -i \"s/MQ_QUEUE/${MQ_QUEUE}/g\"                 /opt/kafka-connect-mq-sink/config/mq-sink.json\n\n/opt/kafka/bin/connect-distributed.sh /opt/kafka/config/connect-distributed.properties &\n\nsleep 60\ncurl -X DELETE -H \"Content-Type: application/json\" http://localhost:8083/connectors/mq-sink-connector\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors --data \"@/opt/kafka-connect-mq-sink/config/mq-sink.json\"\n\ntail -f /dev/null\n```\n\nAgain this is not a production-ready entrypoint container script; it's intended for POC purposes.  The script updates the template files copied into the container with values from the environment (either `--env` using Docker, or from a `ConfigMap` in OpenShift) and then starts Kafka Connect in distributed mode.  It pauses the script for 1 minute to let Kafka Connect start, then finally activates the MQ Sink by POSTing the MQ Sink configuration using `cURL`.\n\nThe two template configuration files that are copied into the container are shown below.\n\n### connect-distributed.properties:\n\n```shell\n# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nsecurity.protocol=SASL_SSL\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.sasl.mechanism=PLAIN\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\n# Producer Side\nproducer.bootstrap.servers=KAFKA_BOOTSTRAP_SERVERS\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.sasl.mechanism=PLAIN\nproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"KAFKA_API_KEY\";\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\noffset.storage.topic=connect-offsets\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,\n# and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nconfig.storage.topic=connect-configs\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\n# Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create\n# the topic before starting Kafka Connect if a specific topic configuration is needed.\n# Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.\n# Since this means there must be at least as many brokers as the maximum replication factor used, we'd like to be able\n# to run this example on a single-broker cluster and so here we instead set the replication factor to 1.\nstatus.storage.topic=connect-status\nstatus.storage.replication.factor=3\n\n#status.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n```\n\n### mq-sink-connector-config.json:\n\n```json\n{\n  \"name\": \"mq-sink-connector\",\n  \"config\":\n  {\n      \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n      \"tasks.max\": \"1\",\n      \"topics\": \"KAFKA_TOPICS\",\n\n      \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n      \"mq.queue.manager\": \"MQ_QUEUE_MANAGER\",\n      \"mq.connection.name.list\": \"MQ_HOST(MQ_PORT)\",\n      \"mq.user.name\": \"MQ_USER\",\n      \"mq.password\": \"MQ_PASSWORD\",\n      \"mq.user.authentication.mqcsp\": true,\n      \"mq.channel.name\": \"MQ_CHANNEL\",\n      \"mq.queue\": \"MQ_QUEUE\",\n      \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n  }\n}\n```\n\nNotice in these two files there are several capitalized variables which are replaced by the entrypoint.sh script at container startup.\n\nTo deploy the container on OpenShift, we create a `ConfigMap` with information about the IBM EventStreams on IBM Cloud instance as well as the local instance of MQ on OpenShift.\n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: mq-kafka-sink-demo-config\n  namespace: mq-demo\ndata:\n  KAFKA_API_KEY: bA ... Qp\n  KAFKA_BOOTSTRAP_SERVERS: >-\n    broker-1- ... eventstreams.cloud.ibm.com:9093,broker-0- ... eventstreams.cloud.ibm.com:9093,broker-4- ... eventstreams.cloud.ibm.com:9093,broker-2- ... eventstreams.cloud.ibm.com:9093,broker-5- ... eventstreams.cloud.ibm.com:9093,broker-3- ... eventstreams.cloud.ibm.com:9093\n  KAFKA_TOPICS: inventory\n  MQ_HOST: mq-service\n  MQ_PORT: \"1414\"\n  MQ_USER: admin\n  MQ_QUEUE_MANAGER: QM1\n  MQ_PASSWORD: passw0rd\n  MQ_CHANNEL: KAFKA.CHANNEL\n  MQ_QUEUE: INVENTORY\n```\n\nFinally to deploy the MQ Sink container, we create a Pod definition on OpenShift:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mq-kafka-sink-demo\n  labels:\n    app: mq-kafka-sink-demo-app\n  namespace: mq-demo\nspec:\n  containers:\n    - name: mq-kafka-sink-demo\n      image: registry/mq-kafka-sink-demo:0.0.1\n      envFrom:\n        - configMapRef:\n            name: mq-kafka-sink-demo-config\n```\n\nWith the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured.  You will see signs of success in the container output (via oc logs, or in the UI):\n\n```shell\n+ curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json\n...\n{\"name\":\"mq-sink-connector\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"mq-service(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink-connector\"},\"tasks\":[{\"connector\":\"mq-sink-connector\",\"task\":0}],\"type\":\"sink\"}\n...\n[2020-06-23 04:26:26,054] INFO Creating task mq-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:419)\n...[2020-06-23 04:26:26,449] INFO Connection to MQ established (com.ibm.eventstreams.connect.mqsink.JMSWriter:229)\n[2020-06-23 04:26:26,449] INFO WorkerSinkTask{id=mq-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)\n```\n\nYou should now have the Kafka Connector MQ Sink running on OpenShift.\n\n## Scenario 3: Rabbitmq on premise to Event Streams on Cloud via RabbitMQ connector source\n\nIn this scenario we are using the [IBM messaging github: source connector for RabbitMQ](https://github.com/ibm-messaging/kafka-connect-rabbitmq-source). The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.\n\nThe [lab repository](https://github.com/jbcodeforce/eda-kconnect-lab) includes a docker compose file (`RabbitMQ-Kconnect-compose.yaml`) under the `infrastructure` folder to run Rabbit MQ and Kconnect together on your local computer.\n\nIn scenario 1, we were using the official IBM MQ Connector docker image, but in this scenario we have to define a Kafka connect image that includes the RabbitMQ and JDBC connectors. Which means we needs to get the different connector jar files, and build a docker image. Those steps were described in [previous section](#scenario-setup).\n\n### Pre-requisites\n\nTo send message to RabbitMQ, we have implemented a simple simulator, to send item sale messages for one of the stores. The code is under `store-sale-producer` folder and we have also uploaded the image into dockerhub. If you need to rebuild the image the following commands may be done:\n\n```shell\n./mvnw clean package -Dquarkus.container-image.build=true -Dquarkus.container-image.group=ibmcase -Dquarkus.container-image.tag=1.0.0\n```\n\n### Start the backend environment\n\nIn this section we start RabbitMQ, Kafka Connect and the Store Item Sale generator app. This app exposes a REST api to generate items sale operations that happen in a predefined set of stores.\n\nUnder the infrastructure folder use the command: `docker-compose -f RabbitMQ-Kconnect-compose.yaml up`.\n\nThe trace includes RabbitMQ, storeSaleGenerator_1 and Kafka connect logs. Here is a small extract of important messages:\n\n```shell\nrabbitmq_1            | 2020-06-17 06:12:58.293 [info] <0.9.0> Server startup complete; 4 plugins started.\nrabbitmq_1            |  * rabbitmq_management\nrabbitmq_1            |  * rabbitmq_web_dispatch\nrabbitmq_1            |  * rabbitmq_management_agent\nrabbitmq_1            |  * rabbitmq_amqp1_0\nrabbitmq_1            |  completed with 4 plugins.\n....\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,056 INFO  [io.quarkus] (main) Profile prod activated.\nstoreSaleGenerator_1  | 2020-06-17 06:12:44,057 INFO  [io.quarkus] (main) Installed features: [cdi, mutiny, resteasy, resteasy-jsonb, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-amqp, vertx]\n```\n\n### Verify the RabbitMQ settings\n\nIn a Web Browser go to [http://localhost:1567/](http://localhost:1567/) using the guest/guest login.\n\nYou should reach this console:\n\n![7](./images/rabbitmq-overview.png)\n\nIf in the Admin tab you do not see **rabbit-user** listed do the following:\n\n* Add a new admin user: **rabbit-user/rabbit-pass** using the Admin tab. Enable the virtual host to be '/'.\n\n![](./images/rabbitmq-user.png)\n\n\nGo to the Queue tab and add `items` queue with default parameters:\n\n![8](./images/rabbitmq-item-queue.png)\n\n\nWith the following result\n\n![9](./images/rabbitmq-item-queue-2.png)\n\n\n### Configure the kafka connector for Rabbitmq source\n\nThe `rabbitmq-source.json` define the connector and the RabbitMQ connection parameters:\n\n```json\n{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n```\n\nThis file is uploaded to Kafka Connect via a PORT operation:\n\n```shell\ncurl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n```\n\nTo verify use: `curl -X GET http://localhost:8083/connectors`.\n\nIn Kafka connect trace you should see:\n\n```shell\n[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n\n```\n\nAnd Rabbitmq that get the connection from Kafka Connect.\n\n```shell\nrabbitmq_1  [info] <0.1766.0> accepting AMQP connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection <0.1766.0> (172.19.0.3:33040 -> 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n```\n\n### Generate sale messages\n\nThe Store application has an OpenAPI [http://localhost:8080/swagger-ui/](http://localhost:8080/swagger-ui/) definition to send messages:  `/stores/start/2` api:\n\n![5](./images/store-app-1.png)\n\nOr use `curl -X POST http://localhost:8080/stores/start/2`\n\nIn the trace you should see something like:\n\n```shell\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":0,\"storeName\":\"SF02\",\"itemCode\":\"IT07\",\"quantity\":7,\"price\":46.79320631709398}\nstoreSaleGenerator_1  INFO  [ibm.gse.eda.sto.inf.ItemSaleGenerator] (executor-thread-1) {\"id\":1,\"storeName\":\"NYC01\",\"itemCode\":\"IT00\",\"quantity\":7,\"price\":0.7764381649099172}\n```\n### Verify messages arrived in Kafka items topic\n\nWe can use the Kafdrop tool to go to the `items` topic as illustrated below. The tool can be started via the `./startKafdrop.sh` command under the `infrastructure` folder after setting a `kakfa.properties` file as:\n\n```properties\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nssl.enabled.protocols=TLSv1.2\nssl.endpoint.identification.algorithm=HTTPS\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"APIKEY\";\n```\n\n![](./images/kafdrop.png)\n\n\n## Scenario 4: Event Streams on Cloud to DB2 on premise via JDBC Sink connector\n\nThis scenario is using the [IBM Kafka Connect sink connector for JDBC](https://github.com/ibm-messaging/kafka-connect-jdbc-sink) to get data from the `inventory topic` and write records to the `inventory` table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.\n\n### Pre-requisites\n\nAs a pre-requisite you need to have a [DB2 instance on cloud](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started) up and running with defined credentials. From the credentials you need the username, password and the `ssljdbcurl` parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".\n\n1. Build and deploy the `inventory-app`. This application is a simple Java microprofile 3.3 app exposing a set of end points for cRUD operations on stores, items and inventory. It is based on [Quarkus](https:quarkus.io). The instructions to build, and deploy this app is in the [README](https://github.com/jbcodeforce/eda-kconnect-lab/tree/master/inventory-app) under the `inventory-app` folder of [this repository](https://github.com/jbcodeforce/eda-kconnect-lab). At the application starts stores and items records are uploaded to the database.\n\n1. Verify the stores and items records are loaded\n\n  * If you deploy the `inventory-app` from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use [the drop sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/drop.sql) and then reload them the [insert sql script](https://raw.githubusercontent.com/jbcodeforce/eda-kconnect-lab/master/inventory-app/src/main/resources/import.sql) from `inventory-app/src/main/resources` folder. For that you can use the `Run sql` menu in the DB2 console:\n\n  ![DB1](./images/db2-1.png)\n\n  Select the database schema matching the username used as credential, and then open the SQL editor:\n\n  ![DB2](./images/db2-2.png)\n\n  Verify the items with `select * from items;`\n\n   ![DB2](./images/db2-3.png)\n\n  Verify the stores with `select * from stores;`\n\n  ![DB2](./images/db2-4.png)\n\n  The inventory has one record to illustrate the relationship between store, item and inventory.\n\n### Run the Kafka Connector in distributed mode\n\nThe docker image built in the [setup](#scenario-setup) has the configuration for kafka connect distributed cluster, we need in this scenario to start the connector and upload the DB2 Sink connector definition. To start it, run the script `./createOrStartKconnect.sh start` under `kconnect` folder.\n\n### Upload the DB2 sink definition\n\nRename the file `db2-sink-config-TMPL.json` as `db2-sink-config.json` and modify the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the `table.name.format`.\n\n```json\n  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"<username>\",\n    \"connection.password\": \"<password>\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"<username>.INVENTORY\"\n  }\n```\n\nOnce done, you can run the `./sendJdbcSinkConfig.sh url-kafka-connect` to upload the above definition to the Kafka connect controller. When running locally the command is `./sendJdbcSinkConfig.sh localhodt:8083`. This script delete previously define connector with the same name, and then perform a POST operation on the `/connectors` end point.\n\nThe connector trace should have something like:\n\n```logs\nconnector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n \terrors.log.enable = false\n \terrors.log.include.messages = false\n \terrors.retry.delay.max.ms = 60000\n \terrors.retry.timeout = 0\n \terrors.tolerance = none\n \theader.converter = null\n \tkey.converter = null\n \tname = jdbc-sink-connector\n \ttasks.max = 1\n \ttransforms = []\n \tvalue.converter = null\n\n```\n\n### Generate some records\n\nThe `integration-tests` folder includes a set of python code to load some records to the expected topic.\n\n1. Start a python environment with `./startPython.sh`\n1. Within the bash, start python to execute the  `ProduceInventoryEvent.py` script, and specify the number of records to send via the --size argument.\n\n```\npython ProduceInventoryEvent.py --size 2\n```\n1. The trace should have something like\n\n```\nProduce to the topic inventory\n[KafkaProducer] - This is the configuration for the producer:\n[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProducerInventory', 'acks': 0, 'request.timeout.ms': 10000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5'}\nsending -> {'storeName': 'LA02', 'itemCode': 'IT09', 'id': 0, 'timestamp': 1591211295.617515}\n[KafkaProducer] - Message delivered to inventory [0]\nsending -> {'storeName': 'PT02', 'itemCode': 'IT00', 'id': 1, 'timestamp': 1591211296.7727954}\n[KafkaProducer] - Message delivered to inventory [0]\n\n```\n\n### Verify records are uploaded into the Inventory database\n\nUsing the DB2 console, use the `select * from inventory;` SQL query to get the last records.\n\n## Scenario 5: Run the solution components end to end on Kubernetes\n\nThis solution covers all the components of the data pipeline. It still uses DB2 and Event Streams on Cloud but deploy all the other component in OpenShift as part of IBM Kubernetes Service.\n\n### Pre-requisites\n\nCreate the following services in IBM Cloud:\n\n* [DB2 instance](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-getting-started).\n* [IBM Kubernetes Service](https://cloud.ibm.com/docs/containers?topic=containers-cs_cluster_tutorial#cs_cluster_tutorial).\n\n### Deployment\n\nTo be finished !\n\n1. Verify the Store Sale Simulator runs\n1. Verify the connectors `http://localhost:8083/connectors`\n","frontmatter":{"title":"Kafka Connect with Event Streams on Cloud","description":"A set of labs and reference for working with Kafka Connect with Event streams on cloud"},"fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/technology/event-streams/kconnect.mdx"}}},"staticQueryHashes":["1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","63531786","63531786","768070550"]}