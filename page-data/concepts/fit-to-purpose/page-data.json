{"componentChunkName":"component---src-pages-concepts-fit-to-purpose-index-mdx","path":"/concepts/fit-to-purpose/","result":{"pageContext":{"frontmatter":{"title":"Fit for purpose","description":"Different fit for purpose evaluation and criteria for technologies involved in EDA"},"relativePagePath":"/concepts/fit-to-purpose/index.mdx","titleType":"append","MdxNode":{"id":"ee5c26f8-0bdb-5150-9d97-f586a8f28220","children":[],"parent":"1adc8d9a-90fd-5f54-a216-c3238bade35e","internal":{"content":"---\ntitle: Fit for purpose\ndescription: Different fit for purpose evaluation and criteria for technologies involved in EDA\n---\n\nIn this note we want to list some of the main criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study.\nFit for purpose practices should be done under a bigger program about application development governance and data governance.\nWe can look at least to the following major subjects:\n\n<AnchorLinks>\n  <AnchorLink>Cloud native applications</AnchorLink>\n  <AnchorLink>Modern data pipeline</AnchorLink>\n  <AnchorLink>MQ Versus Kafka</AnchorLink>\n  <AnchorLink>Kafka Streams vs Apache Flink</AnchorLink>\n</AnchorLinks>\n\n## Cloud native applications\n\nWith the adoption of cloud native and microservice applications (the [12 factors](https://12factor.net/) app), the following needs to be addressed:\n\n* Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the '[reactive](/advantages/reactive/) manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi cloud deployment practice.\n* Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST end points to pull the data from other services, each service pushes the change to their main business entity to a event backbone. Each future service which needs those data, pulls from the messaging system.\n* Adopting common patterns like [command query responsibility seggregation](/patterns/cqrs/) to help implementing complex queries, joining different business entities owned by different microservices, [event sourcing](/patterns/event-sourcing/), [transactional outbox](/patterns/intro/#transactional-outbox) and [SAGA](/patterns/saga/) for long running transaction.\n* Addressing data eventual consistency to propagate change to other components versus ACID transaction.\n* Support \"always-on\" approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers.\n\nSupporting all or part of those requirements will lead to the adoption of event-driven microservices and architecture.\n\n## Modern data pipeline\n\nAs new business applications need to react to events in real time, the adoption of [event backbone](/concepts/terms-and-definitions/#event-backbone) is really part of the IT toolbox. \nModern IT architecture encompasses the adoption of new data hub, where all the data about a 'customer', for example, is accessible in one event backbone. \nTherefore, it is natural to assess the data movement strategy and assess how to offload some of those ETL jobs running at night, \nby adopting real time data ingestion. \n\nWe detailed the new architecture in [this modern data lake](introduction/reference-architecture/#modern-data-lake) article, so from a *fit for purpose* point of view, \nwe need to assess the scope of existing ETL jobs, and refector to streaming logic that can be incorporated into different logs/ topics.\nWith Event Backbone like Kafka, any consumer can join the data log consumption at any point of time, within the retention period. \nBy moving the ETL logic to a streaming application, we do not need to wait for the next morning to get important metrics.\n\n## MQ Versus Kafka\n\nConsider queue system. like IBM MQ, for:\n\n* Exactly once delivery, and to participate into two phase commit transaction\n* Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response.\n* Recall messages in queue are kept until consumer(s) got them.\n\nConsider Kafka as pub/sub and persistence system for:\n\n* Publish events as immutable facts of what happen in an application\n* Get continuous visibility of the data Streams\n* Keep data once consumed, for future consumers, for replayability\n* Scale horizontally the message consumption\n\n## Kafka Streams vs Apache Flink\n\nOnce we have setup data streams, we need technology to support real-time analytics and complex event processing. Historically, analytics performed on static data was done using batch reporting techniques. However, if\ninsights have to be derived in real-time, event-driven architectures help to analyse and look for patterns within events.\n\n[Apache Flink](https://flink.apache.org) (2016) is a framework and **distributed processing** engine for stateful computations over unbounded and bounded data streams. It is considered to be superior to Apache Spark and Hadoop. It supports batch and graph processing and complex event processing. \nThe major stream processing features offered by Flink are:\n\n* Support for event time and out of order streams: use event time for consistent results\n* Consistency, fault tolerance, and high availability: guarantees consistent state updates in the presence of failures and consistent data movement between selected sources and sinks\n* Low latency and high throughput: tune the latency-throughput trade off, making the system suitable for both high-throughput data ingestion and transformations, as well as ultra low latency (millisecond range) applications.\n* Expressive and easy-to-use APIs in Scala and Java: map, reduce, join, with window, split,... Easy to implement the business logic using Function.\n* Support for sessions and unaligned windows: Flink completely decouples windowing from fault tolerance, allowing for richer forms of windows, such as sessions.\n* Connectors and integration points: Kafka, Kinesis, Queue, Database, Devices...\n* Developer productivity and operational simplicity: Start in IDE to develop and deploy and deploy to Kubernetes, [Yarn](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html), [Mesos](http://mesos.apache.org/) or containerized\n* Support batch processing\n* Includes Complex Event Processing capabilities\n\nHere is simple diagram of Flink architecture from the Flink web site:\n\n ![Flink components](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/distributed-runtime.svg)\n\n\nSee this [technology summary](https://jbcodeforce.github.io/flink-studies/) and samples.\n\nSee also [this article from Confluent](https://www.confluent.io/blog/apache-flink-apache-kafka-streams-comparison-guideline-users)","type":"Mdx","contentDigest":"a1477f7e6d41017a8a3d8cbbe547038e","owner":"gatsby-plugin-mdx","counter":696},"frontmatter":{"title":"Fit for purpose","description":"Different fit for purpose evaluation and criteria for technologies involved in EDA"},"exports":{},"rawBody":"---\ntitle: Fit for purpose\ndescription: Different fit for purpose evaluation and criteria for technologies involved in EDA\n---\n\nIn this note we want to list some of the main criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study.\nFit for purpose practices should be done under a bigger program about application development governance and data governance.\nWe can look at least to the following major subjects:\n\n<AnchorLinks>\n  <AnchorLink>Cloud native applications</AnchorLink>\n  <AnchorLink>Modern data pipeline</AnchorLink>\n  <AnchorLink>MQ Versus Kafka</AnchorLink>\n  <AnchorLink>Kafka Streams vs Apache Flink</AnchorLink>\n</AnchorLinks>\n\n## Cloud native applications\n\nWith the adoption of cloud native and microservice applications (the [12 factors](https://12factor.net/) app), the following needs to be addressed:\n\n* Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the '[reactive](/advantages/reactive/) manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi cloud deployment practice.\n* Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST end points to pull the data from other services, each service pushes the change to their main business entity to a event backbone. Each future service which needs those data, pulls from the messaging system.\n* Adopting common patterns like [command query responsibility seggregation](/patterns/cqrs/) to help implementing complex queries, joining different business entities owned by different microservices, [event sourcing](/patterns/event-sourcing/), [transactional outbox](/patterns/intro/#transactional-outbox) and [SAGA](/patterns/saga/) for long running transaction.\n* Addressing data eventual consistency to propagate change to other components versus ACID transaction.\n* Support \"always-on\" approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers.\n\nSupporting all or part of those requirements will lead to the adoption of event-driven microservices and architecture.\n\n## Modern data pipeline\n\nAs new business applications need to react to events in real time, the adoption of [event backbone](/concepts/terms-and-definitions/#event-backbone) is really part of the IT toolbox. \nModern IT architecture encompasses the adoption of new data hub, where all the data about a 'customer', for example, is accessible in one event backbone. \nTherefore, it is natural to assess the data movement strategy and assess how to offload some of those ETL jobs running at night, \nby adopting real time data ingestion. \n\nWe detailed the new architecture in [this modern data lake](introduction/reference-architecture/#modern-data-lake) article, so from a *fit for purpose* point of view, \nwe need to assess the scope of existing ETL jobs, and refector to streaming logic that can be incorporated into different logs/ topics.\nWith Event Backbone like Kafka, any consumer can join the data log consumption at any point of time, within the retention period. \nBy moving the ETL logic to a streaming application, we do not need to wait for the next morning to get important metrics.\n\n## MQ Versus Kafka\n\nConsider queue system. like IBM MQ, for:\n\n* Exactly once delivery, and to participate into two phase commit transaction\n* Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response.\n* Recall messages in queue are kept until consumer(s) got them.\n\nConsider Kafka as pub/sub and persistence system for:\n\n* Publish events as immutable facts of what happen in an application\n* Get continuous visibility of the data Streams\n* Keep data once consumed, for future consumers, for replayability\n* Scale horizontally the message consumption\n\n## Kafka Streams vs Apache Flink\n\nOnce we have setup data streams, we need technology to support real-time analytics and complex event processing. Historically, analytics performed on static data was done using batch reporting techniques. However, if\ninsights have to be derived in real-time, event-driven architectures help to analyse and look for patterns within events.\n\n[Apache Flink](https://flink.apache.org) (2016) is a framework and **distributed processing** engine for stateful computations over unbounded and bounded data streams. It is considered to be superior to Apache Spark and Hadoop. It supports batch and graph processing and complex event processing. \nThe major stream processing features offered by Flink are:\n\n* Support for event time and out of order streams: use event time for consistent results\n* Consistency, fault tolerance, and high availability: guarantees consistent state updates in the presence of failures and consistent data movement between selected sources and sinks\n* Low latency and high throughput: tune the latency-throughput trade off, making the system suitable for both high-throughput data ingestion and transformations, as well as ultra low latency (millisecond range) applications.\n* Expressive and easy-to-use APIs in Scala and Java: map, reduce, join, with window, split,... Easy to implement the business logic using Function.\n* Support for sessions and unaligned windows: Flink completely decouples windowing from fault tolerance, allowing for richer forms of windows, such as sessions.\n* Connectors and integration points: Kafka, Kinesis, Queue, Database, Devices...\n* Developer productivity and operational simplicity: Start in IDE to develop and deploy and deploy to Kubernetes, [Yarn](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html), [Mesos](http://mesos.apache.org/) or containerized\n* Support batch processing\n* Includes Complex Event Processing capabilities\n\nHere is simple diagram of Flink architecture from the Flink web site:\n\n ![Flink components](https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/distributed-runtime.svg)\n\n\nSee this [technology summary](https://jbcodeforce.github.io/flink-studies/) and samples.\n\nSee also [this article from Confluent](https://www.confluent.io/blog/apache-flink-apache-kafka-streams-comparison-guideline-users)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/concepts/fit-to-purpose/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}