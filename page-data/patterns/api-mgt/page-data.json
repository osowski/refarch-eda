{"componentChunkName":"component---src-pages-patterns-api-mgt-index-mdx","path":"/patterns/api-mgt/","result":{"pageContext":{"frontmatter":{"title":"API Management - AsyncAPI","description":"API management in the context of reactive solution"},"relativePagePath":"/patterns/api-mgt/index.mdx","titleType":"append","MdxNode":{"id":"07271422-d4a4-533f-9ffd-927c167611bd","children":[],"parent":"79742729-b4ed-5da3-9940-a6915b3f19ee","internal":{"content":"---\ntitle: API Management - AsyncAPI\ndescription: API management in the context of reactive solution\n--- \n\nThis page summarizes some of the best practices for introducing API Management in development practices and architecture patterns within an enterprise setting.\n\n## Moving from a Pure API Gateway to an API Management system\n\nAn API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns:\n\n* As a **Security Gateway**, placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels.\n* As an **API Gateway**, both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring.\n* To provide **connectivity (HTTP) and mediation (XML, JSON) services** in the internal network, close to the system of record. \n\nAPI Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: \n\n* API lifecycle management to activate, retire, or stage an API product.\n* API governance with security, access, and versioning.\n* Analytics, dashboards, and third party data offload for usage analysis.\n* API socialization based on a portal for the developer community that allows self-service and discovery.\n* An API developer toolkit to facilitate the creation and testing of APIs.\n\n### Classical Pain Points\n\nSome of the familiar pain points that indicate the need for a broader API Management product include:\n\n* Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented.\n* Difficult to tell which subscribers are really using the API and how often, without building a custom solution.\n* Difficult to differentiate between business-critical subscribers versus low value subscribers.\n* Managing different lines of business and organizations is complex.\n* No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios.\n* Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL\n* No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs.\n* The need to ensure consistent security rules\n* Integrating CI/CD pipelines with the API lifecycle\n\n## Enterprise APIs across boundaries\n\nIf you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server.\n\n![](./images/api-mgt-ra.png)\n\nHere is how it works:\n\n1. An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload.\n1. The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers.\n1. An application developer accesses the developer portal and uses search to discover any available APIs.\n1. The application developer uses the API in an application and deploys that application to the device\n1. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system.\n1. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics.\n\n\nDeployment across cloud providers could look like the diagram below, using API Connect in Cloud Pak for Integration:\n\n![](./images/federated-API-mgt.png)\n\nOn the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume. \nThe Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. \n\nThe API Gateway services colocated with the target application services to reduce latency. These would be deployed as <em>StatefulSet</em> on an OpenShift cluster, which means as a set of pods with consistent <em>identities</em>. Identities are defined as:\n\n* <b>Network</b>: A single stable DNS and hostname.\n* <b>Storage</b>: As many VolumeClaims as requested.\n\nThe StatefulSet guarantees that a given network identity will always map to the same storage identity. \n\nAn API Gateway acts as a reverse proxy, and, in this case, exposes the `Inventory APIs`, enforcing user authentication and security policies, \nand handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform \ntransformations and aggregate various services to fulfill a request. \n\nThe Developer Portals can be separated, or centralized depending on API characteristics exposed from different \nclouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal\n is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is \n also a StatefulSet and gets metrics from the gateway.\n\nThe application microservices are accessing remote services and this traffic can also go to the API gateway. \nThose services will also integrate with existing backend services running on-premise, whether they are deployed \nor not on OpenShift. \n\nIn the diagram above, the management service for the API Management product is depicted as running on-premise\n to illustrate that it is a central deployment to manage multiple gateways.\n\nGateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the \nManagement System, which can communicate across availability zones.\n\nThe different API Management services run on OpenShift which can help ensure high availability of each of \nthe components. \n\n## Open API\n\nWithin the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes. \nEither way, the API can be uploaded to the API Management product.\n\nThe important parts are to define the operations exposed and the request / response structure of the data model. \n\n## Support for Async API\n\nCloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. \n\nAsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs.\n\nThe AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation,\n and from discovery to event management\" [asyncapi.com/docs](https://asyncapi.com/docs). \n\nThe goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs, \nand standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures \nwhat OpenAPI is to REST APIs. \n\nWhile OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice \nfor event-driven APIs.\n\n### AsyncAPI Documents\n\nAn AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API. \nFor example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI), \nand the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions (https://dalelane.co.uk/blog/?p=4219).\n\nHere is what it looks like: \n\n![](./images/asyncapi.jpg)\n\nYou may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). \n\nThe creator of the AsyncAPI specification, Fran Méndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\".\nThis forced him to write additonal code to support the necessary EDA-based documentation and code generation.\n\nMany companies use OpenAPI, but in real-world situations, systems need formalized documentation and code generation support for both REST APIs <em>and</em> events. \n\nHere are the structural differences (and similarities) between OpenAPI and AsyncAPI:\n\n![](./images/structurediffs.jpg)\n\nSource: https://www.asyncapi.com/docs/getting-started/coming-from-openapi\n\n**Note a few things:**\n\n* AsyncAPI is compatible with OpenAPI schemas, which is quite useful since the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses.\n* The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled [Why Use Avro for Kafka](#why-use-avro-for-kafka) below). \n* The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that <em>scheme</em> has been renamed to <em>protocol</em> and AsyncAPI introduces a new property called <em>protocolVersion</em>.\n* AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of <em>query</em> and <em>cookie</em>, and header parameters can be defined in the message object. \n\n### Describing Kafka with AsyncAPI\n\nIt is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in [this article](https://dalelane.co.uk/blog/?p=4219) written by Dale Lane.\n\nFirst, there are some minor differences in terminology between Kafka and AsyncAPI that you should note: \n\n#### Comparing Kafka and AsyncAPI Terminology\n\n![kafka vs asyncapi](./images/kafkavsasyncapi.jpg)\n\n### The AsyncAPI Document\n\nConsidering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document:\n\n#### Info\n\nThe Info section has three parts which represent the minimum required information about the application: <em>title</em>, <em>version</em>, and <em>description</em> (optional), used as follows:\n\n```yaml\nasyncapi: 2.0.0\n...\ninfo:\n  title: Account Service\n  version: 1.0.0\n  description: This service is in charge of processing user signups\n```\n\nIn the description field you can use markdown language for rich-text formatting.\n\n#### Id\n\nThe Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example:\n\n```yaml\nasyncapi: '2.0.0'\n...\nid: 'urn:uk:co:usera:apimgmtdemo:inventoryevents'\n...\n```\n\n#### Servers\n\nThe servers section allows you to add and define which servers client applications can connect to,\n for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker).\n\nHere is an example, where you have three Kafka brokers in the same cluster:\n\n```yaml\nservers:\n  broker1:\n    url: andy-broker-0:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the first broker\n  broker2:\n    url: andy-broker-1:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the second broker\n  broker3:\n    url: andy-broker-2:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the third broker\n```\n\nThe example above uses the `kafka` protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any. \nFor example, the most common protocols include: `mqtt` which is widely adopted by the Internet of Things and mobile apps, `amqp`, which is popular \nfor its reliable queueing, `ws` for WebSockets, frequently used in browsers, and `http`, which is used in HTTP streaming APIs.\n\n#### Difference Between the AMQP and MQTT Protocols\n\n* <b>AMQP</b> was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security. \nThe main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly in fanout form, by topic, and also based on headers.\n\n* <b>MQTT</b> The design principles and aims of MQTT are much more simple and focused than those of AMQP. it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth, \nhigh latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems.\n\nWhen using a <em>broker-centric</em> architecture such as Kafka or MQ, you normally specify the URL of the broker. \nFor more classic client-server models, such as REST APIs, your server should be the URL of the server.\n\nOne limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments) \nin a single document.\n\nOne workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension \nfields to explain which ones are in which cluster. This is, however, not recommended because it could \ninterfere with code  generators or other parts of the AsyncAPI ecosystem which may consider them as all being\n members of one large cluster.\n\nSo, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document.\n\n<b>NOTE</b>: As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry \nas your own extension to the AsyncAPI specs.\n\n#### Security\n\nIf the Kafka cluster doesn’t have auth enabled, the protocol used should be `kafka`. Otherwise, \nif client applications are required to provide credentials, the protocol should be `kafka-secure`. \n\nTo identify the type of credentials, add a security section to the server object. The value you put there\n is the name of a securityScheme object you define in the components section.\n\nThe types of security schemes that you can specify aren’t Kafka-specific. Choose the value that describes \nyour type of approach to security.\n\nFor example, if you’re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe \nthis as ```userPassword```. Here is an example:\n\n```yaml\nasyncapi: 2.0.0\n...\nservers:\n  broker1:\n    url: localhost:9092\n    description: Production server\n    protocol: kafka-secure\n    protocolVersion: '1.0.0'\n    security:\n    - saslScramCreds: []\n...\ncomponents:\n  securitySchemes:\n    saslScramCreds:\n      type: userPassword\n      description: Info about how/where to get credentials\n      x-mykafka-sasl-mechanism: 'SCRAM-SHA-256'\n\n```\n\nThe description field allows you to explain the security options that Kafka clients need to use. \nYou could also use an extension (with the -x prefix).\n\n#### Channels\n\nAll brokers support communication through multiple channels (known as <em>topics</em>, <em>event types</em>, <em>routing keys</em>, <em>event names</em> or other terms depending on the system).\nChannels are assigned a name or identifier.\n\nThe channels section of the specification stores all of the mediums where messages flow through. \nHere is a simple example:\n\n```yaml\nchannels:\n  hello:\n    publish:\n      message:\n        payload:\n          type: string\n          pattern: '^hello .+$'\n```\n\nIn this example, you only have one channel called hello. An app would subscribe to this channel \nto receive hello {name} messages. Notice that the payload object defines how the message must be structured. \nIn this example, the message must be of type string and match the regular expression ```'^hello .+$'``` in the\n format hello {name} string.\n\nEach topic (or channel) identifies the operations that you want to describe in the spec. \nHere is another example:\n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      operationId: someUniqueId\n      summary: Interesting messages\n      description: You can get really interesting messages from this topic\n      tags:\n      - name: awesome\n      - name: interesting\n      ...\n```\n\nFor each operation, you can provide a unique id, a short one-line text summary, and a more detailed description \nin plain text or markdown formatting.\n\n#### Bindings\n\nAsyncAPI puts protocol-specific values in sections called <em>bindings</em>.\n\nThe bindings sections allows you to specify the values that Kafka clients should use to perform the operation. \nThe values you can describe here include the consumer group id and the client id.\n\nIf there are expectations about the format of these values, then you can describe those by using \nregular expressions: \n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            pattern: '^[A-Z]{10}[0-5]$'\n          clientId:\n            type: string\n            pattern: '^[a-z]{22}$'\n          bindingVersion: '0.1.0'\n```\n\nYou can instead specify a discrete set of values, in the form of enumerations:\n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            enum:\n            - validone\n            - validtwo\n          clientId:\n            type: string\n            enum:\n            - validoption\n          bindingVersion: '0.1.0'\n\n```\n\n#### Messages\n\nA message is how information is exchanged via a channel between servers and applications. According to the\nAsyncAPI specifications, a message must contain a payload and may also contain headers. The headers may be subdivided \n into protocol-defined headers and header properties defined by the application which can act as supporting\n metadata. The payload contains the data, defined by the application, which must be serialized into a supported\nformat (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response.\n\nAs with all the other levels of the spec, you can provide background and narrative in a description field for the message:\n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    ...\n    subscribe:\n      ...\n      message:\n        description: Description of a single message\n```\n\n### Summary\n\nIn short, the following diagram summarizes the sections described above:\n\n![](./images/asyncapisummary.jpg)\n\nFor more information, the official AsyncAPI specifications can be found [here](https://www.asyncapi.com/docs/specifications/2.0.0).\n\n## Why Use Avro for Kafka?\n\nApache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features:\n\n* Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings\n* Compact format, making it more efficient for high-volume usage\n* Bindings for a wide variety of programming languages\n* A rich, extensible schema language defined in pure JSON\n\n## A simple API Management Demo\n\nBesides a new, event-driven approach to its API model, ACME Inc needs a way to securely provide self-service access to different versions of its APIs, \nto enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria.\n\nIBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing \nAPIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak® solution.\n\nThe API Management demo demonstrates three main areas of interest: \n\n* Version Control\n* API Documentation & Discovery\n* Redirecting to different APIs based on certain criteria\n\n### Types of APIs\n\nAn API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol. \nApplications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one\n of the following types:\n\n#### REST API \n\nA REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged. \nFor example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends \non the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface\n(by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing.\n\nIn IBM API Connect, you can create REST APIs by using user interface functions to create models and data sources (top-down). \nThese are then used by your REST API definition and exposed to your users. API Connect also supports a <em>bottom-up</em> approach where \nyou can import existing APIs into the system and benefit from the API Management capabilities of the product.\n\nAlternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy.\n\nIn either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI \ndefinition file and publishing it using either API Manager or the command line interface.\n\n#### SOAP API\n\nAPI Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file. \nYou can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables. \nYou can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure.\n\nYou can create SOAP API definitions through either the command line interface, or through the API Manager UI.\n\n#### GraphQL\n\nGraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM® API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs.\nSee [this dedicated section](#graphql-apis).\n\n#### AsyncAPI\n\nWe already address in detail in [previous sections](#asyncapi-documents).\n\n### API Manager\n\nYou can manage your APIs by using API Connect's API Manager UI:\n\n![](./images/apimanagerui.png)\n\nThe API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs.\n\n### Developer Portal\n\nThe Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published, \napplication developers can browse and use the APIs from the Developer Portal dashboard, as shown below:\n\n![](./images/devportal1.jpg)\n\n\nThe Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization. \nYou can configure the Developer Portal for test and development purposes, or for internal use only.\n\n### Create Capability\n\nDeveloper can leverage API Connect's <b>Create</b> capability, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach. \nAPIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database.\n\n#### Creating a REST proxy API from an existing target service\n\nAs mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system.\nEssentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager.\n\n\n### Explore Capability\n\nDevelopers can use API Connect's <b>Explore</b> capability to quickly examine their new APIs and try their operations.\n\nDevelopers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above. \nThis can be done through API Connect's UI or with the provided CLI. \n\nDevelopers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as \nthose that were added based on existing data sources. \n\n### Subscribing to an API in the Developer Portal\n\nTo subscribe to an API, from the Developer Portal, the develper clicks `API Products` to find and subscribe to any available APIs:\n\n![](./images/devportal2.jpg)\n\nIn the example above, an API called <em>FindBranch</em> Version 2.0, which is contained in the <em>FindBranch Auto Products</em> product is available. Clicking ```Subscribe``` enables the developer to use the API:\n\n![](./images/devportal3.jpg)\n\nUnder the Application heading, the developer can click ```Select App``` for the new application:\n\n![](./images/devportal4.jpg)\n\nFrom the next screen below, the developer can click Next:\n\n![](./images/devportal5.jpg)\n\nDoing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing ```Done``` in the next screen completes the subscription.\n\n![](./images/devportal6.jpg)\n\n\n### Testing An API in the Developer Portal\n\nTo test an API, the developer clicks ```API Products``` in the Developer Portal dashboard to show all available products:\n\n![](./images/devportal7.jpg)\n\nClicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. \n\n![](./images/devportal8.jpg)\n\nThe developer can click ```GET/details```, for instance, to see the details of the GET operation: \n\n![](./images/devportal10.jpg)\n\nClicking the ```Try it``` tab and pressing ```Send``` allows the developer to test the API to better understand how it works:\n\n![](./images/devportal11.jpg)\n\nAs you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful.\n\n### API Product Managers\n\nAdministrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. \n\nOnce the APIs are ready, API Product Managers can expose them for developer consumption and self-service in the Developer Portal. Developers who have signed up can discover and use any APIs which were exposed. \n\nBecause the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples\n in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js.\n\n### Testing\n\nBesides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above, it is possible to use <b>API Connect Test and Monitor</b> to effortlessly generate\n and schedule API test assertions. This browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs.\n\nYou can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results:\n\n![](./images/testing1.png)\n\nYou enter any required parameters and authorization token and press `Send` to send the request. \n\nAPI Connect Test and Monitor then shows you the response payload as a formatted JSON object:\n\n![](./images/testing2.png)\n\n\nTo generate a test, you then click `Generate Test`, and in the dialog that appears name the test and add it to a project. \nWhen you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload.\n\nYou can add, delete, or modify these assertions, or even add new ones from a broad selection:\n\n![](./images/assertions2.png)\n\nYou can also reorder the assertions by dragging and dropping them:\n\n![](./images/assertions3.png)\n\nAlthough coding is not required, if you wish, you change the underlying code by clicking `CODE` to enter the code view:\n\n![](./images/testing3.png)\n\nOnce you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures:\n\n![](./images/testrpt.png)\n\n### Test Scheduling\n\nIBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling:\n\n![](./images/testpublish.png)\n\nOnce your test is published, you can click `Schedule` to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests:\n\n![](./images/newrun.png)\n\nClicking `Save Run` schedules the tests. \n\nFinally, you can access the dashboard to monitor the health of your API:\n\n![](./images/dashboard.png)\n\n## GraphQL APIs\n\nIBM® API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. \nThe Developer Portal also supports testing GraphQL APIs. \n\n### Advantages of GraphQL over REST APIs\n\nGraphQL provides the following particular advantages over REST APIs:\n\n* The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details. \nWith a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data.\n* The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds. \nIf an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details, \nthen make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request.\n* However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit.\n\n\n### Creating a GraphQL proxy API\n\nThe  demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click `Add`, then `API (from REST, GraphQL or SOAP)`:\n\n![](./images/graphql1.png)\n\nThen select, `From existing GraphQL service (GraphQL proxy)`:\n\n![](./images/graphql2.png)\n\nGive it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. \n\n![](./images/graphql3.png)\n\nNotice that, once you enter the URL, API Connect will find the service and automatically detect the schema. \n\n![](./images/graphql4.png)\n\nYou can click `Next` and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server:\n\n![](./images/graphql5.png)\n\nThe gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. \n\nClicking Next allows you to then activate the API for publishing:\n\n![](./images/graphql6.png)\n\nFinally, clicking `Next` shows a screen such as the one below with checkmarks:\n\n![](./images/graphql7.png)\n\nThis shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies:\n\n![](./images/graphql8.png)\n\n\n## Further Readings\n\n* [IBM Redbook on Agile Integration](https://www.redbooks.ibm.com/abstracts/sg248452.html)\n* [A Demo of Event Endpoint Management - Cloud Pak for Integration](https://community.ibm.com/community/user/integration/blogs/dale-lane1/2021/04/12/a-demo-of-event-endpoint-management])\n","type":"Mdx","contentDigest":"254c6527f8496daea247bbeb1b6b68f0","owner":"gatsby-plugin-mdx","counter":682},"frontmatter":{"title":"API Management - AsyncAPI","description":"API management in the context of reactive solution"},"exports":{},"rawBody":"---\ntitle: API Management - AsyncAPI\ndescription: API management in the context of reactive solution\n--- \n\nThis page summarizes some of the best practices for introducing API Management in development practices and architecture patterns within an enterprise setting.\n\n## Moving from a Pure API Gateway to an API Management system\n\nAn API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns:\n\n* As a **Security Gateway**, placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels.\n* As an **API Gateway**, both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring.\n* To provide **connectivity (HTTP) and mediation (XML, JSON) services** in the internal network, close to the system of record. \n\nAPI Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: \n\n* API lifecycle management to activate, retire, or stage an API product.\n* API governance with security, access, and versioning.\n* Analytics, dashboards, and third party data offload for usage analysis.\n* API socialization based on a portal for the developer community that allows self-service and discovery.\n* An API developer toolkit to facilitate the creation and testing of APIs.\n\n### Classical Pain Points\n\nSome of the familiar pain points that indicate the need for a broader API Management product include:\n\n* Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented.\n* Difficult to tell which subscribers are really using the API and how often, without building a custom solution.\n* Difficult to differentiate between business-critical subscribers versus low value subscribers.\n* Managing different lines of business and organizations is complex.\n* No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios.\n* Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL\n* No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs.\n* The need to ensure consistent security rules\n* Integrating CI/CD pipelines with the API lifecycle\n\n## Enterprise APIs across boundaries\n\nIf you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server.\n\n![](./images/api-mgt-ra.png)\n\nHere is how it works:\n\n1. An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload.\n1. The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers.\n1. An application developer accesses the developer portal and uses search to discover any available APIs.\n1. The application developer uses the API in an application and deploys that application to the device\n1. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system.\n1. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics.\n\n\nDeployment across cloud providers could look like the diagram below, using API Connect in Cloud Pak for Integration:\n\n![](./images/federated-API-mgt.png)\n\nOn the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume. \nThe Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. \n\nThe API Gateway services colocated with the target application services to reduce latency. These would be deployed as <em>StatefulSet</em> on an OpenShift cluster, which means as a set of pods with consistent <em>identities</em>. Identities are defined as:\n\n* <b>Network</b>: A single stable DNS and hostname.\n* <b>Storage</b>: As many VolumeClaims as requested.\n\nThe StatefulSet guarantees that a given network identity will always map to the same storage identity. \n\nAn API Gateway acts as a reverse proxy, and, in this case, exposes the `Inventory APIs`, enforcing user authentication and security policies, \nand handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform \ntransformations and aggregate various services to fulfill a request. \n\nThe Developer Portals can be separated, or centralized depending on API characteristics exposed from different \nclouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal\n is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is \n also a StatefulSet and gets metrics from the gateway.\n\nThe application microservices are accessing remote services and this traffic can also go to the API gateway. \nThose services will also integrate with existing backend services running on-premise, whether they are deployed \nor not on OpenShift. \n\nIn the diagram above, the management service for the API Management product is depicted as running on-premise\n to illustrate that it is a central deployment to manage multiple gateways.\n\nGateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the \nManagement System, which can communicate across availability zones.\n\nThe different API Management services run on OpenShift which can help ensure high availability of each of \nthe components. \n\n## Open API\n\nWithin the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes. \nEither way, the API can be uploaded to the API Management product.\n\nThe important parts are to define the operations exposed and the request / response structure of the data model. \n\n## Support for Async API\n\nCloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. \n\nAsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs.\n\nThe AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation,\n and from discovery to event management\" [asyncapi.com/docs](https://asyncapi.com/docs). \n\nThe goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs, \nand standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures \nwhat OpenAPI is to REST APIs. \n\nWhile OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice \nfor event-driven APIs.\n\n### AsyncAPI Documents\n\nAn AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API. \nFor example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI), \nand the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions (https://dalelane.co.uk/blog/?p=4219).\n\nHere is what it looks like: \n\n![](./images/asyncapi.jpg)\n\nYou may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). \n\nThe creator of the AsyncAPI specification, Fran Méndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\".\nThis forced him to write additonal code to support the necessary EDA-based documentation and code generation.\n\nMany companies use OpenAPI, but in real-world situations, systems need formalized documentation and code generation support for both REST APIs <em>and</em> events. \n\nHere are the structural differences (and similarities) between OpenAPI and AsyncAPI:\n\n![](./images/structurediffs.jpg)\n\nSource: https://www.asyncapi.com/docs/getting-started/coming-from-openapi\n\n**Note a few things:**\n\n* AsyncAPI is compatible with OpenAPI schemas, which is quite useful since the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses.\n* The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled [Why Use Avro for Kafka](#why-use-avro-for-kafka) below). \n* The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that <em>scheme</em> has been renamed to <em>protocol</em> and AsyncAPI introduces a new property called <em>protocolVersion</em>.\n* AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of <em>query</em> and <em>cookie</em>, and header parameters can be defined in the message object. \n\n### Describing Kafka with AsyncAPI\n\nIt is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in [this article](https://dalelane.co.uk/blog/?p=4219) written by Dale Lane.\n\nFirst, there are some minor differences in terminology between Kafka and AsyncAPI that you should note: \n\n#### Comparing Kafka and AsyncAPI Terminology\n\n![kafka vs asyncapi](./images/kafkavsasyncapi.jpg)\n\n### The AsyncAPI Document\n\nConsidering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document:\n\n#### Info\n\nThe Info section has three parts which represent the minimum required information about the application: <em>title</em>, <em>version</em>, and <em>description</em> (optional), used as follows:\n\n```yaml\nasyncapi: 2.0.0\n...\ninfo:\n  title: Account Service\n  version: 1.0.0\n  description: This service is in charge of processing user signups\n```\n\nIn the description field you can use markdown language for rich-text formatting.\n\n#### Id\n\nThe Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example:\n\n```yaml\nasyncapi: '2.0.0'\n...\nid: 'urn:uk:co:usera:apimgmtdemo:inventoryevents'\n...\n```\n\n#### Servers\n\nThe servers section allows you to add and define which servers client applications can connect to,\n for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker).\n\nHere is an example, where you have three Kafka brokers in the same cluster:\n\n```yaml\nservers:\n  broker1:\n    url: andy-broker-0:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the first broker\n  broker2:\n    url: andy-broker-1:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the second broker\n  broker3:\n    url: andy-broker-2:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the third broker\n```\n\nThe example above uses the `kafka` protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any. \nFor example, the most common protocols include: `mqtt` which is widely adopted by the Internet of Things and mobile apps, `amqp`, which is popular \nfor its reliable queueing, `ws` for WebSockets, frequently used in browsers, and `http`, which is used in HTTP streaming APIs.\n\n#### Difference Between the AMQP and MQTT Protocols\n\n* <b>AMQP</b> was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security. \nThe main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly in fanout form, by topic, and also based on headers.\n\n* <b>MQTT</b> The design principles and aims of MQTT are much more simple and focused than those of AMQP. it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth, \nhigh latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems.\n\nWhen using a <em>broker-centric</em> architecture such as Kafka or MQ, you normally specify the URL of the broker. \nFor more classic client-server models, such as REST APIs, your server should be the URL of the server.\n\nOne limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments) \nin a single document.\n\nOne workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension \nfields to explain which ones are in which cluster. This is, however, not recommended because it could \ninterfere with code  generators or other parts of the AsyncAPI ecosystem which may consider them as all being\n members of one large cluster.\n\nSo, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document.\n\n<b>NOTE</b>: As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry \nas your own extension to the AsyncAPI specs.\n\n#### Security\n\nIf the Kafka cluster doesn’t have auth enabled, the protocol used should be `kafka`. Otherwise, \nif client applications are required to provide credentials, the protocol should be `kafka-secure`. \n\nTo identify the type of credentials, add a security section to the server object. The value you put there\n is the name of a securityScheme object you define in the components section.\n\nThe types of security schemes that you can specify aren’t Kafka-specific. Choose the value that describes \nyour type of approach to security.\n\nFor example, if you’re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe \nthis as ```userPassword```. Here is an example:\n\n```yaml\nasyncapi: 2.0.0\n...\nservers:\n  broker1:\n    url: localhost:9092\n    description: Production server\n    protocol: kafka-secure\n    protocolVersion: '1.0.0'\n    security:\n    - saslScramCreds: []\n...\ncomponents:\n  securitySchemes:\n    saslScramCreds:\n      type: userPassword\n      description: Info about how/where to get credentials\n      x-mykafka-sasl-mechanism: 'SCRAM-SHA-256'\n\n```\n\nThe description field allows you to explain the security options that Kafka clients need to use. \nYou could also use an extension (with the -x prefix).\n\n#### Channels\n\nAll brokers support communication through multiple channels (known as <em>topics</em>, <em>event types</em>, <em>routing keys</em>, <em>event names</em> or other terms depending on the system).\nChannels are assigned a name or identifier.\n\nThe channels section of the specification stores all of the mediums where messages flow through. \nHere is a simple example:\n\n```yaml\nchannels:\n  hello:\n    publish:\n      message:\n        payload:\n          type: string\n          pattern: '^hello .+$'\n```\n\nIn this example, you only have one channel called hello. An app would subscribe to this channel \nto receive hello {name} messages. Notice that the payload object defines how the message must be structured. \nIn this example, the message must be of type string and match the regular expression ```'^hello .+$'``` in the\n format hello {name} string.\n\nEach topic (or channel) identifies the operations that you want to describe in the spec. \nHere is another example:\n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      operationId: someUniqueId\n      summary: Interesting messages\n      description: You can get really interesting messages from this topic\n      tags:\n      - name: awesome\n      - name: interesting\n      ...\n```\n\nFor each operation, you can provide a unique id, a short one-line text summary, and a more detailed description \nin plain text or markdown formatting.\n\n#### Bindings\n\nAsyncAPI puts protocol-specific values in sections called <em>bindings</em>.\n\nThe bindings sections allows you to specify the values that Kafka clients should use to perform the operation. \nThe values you can describe here include the consumer group id and the client id.\n\nIf there are expectations about the format of these values, then you can describe those by using \nregular expressions: \n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            pattern: '^[A-Z]{10}[0-5]$'\n          clientId:\n            type: string\n            pattern: '^[a-z]{22}$'\n          bindingVersion: '0.1.0'\n```\n\nYou can instead specify a discrete set of values, in the form of enumerations:\n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            enum:\n            - validone\n            - validtwo\n          clientId:\n            type: string\n            enum:\n            - validoption\n          bindingVersion: '0.1.0'\n\n```\n\n#### Messages\n\nA message is how information is exchanged via a channel between servers and applications. According to the\nAsyncAPI specifications, a message must contain a payload and may also contain headers. The headers may be subdivided \n into protocol-defined headers and header properties defined by the application which can act as supporting\n metadata. The payload contains the data, defined by the application, which must be serialized into a supported\nformat (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response.\n\nAs with all the other levels of the spec, you can provide background and narrative in a description field for the message:\n\n```yaml\nasyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    ...\n    subscribe:\n      ...\n      message:\n        description: Description of a single message\n```\n\n### Summary\n\nIn short, the following diagram summarizes the sections described above:\n\n![](./images/asyncapisummary.jpg)\n\nFor more information, the official AsyncAPI specifications can be found [here](https://www.asyncapi.com/docs/specifications/2.0.0).\n\n## Why Use Avro for Kafka?\n\nApache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features:\n\n* Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings\n* Compact format, making it more efficient for high-volume usage\n* Bindings for a wide variety of programming languages\n* A rich, extensible schema language defined in pure JSON\n\n## A simple API Management Demo\n\nBesides a new, event-driven approach to its API model, ACME Inc needs a way to securely provide self-service access to different versions of its APIs, \nto enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria.\n\nIBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing \nAPIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak® solution.\n\nThe API Management demo demonstrates three main areas of interest: \n\n* Version Control\n* API Documentation & Discovery\n* Redirecting to different APIs based on certain criteria\n\n### Types of APIs\n\nAn API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol. \nApplications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one\n of the following types:\n\n#### REST API \n\nA REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged. \nFor example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends \non the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface\n(by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing.\n\nIn IBM API Connect, you can create REST APIs by using user interface functions to create models and data sources (top-down). \nThese are then used by your REST API definition and exposed to your users. API Connect also supports a <em>bottom-up</em> approach where \nyou can import existing APIs into the system and benefit from the API Management capabilities of the product.\n\nAlternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy.\n\nIn either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI \ndefinition file and publishing it using either API Manager or the command line interface.\n\n#### SOAP API\n\nAPI Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file. \nYou can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables. \nYou can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure.\n\nYou can create SOAP API definitions through either the command line interface, or through the API Manager UI.\n\n#### GraphQL\n\nGraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM® API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs.\nSee [this dedicated section](#graphql-apis).\n\n#### AsyncAPI\n\nWe already address in detail in [previous sections](#asyncapi-documents).\n\n### API Manager\n\nYou can manage your APIs by using API Connect's API Manager UI:\n\n![](./images/apimanagerui.png)\n\nThe API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs.\n\n### Developer Portal\n\nThe Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published, \napplication developers can browse and use the APIs from the Developer Portal dashboard, as shown below:\n\n![](./images/devportal1.jpg)\n\n\nThe Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization. \nYou can configure the Developer Portal for test and development purposes, or for internal use only.\n\n### Create Capability\n\nDeveloper can leverage API Connect's <b>Create</b> capability, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach. \nAPIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database.\n\n#### Creating a REST proxy API from an existing target service\n\nAs mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system.\nEssentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager.\n\n\n### Explore Capability\n\nDevelopers can use API Connect's <b>Explore</b> capability to quickly examine their new APIs and try their operations.\n\nDevelopers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above. \nThis can be done through API Connect's UI or with the provided CLI. \n\nDevelopers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as \nthose that were added based on existing data sources. \n\n### Subscribing to an API in the Developer Portal\n\nTo subscribe to an API, from the Developer Portal, the develper clicks `API Products` to find and subscribe to any available APIs:\n\n![](./images/devportal2.jpg)\n\nIn the example above, an API called <em>FindBranch</em> Version 2.0, which is contained in the <em>FindBranch Auto Products</em> product is available. Clicking ```Subscribe``` enables the developer to use the API:\n\n![](./images/devportal3.jpg)\n\nUnder the Application heading, the developer can click ```Select App``` for the new application:\n\n![](./images/devportal4.jpg)\n\nFrom the next screen below, the developer can click Next:\n\n![](./images/devportal5.jpg)\n\nDoing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing ```Done``` in the next screen completes the subscription.\n\n![](./images/devportal6.jpg)\n\n\n### Testing An API in the Developer Portal\n\nTo test an API, the developer clicks ```API Products``` in the Developer Portal dashboard to show all available products:\n\n![](./images/devportal7.jpg)\n\nClicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. \n\n![](./images/devportal8.jpg)\n\nThe developer can click ```GET/details```, for instance, to see the details of the GET operation: \n\n![](./images/devportal10.jpg)\n\nClicking the ```Try it``` tab and pressing ```Send``` allows the developer to test the API to better understand how it works:\n\n![](./images/devportal11.jpg)\n\nAs you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful.\n\n### API Product Managers\n\nAdministrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. \n\nOnce the APIs are ready, API Product Managers can expose them for developer consumption and self-service in the Developer Portal. Developers who have signed up can discover and use any APIs which were exposed. \n\nBecause the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples\n in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js.\n\n### Testing\n\nBesides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above, it is possible to use <b>API Connect Test and Monitor</b> to effortlessly generate\n and schedule API test assertions. This browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs.\n\nYou can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results:\n\n![](./images/testing1.png)\n\nYou enter any required parameters and authorization token and press `Send` to send the request. \n\nAPI Connect Test and Monitor then shows you the response payload as a formatted JSON object:\n\n![](./images/testing2.png)\n\n\nTo generate a test, you then click `Generate Test`, and in the dialog that appears name the test and add it to a project. \nWhen you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload.\n\nYou can add, delete, or modify these assertions, or even add new ones from a broad selection:\n\n![](./images/assertions2.png)\n\nYou can also reorder the assertions by dragging and dropping them:\n\n![](./images/assertions3.png)\n\nAlthough coding is not required, if you wish, you change the underlying code by clicking `CODE` to enter the code view:\n\n![](./images/testing3.png)\n\nOnce you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures:\n\n![](./images/testrpt.png)\n\n### Test Scheduling\n\nIBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling:\n\n![](./images/testpublish.png)\n\nOnce your test is published, you can click `Schedule` to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests:\n\n![](./images/newrun.png)\n\nClicking `Save Run` schedules the tests. \n\nFinally, you can access the dashboard to monitor the health of your API:\n\n![](./images/dashboard.png)\n\n## GraphQL APIs\n\nIBM® API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. \nThe Developer Portal also supports testing GraphQL APIs. \n\n### Advantages of GraphQL over REST APIs\n\nGraphQL provides the following particular advantages over REST APIs:\n\n* The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details. \nWith a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data.\n* The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds. \nIf an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details, \nthen make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request.\n* However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit.\n\n\n### Creating a GraphQL proxy API\n\nThe  demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click `Add`, then `API (from REST, GraphQL or SOAP)`:\n\n![](./images/graphql1.png)\n\nThen select, `From existing GraphQL service (GraphQL proxy)`:\n\n![](./images/graphql2.png)\n\nGive it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. \n\n![](./images/graphql3.png)\n\nNotice that, once you enter the URL, API Connect will find the service and automatically detect the schema. \n\n![](./images/graphql4.png)\n\nYou can click `Next` and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server:\n\n![](./images/graphql5.png)\n\nThe gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. \n\nClicking Next allows you to then activate the API for publishing:\n\n![](./images/graphql6.png)\n\nFinally, clicking `Next` shows a screen such as the one below with checkmarks:\n\n![](./images/graphql7.png)\n\nThis shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies:\n\n![](./images/graphql8.png)\n\n\n## Further Readings\n\n* [IBM Redbook on Agile Integration](https://www.redbooks.ibm.com/abstracts/sg248452.html)\n* [A Demo of Event Endpoint Management - Cloud Pak for Integration](https://community.ibm.com/community/user/integration/blogs/dale-lane1/2021/04/12/a-demo-of-event-endpoint-management])\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/patterns/api-mgt/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}