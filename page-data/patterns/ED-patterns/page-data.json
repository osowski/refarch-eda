{"componentChunkName":"component---src-pages-patterns-ed-patterns-mdx","path":"/patterns/ED-patterns/","result":{"pageContext":{"frontmatter":{"title":"Understanding event driven microservice patterns","description":"Understanding event driven microservice patterns"},"relativePagePath":"/patterns/ED-patterns.mdx","titleType":"append","MdxNode":{"id":"7e6e6aac-6056-5e80-b2cf-c894219a720b","children":[],"parent":"8440c9ad-a873-5765-bd5e-9dab190ccdc0","internal":{"content":"---\ntitle: Understanding event driven microservice patterns\ndescription: Understanding event driven microservice patterns\n---\n\n!!! abstract\n    In this article, we are detailing some of the most import event-driven patterns to be used during your microservice implementation and when adopting kafka as an event backbone.\n\nAdopting messaging (Pub/Sub) as a microservice communication backbone involves using at least the following patterns:\n\n* [Decompose by subdomain](https://microservices.io/patterns/decomposition/decompose-by-subdomain.html), event driven microservices are still microservices, so we need to find them, and the domain-driven subdomains is a good approach to identify and classify business function and therefore microservices. With the event storming method, aggregates help to find those subdomain of responsibility.\n* [Database per service](https://microservices.io/patterns/data/database-per-service.html) to enforce each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other service when database schema changes. The database technology is selected from business requirements. The implementation of transactions that span multiple services is complex and enforce using the Saga pattern. Queries that goes over multiple entities is a challenge and CQRS represents an interesting solution.\n* [Strangler pattern](#strangler-pattern) is used to incrementally migrate an existing, monolytic application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then aS soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps.\n* [Event sourcing](./event-sourcing/) persists the state of a business entity such an Order as a sequence of state-changing events.\n* [Command Query Responsibility Segregation](./cqrs/) helps to separate queries from commands and help to address queries with cross-microservice boundary.\n* [Saga pattern:](./saga/) Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice interested in other business entities, subscribe to those events and it can update its own states and business entities when receiving such events. Business entity keys needs to be unique, immutable.\n* [Event reprocessing with dead letter](#event-reprocessing-with-dead-letter-pattern): event driven microservice may have to call external service via synchronous call, we need to process failure to get response from those service, using event backbone.\n* [Transactional outbox](https://microservices.io/patterns/data/transactional-outbox.html): A service command typically needs to update the database and send messages/events. The approach is to use a outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone.\n\n\n## Strangler pattern\n\n### Problem\n\nHow to migrate a monolytics application to microservice without doing a big bang, redeveloping the application from white page. Replacing and rewritting an existing application can be a huge investment. Rewritting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes.\n\nThe figure below illustrates a typical mainframe application, with external Java based user interface connected to the mainframe via iop/corba and with three\n different applications to manage product, order and customer.\n\n![](../images/stangler.png)\n\n### Solution\n\nThe approach is to use a \"strangler\" interface to dispatch request to new or old features. Existing features to migrate are selected by trying to isolate sub components.\n\nOne of main challenge is to isolate data store and how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model.\n\nThe facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP).\n\nThe following figure illustrates an implementation using event driven implementation with \ndata replication from the write model to the read model being on the mainframe.\n\n![](../images/strangler-2.png)\n\n## Transaction outbox\n\nSee the documentation of the pattern in Chris Richardson's site / book: [Transactional outbox](https://microservices.io/patterns/data/transactional-outbox.html).\n\nA service command typically needs to update the database and send messages/events. The approach is to use a outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone.\n\n![2](./images/outbox.png)","type":"Mdx","contentDigest":"8f2b1d30c6287e9225a202ef5548547c","counter":305,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Understanding event driven microservice patterns","description":"Understanding event driven microservice patterns"},"exports":{},"rawBody":"---\ntitle: Understanding event driven microservice patterns\ndescription: Understanding event driven microservice patterns\n---\n\n!!! abstract\n    In this article, we are detailing some of the most import event-driven patterns to be used during your microservice implementation and when adopting kafka as an event backbone.\n\nAdopting messaging (Pub/Sub) as a microservice communication backbone involves using at least the following patterns:\n\n* [Decompose by subdomain](https://microservices.io/patterns/decomposition/decompose-by-subdomain.html), event driven microservices are still microservices, so we need to find them, and the domain-driven subdomains is a good approach to identify and classify business function and therefore microservices. With the event storming method, aggregates help to find those subdomain of responsibility.\n* [Database per service](https://microservices.io/patterns/data/database-per-service.html) to enforce each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other service when database schema changes. The database technology is selected from business requirements. The implementation of transactions that span multiple services is complex and enforce using the Saga pattern. Queries that goes over multiple entities is a challenge and CQRS represents an interesting solution.\n* [Strangler pattern](#strangler-pattern) is used to incrementally migrate an existing, monolytic application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then aS soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps.\n* [Event sourcing](./event-sourcing/) persists the state of a business entity such an Order as a sequence of state-changing events.\n* [Command Query Responsibility Segregation](./cqrs/) helps to separate queries from commands and help to address queries with cross-microservice boundary.\n* [Saga pattern:](./saga/) Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice interested in other business entities, subscribe to those events and it can update its own states and business entities when receiving such events. Business entity keys needs to be unique, immutable.\n* [Event reprocessing with dead letter](#event-reprocessing-with-dead-letter-pattern): event driven microservice may have to call external service via synchronous call, we need to process failure to get response from those service, using event backbone.\n* [Transactional outbox](https://microservices.io/patterns/data/transactional-outbox.html): A service command typically needs to update the database and send messages/events. The approach is to use a outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone.\n\n\n## Strangler pattern\n\n### Problem\n\nHow to migrate a monolytics application to microservice without doing a big bang, redeveloping the application from white page. Replacing and rewritting an existing application can be a huge investment. Rewritting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes.\n\nThe figure below illustrates a typical mainframe application, with external Java based user interface connected to the mainframe via iop/corba and with three\n different applications to manage product, order and customer.\n\n![](../images/stangler.png)\n\n### Solution\n\nThe approach is to use a \"strangler\" interface to dispatch request to new or old features. Existing features to migrate are selected by trying to isolate sub components.\n\nOne of main challenge is to isolate data store and how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model.\n\nThe facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP).\n\nThe following figure illustrates an implementation using event driven implementation with \ndata replication from the write model to the read model being on the mainframe.\n\n![](../images/strangler-2.png)\n\n## Transaction outbox\n\nSee the documentation of the pattern in Chris Richardson's site / book: [Transactional outbox](https://microservices.io/patterns/data/transactional-outbox.html).\n\nA service command typically needs to update the database and send messages/events. The approach is to use a outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone.\n\n![2](./images/outbox.png)","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/patterns/ED-patterns.mdx"}}}}