{"componentChunkName":"component---src-pages-patterns-realtime-analytics-index-mdx","path":"/patterns/realtime-analytics/","result":{"pageContext":{"frontmatter":{"title":"Patterns in Event-Driven Architectures","description":"Process continuous streaming events"},"relativePagePath":"/patterns/realtime-analytics/index.mdx","titleType":"append","MdxNode":{"id":"a46a0c3f-ea28-5e9c-8ab3-9fc17bc6736c","children":[],"parent":"2900f6e1-7063-5bc3-bac4-37bb47304f03","internal":{"content":"---\ntitle: Patterns in Event-Driven Architectures\ndescription: Process continuous streaming events\n---\n\n# Realtime Analytics\n\nOne of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence.\n\nIn this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture.\n\n## Streaming analytics (real-time analytics)\n\n[Streaming analytics](https://cloud.ibm.com/catalog/services/streaming-analytics) provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities:\n\n* **Ingest** many sources of events.\n* **Prepare** data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment.\n* **Detect and predict** event patterns using scoring and classification.\n* **Decide** by applying business rules and business logic.\n* **Act** by directly executing an action, or in event-driven systems publishing an event notification or command.\n\n![1](./images/rt-analytics-app-pattern.png)\n\n### Basic streaming analytics capabilities\n\nTo support the real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component:\n\n* Continuous event ingestion and analytical processing.\n* Processing across multiple event streams.\n* Low latency processing, where data do not have to be stored.\n* Processing of high-volume and high-velocity streams of data.\n* Continuous query and analysis of the feed.\n* Correlation across events and streams.\n* Windowing and stateful processing.\n* Query and analysis of stored data.\n* Development and execution of data pipelines.\n* Development and execution of analytics pipelines.\n* Scoring of machine learning models in line in the real-time event stream processing.\n\n### Support for real-time analytics and decision-making\n\nBeyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time.\n\nThese capabilities include the following:\n\n* Geospatial\n  * Location-based analytics\n  * Geofencing & map matching\n  * Spatio-temporal hangout detection\n* Time series analysis\n  * Timestamped data analysis\n  * Anomaly detection & forecasting\n* Text analytics\n  * Natural Language Processing & Natural Language Understanding\n  * Sentiment analysis & entity extraction\n* Video and audio\n  * Speech-to-text conversion\n  * Image recognition\n* Rules\n  * Decisions described as business logic\n* Complex Event Processing (CEP)\n  * Temporal pattern detection\n* Entity Analytics\n  * Relationships between entities\n  * Probabilistic matching\n\n### Application programming languages and standards\n\nFew standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform.  The commonly used languages include the following:\n* Python supports working with data and is popular with data scientists and data engineers.\n* Java is the pervasive application development language. Kafka Streams offers a DSL to support most of the event streaming processing implementation.\n* Scala adds functional programming and immutable objects to Java.\n\nOther platform specific languages have emerged when real-time processing demands stringent performance requirements real time processing performance is required.\n\nMore recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications.\n\nBeam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL.\n\nStreaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine.\n\nSee https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities.  Leading engines include Google Cloud DataFlow, [Apache Flink](https://flink.apache.org/), Apache Spark, Apache Apex, and IBM Streams.\n\n### Run time characteristics\n\nIn operational terms streaming analytics engines must receive and analyze arriving data continuously:\n\n* The \"Feed Never Ends\"\n  * The collection is unbounded.\n  * Not a request response set based model.\n\n* The \"Firehose Doesn’t Stop\"\n  * Keep drinking and keep up.\n  * The processing rate is greater than or equal to the feed rate.\n  * The analytics engine must be resilient and self-healing.\n\nThese specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams.\n\n## Products\n\n### Streaming Analytics\n\nThe market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together.  The potential product selection list for the streaming analytics component in the event driven architecture would need to consider:\n\nTop Open Source projects:\n* Flink - real time streaming engine, both real time and batch analytics in one tool.\n* Spark Streaming - micro batch processing through spark engine.\n* Storm - Has not shown enough adoption.\n* Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators\n\nMajor Cloud Platform Providers support:\n* Google Cloud DataFlow – proprietary engine open source streams application language ( Beam )\n* Azure Stream Analytics – proprietary engine , SQL interface\n* Amazon Kinesis - proprietary AWS\n\nIBM offerings\n* IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads)\n* IBM Event streams (Kafka based event log/streaming platform)\n\nEvaluation of the various options, highlights\n* The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams.\n* Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics.\n\nOur decision for the Event Driven Architecture is to include:\n\n* IBM streams as the performant, functionally rich real time event stream processing engine\n* Event Streams ([Kafka Streams](../../technology/kafka-streams/)), for manipulation of event streams within microservices\n\nIBM streams also supports Apache Beam as the open source Streams Application language,  which would allow portability of streams applications across, Flink, Spark, Google DataFlow..\n\n## An anomaly detection in event-driven solutions\n\nWe have implemented a separate solution based on the fresh food delivery to do anomaly detection for Refrigerated container in [this repository](https://github.com/ibm-cloud-architecture/refarch-reefer-ml) with the following high level component view:\n\n![2](./images/cp-solution-view.png)\n","type":"Mdx","contentDigest":"b21d214b7abf47d44d54fb6b0534d825","counter":381,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Patterns in Event-Driven Architectures","description":"Process continuous streaming events"},"exports":{},"rawBody":"---\ntitle: Patterns in Event-Driven Architectures\ndescription: Process continuous streaming events\n---\n\n# Realtime Analytics\n\nOne of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence.\n\nIn this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture.\n\n## Streaming analytics (real-time analytics)\n\n[Streaming analytics](https://cloud.ibm.com/catalog/services/streaming-analytics) provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities:\n\n* **Ingest** many sources of events.\n* **Prepare** data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment.\n* **Detect and predict** event patterns using scoring and classification.\n* **Decide** by applying business rules and business logic.\n* **Act** by directly executing an action, or in event-driven systems publishing an event notification or command.\n\n![1](./images/rt-analytics-app-pattern.png)\n\n### Basic streaming analytics capabilities\n\nTo support the real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component:\n\n* Continuous event ingestion and analytical processing.\n* Processing across multiple event streams.\n* Low latency processing, where data do not have to be stored.\n* Processing of high-volume and high-velocity streams of data.\n* Continuous query and analysis of the feed.\n* Correlation across events and streams.\n* Windowing and stateful processing.\n* Query and analysis of stored data.\n* Development and execution of data pipelines.\n* Development and execution of analytics pipelines.\n* Scoring of machine learning models in line in the real-time event stream processing.\n\n### Support for real-time analytics and decision-making\n\nBeyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time.\n\nThese capabilities include the following:\n\n* Geospatial\n  * Location-based analytics\n  * Geofencing & map matching\n  * Spatio-temporal hangout detection\n* Time series analysis\n  * Timestamped data analysis\n  * Anomaly detection & forecasting\n* Text analytics\n  * Natural Language Processing & Natural Language Understanding\n  * Sentiment analysis & entity extraction\n* Video and audio\n  * Speech-to-text conversion\n  * Image recognition\n* Rules\n  * Decisions described as business logic\n* Complex Event Processing (CEP)\n  * Temporal pattern detection\n* Entity Analytics\n  * Relationships between entities\n  * Probabilistic matching\n\n### Application programming languages and standards\n\nFew standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform.  The commonly used languages include the following:\n* Python supports working with data and is popular with data scientists and data engineers.\n* Java is the pervasive application development language. Kafka Streams offers a DSL to support most of the event streaming processing implementation.\n* Scala adds functional programming and immutable objects to Java.\n\nOther platform specific languages have emerged when real-time processing demands stringent performance requirements real time processing performance is required.\n\nMore recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications.\n\nBeam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL.\n\nStreaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine.\n\nSee https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities.  Leading engines include Google Cloud DataFlow, [Apache Flink](https://flink.apache.org/), Apache Spark, Apache Apex, and IBM Streams.\n\n### Run time characteristics\n\nIn operational terms streaming analytics engines must receive and analyze arriving data continuously:\n\n* The \"Feed Never Ends\"\n  * The collection is unbounded.\n  * Not a request response set based model.\n\n* The \"Firehose Doesn’t Stop\"\n  * Keep drinking and keep up.\n  * The processing rate is greater than or equal to the feed rate.\n  * The analytics engine must be resilient and self-healing.\n\nThese specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams.\n\n## Products\n\n### Streaming Analytics\n\nThe market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together.  The potential product selection list for the streaming analytics component in the event driven architecture would need to consider:\n\nTop Open Source projects:\n* Flink - real time streaming engine, both real time and batch analytics in one tool.\n* Spark Streaming - micro batch processing through spark engine.\n* Storm - Has not shown enough adoption.\n* Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators\n\nMajor Cloud Platform Providers support:\n* Google Cloud DataFlow – proprietary engine open source streams application language ( Beam )\n* Azure Stream Analytics – proprietary engine , SQL interface\n* Amazon Kinesis - proprietary AWS\n\nIBM offerings\n* IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads)\n* IBM Event streams (Kafka based event log/streaming platform)\n\nEvaluation of the various options, highlights\n* The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams.\n* Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics.\n\nOur decision for the Event Driven Architecture is to include:\n\n* IBM streams as the performant, functionally rich real time event stream processing engine\n* Event Streams ([Kafka Streams](../../technology/kafka-streams/)), for manipulation of event streams within microservices\n\nIBM streams also supports Apache Beam as the open source Streams Application language,  which would allow portability of streams applications across, Flink, Spark, Google DataFlow..\n\n## An anomaly detection in event-driven solutions\n\nWe have implemented a separate solution based on the fresh food delivery to do anomaly detection for Refrigerated container in [this repository](https://github.com/ibm-cloud-architecture/refarch-reefer-ml) with the following high level component view:\n\n![2](./images/cp-solution-view.png)\n","fileAbsolutePath":"/home/runner/work/refarch-eda/refarch-eda/docs/src/pages/patterns/realtime-analytics/index.mdx"}}}}