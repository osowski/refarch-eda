{"version":3,"sources":["webpack:///./src/pages/use-cases/kafka-streams/lab-3/index.mdx"],"names":["_frontmatter","makeShortcode","name","props","console","warn","InlineNotification","AnchorLinks","AnchorLink","layoutProps","MDXLayout","DefaultLayout","MDXContent","components","mdxType","kind","parentName","isMDXComponent"],"mappings":"0PAMaA,G,UAAe,IAEtBC,EAAgB,SAAAC,GAAI,OAAI,SAA6BC,GAEzD,OADAC,QAAQC,KAAK,cAAgBH,EAAO,4EAC7B,kBAASC,KAGZG,EAAqBL,EAAc,sBACnCM,EAAcN,EAAc,eAC5BO,EAAaP,EAAc,cAC3BQ,EAAc,CAClBT,gBAEIU,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGV,EACF,8BACD,OAAO,YAACO,EAAD,eAAeD,EAAiBN,EAAhC,CAAuCU,WAAYA,EAAYC,QAAQ,cAG5E,YAACR,EAAD,CAAoBS,KAAK,UAAUD,QAAQ,sBACzC,8CADF,4DAGA,YAACP,EAAD,CAAaO,QAAQ,eACrB,YAACN,EAAD,CAAYM,QAAQ,cAApB,YACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,0BACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,2BACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,uBACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,qBACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,uBACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,0BAEA,kCACA,wDAAuC,6BAAGE,WAAW,KAAQ,CACzD,KAAQ,uBAD2B,WAAvC,qFAGA,yDACA,sBACE,kBAAIA,WAAW,MAAf,6FACA,kBAAIA,WAAW,MAAf,uDACA,kBAAIA,WAAW,MAAf,yCACA,kBAAIA,WAAW,MAAf,qDACA,kBAAIA,WAAW,MAAf,wDAEF,oFACA,yBAAQ,gCAAMA,WAAW,KAAQ,CAC7B,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPV,WAUJ,gCAAMA,WAAW,QAAW,CAC1B,UAAa,qCACb,MAAS,CACP,cAAiB,qBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,grBACnB,eAAkB,QAClB,QAAW,YAnBX,OAsBR,+BAAKA,WAAW,QAAW,CACrB,UAAa,0BACb,IAAO,IACP,MAAS,IACT,IAAO,qFACP,OAAU,CAAC,0FAA2F,0FAA2F,2FACjM,MAAS,kCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,UAtCT,WAyCR,yQACA,iDAAgC,6BAAGA,WAAW,KAAQ,CAClD,KAAQ,oDADoB,wBAAhC,iEAGA,2LAA0K,0BAAYA,WAAW,KAAvB,0BAA1K,KACA,iDACA,qBAAG,sBAAQA,WAAW,KAAnB,SACH,sBACE,kBAAIA,WAAW,MAAf,oDACA,kBAAIA,WAAW,MAAf,kBAEF,qBAAG,sBAAQA,WAAW,KAAnB,eACH,qBAAG,sBAAQA,WAAW,KAAnB,UACH,sBACE,kBAAIA,WAAW,MAAf,+GAGF,qBAAG,sBAAQA,WAAW,KAAnB,0BACH,sBACE,kBAAIA,WAAW,MAAf,sEAEF,qBAAG,sBAAQA,WAAW,KAAnB,iCACH,sBACE,kBAAIA,WAAW,MAAf,WAEF,qBAAG,sBAAQA,WAAW,KAAnB,kCACH,sBACE,kBAAIA,WAAW,MAAf,eAEF,qBAAG,sBAAQA,WAAW,KAAnB,sBACH,sBACE,kBAAIA,WAAW,MAAf,mMAEF,qBAAG,sBAAQA,WAAW,KAAnB,eAAH,yCAA4F,0BAAYA,WAAW,KAAvB,kFAA5F,KACA,yEAAwD,6BAAGA,WAAW,KAAQ,CAC1E,KAAQ,yEAD4C,wEAAxD,KAGA,+CACA,oIAAmH,6BAAGA,WAAW,KAAQ,CACrI,KAAQ,qDADuG,gBAAnH,aAEuC,6BAAGA,WAAW,KAAQ,CACzD,KAAQ,yEAD2B,qBAFvC,cAKA,iDACA,sGACA,4DACA,sBACE,kBAAIA,WAAW,MAAf,mFAAwG,6BAAGA,WAAW,MAAS,CAC3H,KAAQ,6DAD4F,+BAAxG,4BAIF,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,uRAOL,4CAA2B,0BAAYA,WAAW,KAAvB,qCAA3B,wBACA,qBAAG,kBAAIA,WAAW,KAAf,4EAAgG,0BAAYA,WAAW,MAAvB,uDACnG,4CACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,yBAIL,yCAAwB,6BAAGA,WAAW,KAAQ,CAC1C,KAAQ,qCADY,iCAAxB,uGAEkJ,6BAAGA,WAAW,KAAQ,CACpK,KAAQ,iCADsI,iCAGlJ,sBACE,kBAAIA,WAAW,MAAf,6BAAkD,0BAAYA,WAAW,MAAvB,0BAAlD,UAAgI,0BAAYA,WAAW,MAAvB,iBAAhI,aAEF,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wtBA0BL,qCAAoB,6BAAGA,WAAW,KAAQ,CACtC,KAAQ,qCADQ,oCAApB,oCAGA,kEAAiD,0BAAYA,WAAW,KAAvB,0BAAjD,SACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,wOASL,qHAAoG,6BAAGA,WAAW,KAAQ,CACtH,KAAQ,yCADwF,UAApG,iEAEqF,6BAAGA,WAAW,KAAQ,CACvG,KAAQ,sBADyE,UAFrF,uIAKA,uDACA,2LAA0K,0BAAYA,WAAW,KAAvB,eAA1K,0CAA4Q,0BAAYA,WAAW,KAAvB,0BAA5Q,KACA,8KACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,sDAIL,sEAAqD,0BAAYA,WAAW,KAAvB,kBAArD,4MACA,qBAAG,0BAAYA,WAAW,KAAvB,oFACH,4EACA,qBAAG,6BAAGA,WAAW,KAAQ,CACrB,KAAQ,sJADT,sJAGH,oDACA,kCAAiB,0BAAYA,WAAW,KAAvB,2BAAjB,oFACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,uXAkBL,8EAA6D,0BAAYA,WAAW,KAAvB,SAA7D,wFACA,yFACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,4PAaL,wKACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,4kBAmBL,0JACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wZAaL,2EACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wEAQL,qHACA,2CACA,6GACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,gEAIL,gHAA+F,0BAAYA,WAAW,KAAvB,WAA/F,gDACkC,0BAAYA,WAAW,KAAvB,WADlC,2BAEA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,iBADZ,yLASL,iHACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,keAaL,qDACA,gGAA+E,0BAAYA,WAAW,KAAvB,oBAA/E,2DAEA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,qVAcL,oGACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,gOAKL,iDAAgC,0BAAYA,WAAW,KAAvB,mBAAhC,uGACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,iOAIL,yMACA,kDACA,2HACA,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,qMACA,mBAAKA,WAAW,MAAK,gCAAMA,WAAW,OAAU,CAC5C,UAAa,mBADI,wLAOvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,sGAA0H,0BAAYA,WAAW,KAAvB,iBAA1H,+EAAmQ,6BAAGA,WAAW,KAAQ,CACrR,KAAQ,oEADuP,6BAAnQ,gCAEuE,6BAAGA,WAAW,KAAQ,CACzF,KAAQ,2EAD2D,sBAFvE,MAMF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,uCAA2D,0BAAYA,WAAW,KAAvB,SAA3D,oBAAiI,6BAAGA,WAAW,KAAQ,CACnJ,KAAQ,8FADqH,uBAGjI,mBAAKA,WAAW,MAAK,gCAAMA,WAAW,OAAU,CAC5C,UAAa,mBADI,uGAMvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,cAAkC,0BAAYA,WAAW,KAAvB,0BAAlC,2OAGJ,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,qlBAcL,wFAAuE,0BAAYA,WAAW,KAAvB,QAAvE,wBAAgJ,0BAAYA,WAAW,KAAvB,aAAhJ,oEAA0Q,0BAAYA,WAAW,KAAvB,SAA1Q,sFACA,oEACA,uBAAK,gCAAMA,WAAW,OAAU,IAA3B,g0BAaL,sBACE,kBAAIA,WAAW,MAAf,uBAA4C,0BAAYA,WAAW,MAAvB,QAA5C,mGAEF,uBAAK,gCAAMA,WAAW,OAAU,IAA3B,mNAOL,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,mCACA,mBAAKA,WAAW,MAAK,gCAAMA,WAAW,OAAU,CAC5C,UAAa,mBADI,sCAKrB,iBAAGA,WAAW,MAAd,yEACA,mBAAKA,WAAW,MAAK,gCAAMA,WAAW,OAAU,IAA3B,ivBAiBzB,2DACA,0DAAyC,0BAAYA,WAAW,KAAvB,uBAAzC,0gBACA,kEACA,sBACE,kBAAIA,WAAW,MAAf,8DACA,kBAAIA,WAAW,MAAf,4DACA,kBAAIA,WAAW,MAAf,8EAEF,4BAAW,0BAAYA,WAAW,KAAvB,uBAAX,+KACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,ynCAiCL,sBACE,kBAAIA,WAAW,MAAf,+EACA,kBAAIA,WAAW,MAAf,+TACA,kBAAIA,WAAW,MAAf,4HACA,kBAAIA,WAAW,MAAf,sEAEF,+DACA,sBACE,kBAAIA,WAAW,MAAf,+FAEF,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,o0BAkCL,sBACE,kBAAIA,WAAW,MAAf,wEAEF,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,otCAiCL,0DACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,+VAOL,uCACA,0HAAyG,6BAAGA,WAAW,KAAQ,CAC3H,KAAQ,yHAD6F,yBAAzG,4DAGA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,ydAaL,mCAAkB,0BAAYA,WAAW,KAAvB,gBAAlB,oEACA,6CACA,4CAA2B,6BAAGA,WAAW,KAAQ,CAC7C,KAAQ,2DADe,aAA3B,uMAIA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,ibAiBL,qDACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,8ZAcL,qbACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,whBAkBL,6BACA,8GAA6F,0BAAYA,WAAW,KAAvB,cAA7F,iDACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,0MASL,qBAAG,kBAAIA,WAAW,KAAf,sKAA0L,0BAAYA,WAAW,MAAvB,mFAA1L,yRAAH,KACA,2CACA,yIAAwH,6BAAGA,WAAW,KAAQ,CAC1I,KAAQ,4FAD4G,mDAAxH,KAGA,6CACA,6CAA4B,6BAAGA,WAAW,KAAQ,CAC9C,KAAQ,6FADgB,4BAA5B,0DAGA,+IAA8H,kBAAIA,WAAW,MAA7I,uFAEA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,q1BAeL,gHACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,80BA6BL,8JACA,uBAAK,gCAAMA,WAAW,OAAU,IAA3B,oSAKL,yCACA,uBAAK,gCAAMA,WAAW,OAAU,IAA3B,kRAUL,oFACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,4DAIL,6JAA4I,6BAAGA,WAAW,KAAQ,CAC9J,KAAQ,gDADgI,WAA5I,KAGA,+CACA,kOAAiN,6BAAGA,WAAW,KAAQ,CACnO,KAAQ,mMADqM,+BAAjN,MAGA,8MACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,2cAeL,6DACA,iGACA,uBAAK,gCAAMA,WAAW,OAAU,IAA3B,yBAEL,iHACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,oOASL,+DACA,8DAA6C,0BAAYA,WAAW,KAAvB,kBAA7C,sBACc,0BAAYA,WAAW,KAAvB,iDADd,gBACwH,0BAAYA,WAAW,KAAvB,WADxH,0CAEwB,kBAAIA,WAAW,KAAf,YAFxB,4CAEyG,0BAAYA,WAAW,KAAvB,cAFzG,eAGA,8DAA6C,0BAAYA,WAAW,KAAvB,kEAA7C,KACA,iDACA,8MACA,uBAAK,gCAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,+DAIL,2DAA0C,0BAAYA,WAAW,KAAvB,gCAA1C,cACA,4MACA,oEAAmD,6BAAGA,WAAW,KAAQ,CACrE,KAAQ,yDADuC,+BAAnD,MAOJJ,EAAWK,gBAAiB","file":"component---src-pages-use-cases-kafka-streams-lab-3-index-mdx-3051af96de14c977aa13.js","sourcesContent":["import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/runner/work/refarch-eda/refarch-eda/docs/node_modules/gatsby-theme-carbon/src/templates/Default.js\";\nexport const _frontmatter = {};\n\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component '\" + name + \"' was not imported, exported, or provided by MDXProvider as global scope\");\n  return <div {...props} />;\n};\n\nconst InlineNotification = makeShortcode(\"InlineNotification\");\nconst AnchorLinks = makeShortcode(\"AnchorLinks\");\nconst AnchorLink = makeShortcode(\"AnchorLink\");\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <InlineNotification kind=\"warning\" mdxType=\"InlineNotification\">\n      <strong>Work in progress</strong> Updated 09/23/2020 - End to end testing could be better\n    </InlineNotification>\n    <AnchorLinks mdxType=\"AnchorLinks\">\n    <AnchorLink mdxType=\"AnchorLink\">Overview</AnchorLink>\n    <AnchorLink mdxType=\"AnchorLink\">Scenario Prerequisites</AnchorLink>\n    <AnchorLink mdxType=\"AnchorLink\">Develop the application</AnchorLink>\n    <AnchorLink mdxType=\"AnchorLink\">Interactive queries</AnchorLink>\n    <AnchorLink mdxType=\"AnchorLink\">Integration Tests</AnchorLink>\n    <AnchorLink mdxType=\"AnchorLink\">Deploy to OpenShift</AnchorLink>\n    <AnchorLink mdxType=\"AnchorLink\">Another item producer</AnchorLink>\n    </AnchorLinks>\n    <h2>{`Overview`}</h2>\n    <p>{`In this lab, we’re going to use `}<a parentName=\"p\" {...{\n        \"href\": \"https://quarkus.io\"\n      }}>{`Quarkus`}</a>{` to develop the logic with Kafka streams api and microprofile reactive messaging.`}</p>\n    <p>{`The requirements to address are:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`consume item sold from items topic, item has unique key. Item event has store information`}</li>\n      <li parentName=\"ul\">{`compute for each item its current stock cross store`}</li>\n      <li parentName=\"ul\">{`compute the store stock for each item`}</li>\n      <li parentName=\"ul\">{`generate inventory event for store - item - stock`}</li>\n      <li parentName=\"ul\">{`expose APIs to get stock for a store or for an item`}</li>\n    </ul>\n    <p>{`Here is a simple diagram to illustrate the components used:`}</p>\n    <p>{` `}<span parentName=\"p\" {...{\n        \"className\": \"gatsby-resp-image-wrapper\",\n        \"style\": {\n          \"position\": \"relative\",\n          \"display\": \"block\",\n          \"marginLeft\": \"auto\",\n          \"marginRight\": \"auto\",\n          \"maxWidth\": \"736px\"\n        }\n      }}>{`\n      `}<span parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-background-image\",\n          \"style\": {\n            \"paddingBottom\": \"42.36111111111111%\",\n            \"position\": \"relative\",\n            \"bottom\": \"0\",\n            \"left\": \"0\",\n            \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABnElEQVQoz12SyW4TQRCG/bQcuHHjwlsgJRcuHEGASKQgc8AHgmIRxRbB9ogo3iCWJx57Fs/SM+Pu/qgekxAo6S91Vdf2V3frtNvja6fNxacOC8+DLINtKoghibFxIIgEMTqKGhg5b32fYDIjms8JZzPUcglFQeuHd0rvssPnL12uplOc9PwOzy8ecdB/zIvBU87895QaKmNRugnhqP+Tg5cfOTk54vXbd3ij0b4gWc6daG2oKmh/O+Ow+4w3V084vn7F4HZIVe6w1mCNQRRK7DTJybOUVFjVSgmrLS0ThUz8mCRX9GYBUaruG/hxxehmQ5TVDH6FGCm207qBMZp/RO5sktByu9kkW5I0p30+JFUVZalkfRF975rv4xtm/poP3UuUUNrnGuqdm9j+hTTBFXTKiXOKxulaqJd1TVVXDyax4tdoidu5afhPXL4raNZrtPDX0v0OMiLNMgXODhYLkiBobCuxVu7Ng/gGabqnXIQh5WaDEpRyLqTBSr7Brbx4IF9iOR4zHQ6Zyyv6k8m9P1utmvjyT56SvFoV/AaWvlsqWlP79gAAAABJRU5ErkJggg==')\",\n            \"backgroundSize\": \"cover\",\n            \"display\": \"block\"\n          }\n        }}></span>{`\n  `}<img parentName=\"span\" {...{\n          \"className\": \"gatsby-resp-image-image\",\n          \"alt\": \"1\",\n          \"title\": \"1\",\n          \"src\": \"/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/d2d67/item-aggregator-ctx.png\",\n          \"srcSet\": [\"/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/7fc1e/item-aggregator-ctx.png 288w\", \"/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/a5df1/item-aggregator-ctx.png 576w\", \"/refarch-eda/static/f3188b6e60b2cdd96da5618eff13295b/d2d67/item-aggregator-ctx.png 736w\"],\n          \"sizes\": \"(max-width: 736px) 100vw, 736px\",\n          \"style\": {\n            \"width\": \"100%\",\n            \"height\": \"100%\",\n            \"margin\": \"0\",\n            \"verticalAlign\": \"middle\",\n            \"position\": \"absolute\",\n            \"top\": \"0\",\n            \"left\": \"0\"\n          },\n          \"loading\": \"lazy\"\n        }}></img>{`\n    `}</span></p>\n    <p>{`The goal of this lab, is to develop the green component which exposes an API to support Kafka stream interactive queries on top of the aggregates save in state store (light blue storage/per service deployed and persisted in kafka as topic).`}</p>\n    <p>{`We will be testing using `}<a parentName=\"p\" {...{\n        \"href\": \"https://kafka.apache.org/documentation/streams/\"\n      }}>{`Apache Kafka Streams`}</a>{` TopologyTestDriver to mimic a Topology, a Stream and Table. `}</p>\n    <p>{`This application is deployed to OpenShift cluster with Event Streams running. We use the quarkus kubernetes plugin with all the needed definitions are done in the `}<inlineCode parentName=\"p\">{`application.properties`}</inlineCode>{`.`}</p>\n    <h2>{`Scenario Pre-requisites`}</h2>\n    <p><strong parentName=\"p\">{`Java`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`For the purposes of this lab we suggest Java 11+`}</li>\n      <li parentName=\"ul\">{`Quarkus 1.8.1`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Git client`}</strong></p>\n    <p><strong parentName=\"p\">{`Maven`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`Maven will be needed for bootstrapping our application from the command-line and running\nour application.`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`An IDE of your choice`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`Ideally an IDE that supports Quarkus (such as Visual Studio Code)`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`OpenShift Container Platform`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`v4.4.x`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`IBM Cloud Pak for Integration`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`CP4I2020.2`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`IBM Event Streams`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`The lab uses Event Streams v10 on Cloud Pack for Integration. If using a previous version such as ESv2019.4.2, there are some differences to how to establish connection to the Kafka brokers.`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Code Source`}</strong>{`: clone the following git repository: `}<inlineCode parentName=\"p\">{`git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory`}</inlineCode>{`.`}</p>\n    <p>{`The final source code is in this Git repository: `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory\"\n      }}>{`https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory`}</a>{`.`}</p>\n    <h2>{`Use application as-is`}</h2>\n    <p>{`If you do not want to develop the application, you can deploy the esisting final app on OpenShift using our `}<a parentName=\"p\" {...{\n        \"href\": \"https://hub.docker.com/r/ibmcase/item-aggregator\"\n      }}>{`docker image`}</a>{`. See the `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory\"\n      }}>{`repository readme`}</a>{` to do so.`}</p>\n    <h2>{`Develop the application`}</h2>\n    <p>{`You can use the code of the cloned repository as source for your development.`}</p>\n    <h3>{`Setting up the Quarkus Application`}</h3>\n    <ul>\n      <li parentName=\"ul\">{`We will bootstrap the Quarkus application with the following Maven command (See `}<a parentName=\"li\" {...{\n          \"href\": \"https://quarkus.io/guides/maven-tooling#project-creation\"\n        }}>{`Quarkus maven tooling guide`}</a>{` for more information):`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`mvn io.quarkus:quarkus-maven-plugin:1.8.1.Final:create \\\\\n    -DprojectGroupId=ibm.garage \\\\\n    -DprojectArtifactId=quarkus-kstreams-lab3 \\\\\n    -Dextensions=\"resteasy-jsonb, quarkus-resteasy-mutiny,smallrye-health,quarkus-smallrye-openapi,openshift,kubernetes-config\"\n`}</code></pre>\n    <p>{`You can replace the `}<inlineCode parentName=\"p\">{`projectGroupId, projectArtifactId`}</inlineCode>{` fields as you like.`}</p>\n    <p><em parentName=\"p\">{`Recall that is if you want to add a quarkus extension do something like: `}<inlineCode parentName=\"em\">{`./mvnw quarkus:add-extension -Dextensions=\"kafka\"`}</inlineCode></em></p>\n    <h3>{`Start the dev mode`}</h3>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`./mvnw quarkus:dev\n`}</code></pre>\n    <p>{`Going to the URL `}<a parentName=\"p\" {...{\n        \"href\": \"http://localhost:8080/swagger-ui\"\n      }}>{`http://localhost:8080/swagger`}</a>{`. The API is empty but health and open API are predefined due to Quarkus plugins. Health works too: `}<a parentName=\"p\" {...{\n        \"href\": \"http://localhost:8080/health\"\n      }}>{`http://localhost:8080/health`}</a></p>\n    <ul>\n      <li parentName=\"ul\">{`Let add a simple resource `}<inlineCode parentName=\"li\">{`InventoryResource.java`}</inlineCode>{` under `}<inlineCode parentName=\"li\">{`src/main/java`}</inlineCode>{` folder.`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-java\"\n      }}>{`package ibm.garage.lab3.api;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\n\nimport io.smallrye.mutiny.Uni;\nimport io.vertx.core.json.JsonObject;\n\n@ApplicationScoped\n@Path(\"/inventory\")\npublic class InventoryResource {\n    \n    @GET\n    @Path(\"/store/{storeID}\")\n    @Produces(MediaType.APPLICATION_JSON)\n    public  Uni<JsonObject> getStock(@PathParam(\"storeID\") String storeID) {\n            JsonObject stock = new JsonObject(\"{\\\\\"name\\\\\": \\\\\"hello you\\\\\", \\\\\"id\\\\\": \\\\\"\" + storeID + \"\\\\\"}\");\n            return Uni.createFrom().item( stock);\n    }\n}\n`}</code></pre>\n    <p>{`A refresh on `}<a parentName=\"p\" {...{\n        \"href\": \"http://localhost:8080/swagger-ui\"\n      }}>{`http://localhost:8080/swagger-ui`}</a>{` should bring you a working API.`}</p>\n    <p>{`Let add a minimum of configuration to the `}<inlineCode parentName=\"p\">{`application.properties`}</inlineCode>{` file`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`quarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n\nquarkus.log.console.level=INFO\nquarkus.log.console.enable=true\nquarkus.http.port=8080\nquarkus.swagger-ui.always-include=true\nquarkus.openshift.expose=true\n`}</code></pre>\n    <p>{`To move from imperative programming to a more reactive approach, we are using Uni class from `}<a parentName=\"p\" {...{\n        \"href\": \"https://smallrye.io/smallrye-mutiny/\"\n      }}>{`Mutiny`}</a>{` to get our API being asynchronous non-blocking: Quarkus uses `}<a parentName=\"p\" {...{\n        \"href\": \"https://vertx.io/\"\n      }}>{`Vert.x`}</a>{` to support non-blocking IO programming model and Mutiny is another abstraction to manage mono or multi elements in a reactive way.`}</p>\n    <h3>{`Deploy to OpenShift using s2i`}</h3>\n    <p>{`Before going too far in the development, let deploy this simple app to OpenShift using the source to image capability. We assume you are logged to the cluster via `}<inlineCode parentName=\"p\">{`oc login...`}</inlineCode>{`, and that you have created a project: `}<inlineCode parentName=\"p\">{`oc new-project jb-lab3`}</inlineCode>{`.`}</p>\n    <p>{`The following command should package the application and create OpenShift manifests, build a docker images and push it to OpenShift Private registry.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`./mvnw package -Dquarkus.kubernetes.deploy=true\n`}</code></pre>\n    <p>{`It can take some seconds to build and deploy: `}<inlineCode parentName=\"p\">{`oc get pods -w`}</inlineCode>{` lets you see the build pods and the running app once the build is done. As we expose the application an OpenShift route was created. The url is visible at the end of the build output, something like:`}</p>\n    <p><inlineCode parentName=\"p\">{`...The deployed application can be accessed at: http://quarkus-kstreams-lab3...`}</inlineCode></p>\n    <p>{`For example this was the URL to access the swagger:`}</p>\n    <p><a parentName=\"p\" {...{\n        \"href\": \"http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/\"\n      }}>{`http://quarkus-kstreams-lab3-jbsandbox.gse-eda-demo-2020-08-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud/swagger-ui/`}</a></p>\n    <h3>{`Define the domain entities`}</h3>\n    <p>{`Under the `}<inlineCode parentName=\"p\">{`src/main/java/../domain`}</inlineCode>{` folder add the two classes representing the business entities we will be using:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{`package ibm.gse.eda.inventory.domain;\nimport java.time.LocalDateTime;\n\npublic class Item {\n    public static String RESTOCK = \"RESTOCK\";\n    public static String SALE = \"SALE\";\n    public String storeName;\n    public String sku;\n    public int quantity;\n    public String type;\n    public Double price;\n    public String timestamp;\n\n    public Item(){}\n}\n`}</code></pre>\n    <p>{`This item will also being used for event structure on `}<inlineCode parentName=\"p\">{`items`}</inlineCode>{` topic. The type attribute is to specify if this is a sale event or a restock event.`}</p>\n    <p>{`The inventory per store includes a map of item.sku and quantity.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{`package ibm.gse.eda.inventory.domain;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class Inventory {\n    public String storeName;\n    public HashMap<String,Long> stock = new HashMap<String,Long>();\n    public Inventory(){}\n}\n`}</code></pre>\n    <p>{`As part of the logic we want to add methods in the Inventory class to update the quantity given an item. So the two following methods are added`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{`public Inventory updateStockQuantity(String k, Item newValue) {\n        this.storeName = k;\n        if (newValue.type.equals(\"SALE\")) \n            newValue.quantity=-newValue.quantity;\n        return this.updateStock(newValue.sku,newValue.quantity);\n    }\n\n    public Inventory updateStock(String sku, long newV) {\n        if (stock.get(sku) == null) {\n            stock.put(sku, Long.valueOf(newV));\n        } else {\n            Long currentValue = stock.get(sku);\n            stock.put(sku, Long.valueOf(newV) + currentValue );\n        }\n        return this;\n    }\n`}</code></pre>\n    <p>{`Modify the InventoryResource to return the inventory instead of JsonObject (we will connect interactive query later in this lab).`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-java\"\n      }}>{`public  Uni<Inventory> getStock(@PathParam(\"storeID\") String storeID) {\n        Inventory stock = new Inventory();\n        stock.storeName = storeID;\n        Item newItem = new Item();\n        newItem.quantity = 10;\n        newItem.sku=\"item-01\";\n        newItem.type = Item.RESTOCK;\n        stock.updateStockQuantity(storeID, newItem);\n            return Uni.createFrom().item( stock);\n    }\n`}</code></pre>\n    <p>{`You should get a json document like the following:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\"stock\": {\n    \"item-01\": 10\n  },\n  \"storeName\": \"Store-A\"\n}\n`}</code></pre>\n    <p>{`Now we are good with the REST end point. Lets add Kafka-streams to connect to Event Streams.`}</p>\n    <h3>{`Add Kafka streams`}</h3>\n    <p>{`We need Kafka and Kafka streams plugins use the following commands to add extension:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`./mvnw quarkus:add-extension -Dextensions=\"kafka-streams\"\n`}</code></pre>\n    <p>{`Since we will be using the Kafka Streams testing functionality we will need to edit the `}<inlineCode parentName=\"p\">{`pom.xml`}</inlineCode>{` to add\nthe dependency to our project. Open `}<inlineCode parentName=\"p\">{`pom.xml`}</inlineCode>{` and add the following.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-xml\"\n      }}>{`<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-streams-test-utils</artifactId>\n    <version>2.5.0</version>\n    <scope>test</scope>\n</dependency>\n`}</code></pre>\n    <p>{`Modify the properties to add kafka, kafka-streams and reactive messaging parameters like`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`quarkus.kafka-streams.auto.offset.reset=latest\nquarkus.kafka-streams.health.enabled=true\nquarkus.kafka-streams.consumer.session.timeout.ms=7000\nquarkus.kafka-streams.consumer.heartbeat.interval.ms=200\nquarkus.kafka-streams.application-id=item-aggregator\nquarkus.kafka-streams.topics=items,inventory\n\nmp.messaging.incoming.item-channel.connector=smallrye-kafka\nmp.messaging.incoming.item-channel.topic=items\nmp.messaging.incoming.item-channel.group.id=item-aggregator\n`}</code></pre>\n    <h3>{`Define an item deserializer`}</h3>\n    <p>{`The item needs to be deserialized to a Item bean, so we add a new class `}<inlineCode parentName=\"p\">{`ItemDeserializer`}</inlineCode>{` under :\nibm.gse.eda.inventory.infrastructure folder. `}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-java\"\n      }}>{`package ibm.gse.eda.inventory.infrastructure;\n\nimport ibm.gse.eda.inventory.domain.Item;\nimport io.quarkus.kafka.client.serialization.JsonbDeserializer;\n\npublic class ItemDeserializer extends JsonbDeserializer<Item> {\n    public ItemDeserializer(){\n        // pass the class to the parent.\n        super(Item.class);\n    }\n}\n`}</code></pre>\n    <p>{`and a declaration in the properties file (change the class name if needed):`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`mp.messaging.incoming.item-channel.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer\nmp.messaging.incoming.item-channel.value.deserializer=ibm.gse.eda.inventory.infrastructure.ItemDeserializer\n`}</code></pre>\n    <p>{`Now restart in dev mode: `}<inlineCode parentName=\"p\">{`mvn quarkus:dev`}</inlineCode>{`… it should compile, and starts running… but could not connect… to kafka. You may see this message:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-logs\"\n      }}>{`WARN  [or.ap.ka.cl.NetworkClient] (kafka-admin-client-thread | adminclient-1) [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n`}</code></pre>\n    <p>{`We could use a docker-compose and strimzi kafka image to run a local cluster, but we will use directly Event Streams deployed on OpenShift for as our development kafka cluster.`}</p>\n    <h3>{`Connect to Event Streams`}</h3>\n    <p>{`We need to complete the configuration to connect to the remote Event Streams running on OpenShift.`}</p>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Create the items and inventory topics, following the instructions as described `}{`[in this note]`}{`(../.. /overview/pre-requisites#creating-event-streams-topics) or using the following command:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`cloudctl es topic-create --name items --partitions 3 --replication-factor 3\ncloudctl es topic-create --name inventory --partitions 1 --replication-factor 3\ncloudctl es topics\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`To connect from your computer to Event Streams running on OpenShift, we need to define a user with `}<inlineCode parentName=\"p\">{`scram-sha-512`}</inlineCode>{` password, as this is the mechanism for external to the cluster connection. `}<a parentName=\"p\" {...{\n            \"href\": \"https://ibm.github.io/event-streams/getting-started/connecting/\"\n          }}>{`See product documentation`}</a>{` on how to do it, or use our `}<a parentName=\"p\" {...{\n            \"href\": \"http://localhost:8000/use-cases/overview/pre-requisites#get-shram-user\"\n          }}>{`quick summary here`}</a>{`.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Get Server TLS certificate into the `}<inlineCode parentName=\"p\">{`certs`}</inlineCode>{` folder. See our `}<a parentName=\"p\" {...{\n            \"href\": \"http://localhost:8000/use-cases/overview/pre-requisites#get-tls-server-public-certificate\"\n          }}>{`quick summary here`}</a></p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`oc get secret minimal-prod-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f - \n\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Modify the `}<inlineCode parentName=\"p\">{`application.properties`}</inlineCode>{` file to define the kafka connection properties. We need two type of definitions, one for the kafka admin client so the kafka stream can create topics to backup state stores, and one for kafka streams consumer and producer tasks:`}</p>\n      </li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`kafka.bootstrap.servers=\\${KAFKA_BROKERS}\nkafka.security.protocol=\\${SECURE_PROTOCOL}\nkafka.ssl.protocol=TLSv1.2\n%dev.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\\\=\\\\\"\\${KAFKA_USER}\\\\\" password\\\\=\\\\\"\\${KAFKA_PASSWORD}\\\\\";\n%dev.kafka.sasl.mechanism=SCRAM-SHA-512\nkafka.ssl.truststore.location=\\${KAFKA_CERT_PATH}\nkafka.ssl.truststore.password=\\${KAFKA_CERT_PWD}\nkafka.ssl.truststore.type=PKCS12\n%prod.kafka.ssl.keystore.location=\\${USER_CERT_PATH}\n%prod.kafka.ssl.keystore.password=\\${USER_CERT_PWD}\n%prod.kafka.ssl.keystore.type=PKCS12\n`}</code></pre>\n    <p>{`The above settings take into account that when running locally (`}<inlineCode parentName=\"p\">{`%dev`}</inlineCode>{` profile) we use the `}<inlineCode parentName=\"p\">{`scram-sha`}</inlineCode>{` mechanism to authenticate, and when we deploy on openshift, the `}<inlineCode parentName=\"p\">{`%prod`}</inlineCode>{` profile is used with TLS mutual authentication  (client certificate in keystore).`}</p>\n    <p>{`The same approach applies for Kafka Stream:`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`quarkus.kafka-streams.bootstrap-servers=\\${KAFKA_BROKERS}\nquarkus.kafka-streams.security.protocol=\\${SECURE_PROTOCOL}\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\n%dev.quarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\n%dev.quarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\\\=\\\\\"\\${KAFKA_USER}\\\\\" password\\\\=\\\\\"\\${KAFKA_PASSWORD}\\\\\";\nquarkus.kafka-streams.ssl.truststore.location=\\${KAFKA_CERT_PATH}\nquarkus.kafka-streams.ssl.truststore.password=\\${KAFKA_CERT_PWD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n# Only if TLS is used for authentication instead of scram\n%prod.quarkus.kafka-streams.ssl.keystore.location=\\${USER_CERT_PATH}\n%prod.quarkus.kafka-streams.ssl.keystore.password=\\${USER_CERT_PWD}\n%prod.quarkus.kafka-streams.ssl.keystore.type=PKCS12\n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">{`Define a file, like `}<inlineCode parentName=\"li\">{`.env`}</inlineCode>{`, to set environment variables, and modify the settings from your Event Streams configuration.`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{}}>{`KAFKA_BROKERS=minimal-prod-kafka-bootstrap-eventstreams....containers.appdomain.cloud:443\nKAFKA_USER=\nKAFKA_PASSWORD=\nKAFKA_CERT_PATH=\\${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=\nSECURE_PROTOCOL=SASL_SSL\n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Restart the quarkus in dev mode`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`source .env\n./mvnw quarkus:dev\n`}</code></pre>\n        <p parentName=\"li\">{`normally you should not get any exception and should get a trace like`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{}}>{`   AdminClientConfig values: \n   bootstrap.servers = [minimal-prod-kafka-bootstrap-eventstreams.gse-.....containers.appdomain.cloud:443]\n   client.dns.lookup = default\n   client.id = \n   connections.max.idle.ms = 300000\n   default.api.timeout.ms = 60000\n   metadata.max.age.ms = 300000\n   metric.reporters = []\n   metrics.num.samples = 2\n   metrics.recording.level = INFO\n   metrics.sample.window.ms = 30000\n\n   ....\n   INFO  [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kafka-streams, kubernetes, kubernetes-client, mutiny, resteasy, resteasy-jsonb, resteasy-mutiny, smallrye-context-propagation, smallrye-health, smallrye-openapi, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, swagger-ui, vertx]\n`}</code></pre>\n      </li>\n    </ul>\n    <h3>{`Define the kafka streams topology`}</h3>\n    <p>{`While in dev mode, we can add the `}<inlineCode parentName=\"p\">{`StoreInventoryAgent`}</inlineCode>{` class under the domain folder. We want to apply a domain driven design implementation approach using domain classes to represent the business logic and code expressed with ubiquituous language. The proposed implementation almost reach this language, adding only into the vocabulary the concept of streams and table in the form of KStream and KTable. We could have avoid that but it will not bring that much value for the implementation. Stream and tables are clear enought terms to be understood by business analysts.`}</p>\n    <p>{`The requirements can be bullet listed as:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`out-topic: inventory: contains the inventory stock events.`}</li>\n      <li parentName=\"ul\">{`Keep a table view in the form: <storeID, <itemID, count>`}</li>\n      <li parentName=\"ul\">{`Add query to get inventory snapshot for store. Expose the result as API. `}</li>\n    </ul>\n    <p>{`The `}<inlineCode parentName=\"p\">{`StoreInventoryAgent`}</inlineCode>{` class processes items to generate inventory view per store. To separate the Kafka plumbing from the business methods the code for Kafka is in the infrastructure package. `}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{`import javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Produces;\nimport javax.inject.Inject;\n\n@ApplicationScoped\npublic class StoreInventoryAgent {\n    @Inject\n    public ItemStream itemStream;\n\n    @Inject\n    public InventoryAggregate inventoryAggregate;\n\n    @Produces\n    public Topology processItemStream(){\n        KStream<String,Item> items = itemStream.getItemStreams();     \n        // process items and aggregate at the store level \n        KTable<String,Inventory> inventory = items\n            // use store name as key\n            .groupByKey(ItemStream.buildGroupDefinition())\n            // update the current stock for this store - item pair\n            // change the value type\n            .aggregate(\n                () ->  new Inventory(), // initializer\n                (k , newItem, existingInventory) \n                    -> existingInventory.updateStockQuantity(k,newItem), \n                    InventoryAggregate.materializeAsInventoryStore());       \n        inventoryAggregate.produceInventoryStockStream(inventory);\n        return itemStream.run();\n    }\n}\n`}</code></pre>\n    <ol>\n      <li parentName=\"ol\">{`process item sale or restock events from the items stream. Should be clear.`}</li>\n      <li parentName=\"ol\">{`build an inventory per store and items. So after explaining the Inventory class it should not be not that complex to see what the aggregate method does: it gets a new item, and is not present yet in the table, then it create a new Inventory entry, and then take existing inventory to update with the new item event.`}</li>\n      <li parentName=\"ol\">{`aggregate is a Ktable that we want to persist in state store, a distributed storage, so we use a materialize as a store.`}</li>\n      <li parentName=\"ol\">{`the last step is to produce an inventory event as output stream  `}</li>\n    </ol>\n    <h3>{`Add kafka related infrastructure code`}</h3>\n    <ul>\n      <li parentName=\"ul\">{`The item stream define lower level method to get access to kafka records from Kafka topic.`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{`@ApplicationScoped\npackage ibm.gse.eda.inventory.infrastructure;\n\npublic class ItemStream {\n\n    @Inject\n    @ConfigProperty(name=\"mp.messaging.incoming.item-channel.topic\")\n    public String itemSoldInputStreamName;\n    \n    private JsonbSerde<Item> itemSerde = new JsonbSerde<>(Item.class);\n    public StreamsBuilder builder;\n      \n    public ItemStream(){\n        builder = new StreamsBuilder();\n    }\n\n    public KStream<String,Item> getItemStreams(){\n        return builder.stream(itemSoldInputStreamName, \n                        Consumed.with(Serdes.String(), itemSerde));\n    }\n\n    public Topology run() {\n        return builder.build();\n    }\n\n    public static Grouped<String, Item> buildGroupDefinition() {\n        return Grouped.with(Serdes.String(),itemSerde);\n    }\n    \n   \n}       \n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">{`Add the inventory aggregate to generate events: InventoryAggregate.`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{`package ibm.gse.eda.inventory.infrastructure;\n@ApplicationScoped\npublic class InventoryAggregate {\n\n    public InventoryAggregate() {\n    }\n\n\n    // store to keep stock per store-id\n    public static String INVENTORY_STORE_NAME = \"StoreInventoryStock\";\n\n    @Inject\n    @ConfigProperty(name = \"mp.messaging.outgoing.inventory-channel.topic\")\n    public String inventoryStockOutputStreamName;\n\n    private static JsonbSerde<Inventory> inventorySerde = new JsonbSerde<>(Inventory.class);\n    KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(INVENTORY_STORE_NAME);\n\n    public static Materialized<String, Inventory, KeyValueStore<Bytes, byte[]>> materializeAsInventoryStore() {\n        return Materialized.<String, Inventory, KeyValueStore<Bytes, byte[]>>as(INVENTORY_STORE_NAME)\n                .withKeySerde(Serdes.String()).withValueSerde(inventorySerde);\n    }\n\n    public void produceInventoryStockStream(KTable<String, Inventory> inventory) {\n        KStream<String, Inventory> inventories = inventory.toStream();\n        inventories.print(Printed.toSysOut());\n\n        inventories.to(inventoryStockOutputStreamName, Produced.with(Serdes.String(), inventorySerde));\n    }\n\n`}</code></pre>\n    <p>{`And define the out going channel:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`mp.messaging.outgoing.inventory-channel.connector=smallrye-kafka\nmp.messaging.outgoing.inventory-channel.topic=inventory\nmp.messaging.outgoing.inventory-channel.key.serializer=org.apache.kafka.common.serialization.StringSerializer\nmp.messaging.outgoing.inventory-channel.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n`}</code></pre>\n    <h2>{`Topology test`}</h2>\n    <p>{`We already presented how to use the TopologyTestDriver in previous labs. The class for testing is `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory/blob/master/src/test/java/ut/TestInventory.java\"\n      }}>{`ut.TestInventory.java`}</a>{` and used the agent class, but inject the bean manually.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-java\"\n      }}>{`public void setup() {\n        // as no CDI is used set the topic names\n        agent.itemStream = new ItemStream();\n        agent.itemStream.itemSoldInputStreamName=\"itemSold\";\n        agent.inventoryAggregate = new InventoryAggregate();\n        agent.inventoryAggregate.inventoryStockOutputStreamName = \"inventory\";\n        \n        Topology topology = agent.processItemStream();\n        testDriver = new TopologyTestDriver(topology, getStreamsConfig());\n}\n`}</code></pre>\n    <p>{`Do not use `}<inlineCode parentName=\"p\">{`@QuarkusTest`}</inlineCode>{` in the test class to avoid loading the application.properties. `}</p>\n    <h2>{`Interactive queries`}</h2>\n    <p>{`Now as presented in `}<a parentName=\"p\" {...{\n        \"href\": \"../../../technology/kafka-streams/#interactive-queries\"\n      }}>{`this note`}</a>{`, as soon as we use KTable materialized with state store we can use query to get the last state of the records saved.\nThe API returns a query result on the inventory. We can define such bean as:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{`public class InventoryQueryResult {\n    private static InventoryQueryResult NOT_FOUND = new InventoryQueryResult(null, null, null);\n    private final Inventory result;\n    private final String host;\n    private final Integer port;\n\n    public static InventoryQueryResult notFound() {\n        return NOT_FOUND;\n    }\n\n    public Optional<Inventory> getResult() {\n        return Optional.ofNullable(result);\n    }\n}\n`}</code></pre>\n    <p>{`So the Resource class is not`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-java\"\n      }}>{`@GET\n@Path(\"/store/{storeID}\")\n@Produces(MediaType.APPLICATION_JSON)\npublic Uni<InventoryQueryResult> getStock(@PathParam(\"storeID\") String storeID) {\n    InventoryQueryResult result = queries.getStoreStock(storeID);\n    if (result.getResult().isPresent()) {\n        return Uni.createFrom().item(result);\n    } else {\n        return  Uni.createFrom().item(InventoryQueryResult.notFound());\n    }\n}\n`}</code></pre>\n    <p>{`The queries is the new class to support interactive query. The principle is simple, we need to access the store that has the storeID key we search for. But there is a small problem, due to the fact that the input topic may be partitioned so the local store may not have the data for the given key. Therefore Kafka streams offers an API to get metadata of the store allocation between nodes for the Kafka Streams.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-Java\"\n      }}>{` @Inject\nKafkaStreams streams;\n\nmetadata = streams.queryMetadataForKey(\n    StoreInventoryAgent.STOCKS_STORE_NAME,\n        storeID,\n        Serdes.String().serializer());\n    ...\n    if (metadata.getActiveHost().host().equals(host)) {\n        Inventory result = getStockStore().get(storeID);\n        return InventoryQueryResult.found(result);\n    } else {\n        // call remote or propagate to ask the client to call the other host\n        return InventoryQueryResult.foundRemotely(metadata.getActiveHost());\n    }\n`}</code></pre>\n    <h3>{`API`}</h3>\n    <p>{`Now we want to complete our APIs by adding information on the store metadata from URL `}<inlineCode parentName=\"p\">{`/meta-data`}</inlineCode>{`. The method to add to the Resource class is:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-java\"\n      }}>{`@GET\n@Path(\"/meta-data\")\n@Produces(MediaType.APPLICATION_JSON)\npublic Multi<PipelineMetadata> getMetaData() {\n    return Multi.createFrom().items(queries.getStockStoreMetaData().stream());\n}\n`}</code></pre>\n    <p><em parentName=\"p\">{`It is possible, while testing the API, to get a 404 response. The execption may be linked to the state of the kafka stream processing: for example something like: `}<inlineCode parentName=\"em\">{`java.lang.IllegalStateException: KafkaStreams is not running. State is CREATED.`}</inlineCode>{`. This may be due to the test data we have, as once kafka stream for a specific group-id has consumed the records then the offsets are committed, and a new start will not process the old records. Changing the application-id properties can re-read all the records from offset 0.`}</em>{` `}</p>\n    <h2>{`Integration tests`}</h2>\n    <p>{` For running the integration test, we propose to copy the e2e folder from the solution repository and follow the `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory#end-to-end-testing\"\n      }}>{`readme instructions section end-to-end-testing `}</a>{`.`}</p>\n    <h2>{`Deploy to OpenShift`}</h2>\n    <p>{`Be sure to have done `}<a parentName=\"p\" {...{\n        \"href\": \"../../overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift\"\n      }}>{`the steps described here`}</a>{` to get user credentials and Server side certificate. `}</p>\n    <p>{`The deployment is done using Quarkus kubernetes plugin which generates DeploymentConfig and other kubernetes manifests.`}<br parentName=\"p\"></br>{`\n`}{`Here are the interesting properties to set environment variables from secrets `}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`%prod.quarkus.openshift.env-vars.KAFKA_USER.value=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SSL\nquarkus.openshift.env-vars.SECURE_PROTOCOL.value=SASL_SSL\nquarkus.openshift.env-vars.KAFKA_BROKERS.value=sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\nquarkus.openshift.env-vars.KAFKA_CERT_PATH.value=/deployments/certs/server/ca.p12\nquarkus.openshift.env-vars.KAFKA_PASSWORD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.KAFKA_PASSWORD.value=user.password\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.secret=sandbox-rp-cluster-ca-cert\nquarkus.openshift.env-vars.KAFKA_CERT_PWD.value=ca.password\nquarkus.openshift.env-vars.USER_CERT_PATH.value=/deployments/certs/user/user.p12\nquarkus.openshift.env-vars.USER_CERT_PWD.secret=sandbox-rp-tls-cred\nquarkus.openshift.env-vars.USER_CERT_PWD.value=user.password\n`}</code></pre>\n    <p>{`And an extract of the expected generated openshift manifests from those configurations:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-yaml\"\n      }}>{`    spec:\n      containers:\n      - env:\n        - name: KAFKA_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: ca.password\n              name: sandbox-rp-cluster-ca-cert\n        - name: USER_CERT_PATH\n          value: /deployments/certs/user/user.p12\n        - name: USER_CERT_PWD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: KAFKA_BROKERS\n          value: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n        - name: KAFKA_CERT_PATH\n          value: /deployments/certs/server/ca.p12\n        - name: KAFKA_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: user.password\n              name: sandbox-rp-tls-cred\n        - name: SECURE_PROTOCOL\n          value: SASL_SSL\n`}</code></pre>\n    <p>{`Finally the TLS certificated are mounted to the expected locations defined in the environment variables. The properties for that are:`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`quarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=sandbox-rp-cluster-ca-cert\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=sandbox-rp-tls-cred\n`}</code></pre>\n    <p>{`which generates:`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: es-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n`}</code></pre>\n    <p>{`Now any deployment using the following command should work:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`./mvnw clean package -DQuarkus.kubernetes.deploy=true\n`}</code></pre>\n    <p>{`The last piece is to go to EventStreams console and look at the inventory topic for messages generated. As an alternate we could use `}<a parentName=\"p\" {...{\n        \"href\": \"../../overview/pre-requisites#using-kafdrop\"\n      }}>{`Kafdrop`}</a>{`.`}</p>\n    <h2>{`Another item producer`}</h2>\n    <p>{`We have done a simple app to produce item sale or restock events. The app is not exposed with API defined in Swagger or with JAXRS annotations, but expose one method to be exposed as a service. See the `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-eda-tools/blob/05fcdcb7d09d674d9eb2cda2e28601171ba51166/item-kafka-producer/src/main/java/ibm/gse/eda/api/ItemSimulatorFunction.java#L18-L25\"\n      }}>{` ItemSimulatorFunction code`}</a>{`. `}</p>\n    <p>{`The simulator is using reactive messaging, but as we mix imperative with reactive programming, the code is using Emitter, and then Munity Multi to create and send the Kafka records:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-java\"\n      }}>{`@Inject\n@Channel(\"items\")\nEmitter<Item> emitter;\n\n\npublic void sendItems(Integer numberOfRecords) {\n    Multi.createFrom().items(buildItems(numberOfRecords).stream()).subscribe().with(item -> {\n            logger.warning(\"send \" + item.toString());\n            Message<Item> record = KafkaRecord.of(item.storeName,item);\n            emitter.send(record );\n        }, failure -> System.out.println(\"Failed with \" + failure.getMessage()));\n   \n`}</code></pre>\n    <h3>{`Running the application in dev mode`}</h3>\n    <p>{`You can run your application in dev mode that enables live coding using:`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`./mvnw quarkus:dev\n`}</code></pre>\n    <p>{`But as we connect to a remote Kafka Cluster you need to define environment variables as:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`KAFKA_BROKERS=....containers.appdomain.cloud:443\nKAFKA_USER=<a>-scram-user\nKAFKA_PASSWORD=<a-password>\nKAFKA_CERT_PATH=\\${PWD}/certs/es-cert.p12\nKAFKA_CERT_PWD=<server-ca-certificate-password>\nSECURE_PROTOCOL=SASL_SSL\n`}</code></pre>\n    <h3>{`Packaging and running the application`}</h3>\n    <p>{`The application can be packaged using `}<inlineCode parentName=\"p\">{`./mvnw package`}</inlineCode>{`.\nIt produces the `}<inlineCode parentName=\"p\">{`item-kafka-producer-1.0.0-SNAPSHOT-runner.jar`}</inlineCode>{` file in the `}<inlineCode parentName=\"p\">{`/target`}</inlineCode>{` directory.\nBe aware that it’s not an `}<em parentName=\"p\">{`über-jar`}</em>{` as the dependencies are copied into the `}<inlineCode parentName=\"p\">{`target/lib`}</inlineCode>{` directory.`}</p>\n    <p>{`The application is now runnable using `}<inlineCode parentName=\"p\">{`java -jar target/item-kafka-producer-1.0.0-SNAPSHOT-runner.jar`}</inlineCode>{`.`}</p>\n    <h3>{`Deployment to OpenShift`}</h3>\n    <p>{`The code includes declaration to build the necessary environment variables, secrets, volumes to get connected to a Kafka cluster using TLS authentication, and deploy in one command:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`mvn package -DskipTests -Dquarkus.kubernetes.deploy=true\n`}</code></pre>\n    <p>{`See the application.properties for `}<inlineCode parentName=\"p\">{`quarkus.openshift.env-vars.*`}</inlineCode>{` settings.`}</p>\n    <p>{`It is also supposed to be deployed as knative app, but there is an issue on the generation of volume declarations in the knative.yaml file, so we could not make it in one command.`}</p>\n    <p>{`The code is also available as docker image: `}<a parentName=\"p\" {...{\n        \"href\": \"https://hub.docker.com/r/ibmcase/item-kafka-producer\"\n      }}>{`ibmcase/item-kafka-producer`}</a>{`.`}</p>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}