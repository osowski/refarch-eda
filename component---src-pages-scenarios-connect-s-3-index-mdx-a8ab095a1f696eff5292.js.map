{"version":3,"sources":["webpack:///./src/pages/scenarios/connect-s3/index.mdx"],"names":["_frontmatter","makeShortcode","name","props","console","warn","AnchorLinks","AnchorLink","layoutProps","MDXLayout","DefaultLayout","MDXContent","components","mdxType","parentName","isMDXComponent"],"mappings":"2eAMO,IAAMA,EAAe,GAEtBC,EAAgB,SAAAC,GAAI,OAAI,SAA6BC,GAEzD,OADAC,QAAQC,KAAK,aAAeH,EAAO,2EAC5B,kBAASC,KAGZG,EAAcL,EAAc,eAC5BM,EAAaN,EAAc,cAC3BO,EAAc,CAClBR,gBAEIS,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGT,E,oIACF,mBACD,OAAO,YAACM,EAAD,KAAeD,EAAiBL,EAAhC,CAAuCS,WAAYA,EAAYC,QAAQ,cAG5E,YAACP,EAAD,CAAaO,QAAQ,eACvB,YAACN,EAAD,CAAYM,QAAQ,cAApB,YACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,oBACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,yBACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,8BACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,gCACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,cACA,YAACN,EAAD,CAAYM,QAAQ,cAApB,eAEE,kCACA,0EAAyD,mBAAGC,WAAW,KAAQ,CAC3E,KAAQ,wDAD6C,qBAAzD,4BAE2D,mBAAGA,WAAW,KAAQ,CAC7E,KAAQ,+BAD+C,aAF3D,kEAIyF,mBAAGA,WAAW,KAAQ,CAC3G,KAAQ,wEAD6E,2BAJzF,4BAMiE,mBAAGA,WAAW,KAAQ,CACnF,KAAQ,8BADqD,mCANjE,4BAQyE,mBAAGA,WAAW,KAAQ,CAC3F,KAAQ,qEAD6D,gCARzE,wFAUkI,mBAAGA,WAAW,KAAQ,CACpJ,KAAQ,wDADsH,qBAVlI,QAYuC,mBAAGA,WAAW,KAAQ,CACzD,KAAQ,+BAD2B,UAZvC,KAeA,kBAAS,CACP,IAAO,mIACP,IAAO,0DAET,sjBACA,0CACA,qBAAG,sBAAQA,WAAW,KAAnB,iCACH,sBACE,kBAAIA,WAAW,MAAf,iHAAsI,0BAAYA,WAAW,MAAvB,OAAtI,MAEF,qBAAG,sBAAQA,WAAW,KAAnB,YACH,sBACE,kBAAIA,WAAW,MAAf,iDAAsE,mBAAGA,WAAW,MAAS,CACzF,KAAQ,oCAD0D,oBAAtE,+DAGA,kBAAIA,WAAW,MAAf,wBAA6C,0BAAYA,WAAW,MAAvB,UAA7C,4FAA6L,0BAAYA,WAAW,MAAvB,UAA7L,KACA,kBAAIA,WAAW,MAAf,8DAAmF,mBAAGA,WAAW,MAAS,CACtG,KAAQ,mGADuE,wBAAnF,0DAGA,kBAAIA,WAAW,MAAf,qDAA0E,sBAAQA,WAAW,MAAnB,4BAA1E,WAEF,qBAAG,sBAAQA,WAAW,KAAnB,gCACH,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,qCAAyD,mBAAGA,WAAW,KAAQ,CAC3E,KAAQ,+BAD6C,UAAzD,yDAIF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,oKAEF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,qCACA,kBAAIA,WAAW,MACb,kBAAIA,WAAW,MAAf,OAA4B,mBAAGA,WAAW,MAAS,CAC/C,KAAQ,sIACP,0BAAYA,WAAW,KAAvB,sBAFuB,WAA5B,mBAEsG,kBAAIA,WAAW,MAAf,sDAFtG,KAGA,kBAAIA,WAAW,MAAf,MAA2B,mBAAGA,WAAW,MAAS,CAC9C,KAAQ,sEADe,oBAA3B,qGAKJ,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,uBAA2C,0BAAYA,WAAW,KAAvB,8BAA3C,+BACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,IAA3B,iGAIvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,kGACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,mBADI,4FAKvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAK,kBAAIA,WAAW,KAAf,8GAGvB,qBAAG,sBAAQA,WAAW,KAAnB,8BACH,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,iCAAqD,mBAAGA,WAAW,KAAQ,CACvE,KAAQ,wDADyC,qBAArD,6CAIF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,mGAEF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,kJAEF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,uHACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,mBADI,yGAMzB,qBAAG,sBAAQA,WAAW,KAAnB,oFACH,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,mOAEF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,2DAA+E,sBAAQA,WAAW,KAAnB,2BAA/E,qCAEF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,yGACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,mBADI,sPAQzB,+CACA,8MAA6L,0BAAYA,WAAW,KAAvB,gBAA7L,6UACA,6EACA,6SAA4R,0BAAYA,WAAW,KAAvB,SAA5R,mDACA,mEAAkD,0BAAYA,WAAW,KAAvB,oBAAlD,iCAAgJ,0BAAYA,WAAW,KAAvB,8EAAhJ,uHAA8X,0BAAYA,WAAW,KAAvB,aAA9X,OAA2b,0BAAYA,WAAW,KAAvB,aAA3b,kBACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,wBADZ,8tEA0CL,mEACA,4DAA2C,0BAAYA,WAAW,KAAvB,mBAA3C,iCAAwI,0BAAYA,WAAW,KAAvB,uBAAxI,4BAAoO,kBAAIA,WAAW,KAAf,yBAApO,KACA,sBACE,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,qBAApB,+FAAkL,0BAAYA,WAAW,MAAvB,SAAlL,oBAAyP,kBAAIA,WAAW,MAAf,eACzP,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,yBAApB,8DACA,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,8CAApB,oFACA,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,iDAApB,gFACA,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,2DAApB,uEACA,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,2BAApB,6JACA,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,kCAApB,+KAEF,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,owCAyCL,8DAA6C,0BAAYA,WAAW,KAAvB,sBAA7C,qEAAiL,0BAAYA,WAAW,KAAvB,aAAjL,QAA+O,0BAAYA,WAAW,KAAvB,aAA/O,wCAA6U,0BAAYA,WAAW,KAAvB,gBAA7U,6IAAmhB,0BAAYA,WAAW,KAAvB,uCAAnhB,yCAA4oB,0BAAYA,WAAW,KAAvB,uBAA5oB,8CACA,yDACA,iLACA,sBACE,kBAAIA,WAAW,MAAf,wBAA6C,mBAAGA,WAAW,MAAS,CAChE,KAAQ,qDADiC,oDAA7C,2BAGA,kBAAIA,WAAW,MAAf,iBAAsC,0BAAYA,WAAW,MAAvB,sCAAtC,QAA8H,0BAAYA,WAAW,MAAvB,mDAA9H,KACA,kBAAIA,WAAW,MAAf,6EAAkG,0BAAYA,WAAW,MAAvB,qBAAlG,aACA,kBAAIA,WAAW,MAAf,kHAEF,4CACA,sBACE,kBAAIA,WAAW,MAAf,6BAAkD,mBAAGA,WAAW,MAAS,CACrE,KAAQ,qDADsC,oDAAlD,2CAEyG,mBAAGA,WAAW,MAAS,CAC5H,KAAQ,oDAD6F,mDAFzG,6HAI0L,mBAAGA,WAAW,MAAS,CAC7M,KAAQ,gCAD8K,8BAJ1L,cAOA,kBAAIA,WAAW,MAAf,sLAEF,mEACA,wMACA,sBACE,kBAAIA,WAAW,MAAf,+GAAoI,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC9K,UAAa,mBADsI,4IAIzJ,kBAAIA,WAAW,MAAf,+IAAoK,kBAAIA,WAAW,MAAf,2DAApK,IAA6P,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CACvS,UAAa,mBAD+P,2BAIlR,kBAAIA,WAAW,MAAf,YAAiC,0BAAYA,WAAW,MAAvB,iDAAjC,gBAA4I,0BAAYA,WAAW,MAAvB,WAA5I,6BAEF,oDACA,iQAAgP,mBAAGA,WAAW,KAAQ,CAClQ,KAAQ,6DACP,0BAAYA,WAAW,KAAvB,kBAF2O,oBAAhP,yCAGA,4DAA2C,0BAAYA,WAAW,KAAvB,kBAA3C,iCAAuI,0BAAYA,WAAW,KAAvB,qBAAvI,6BACA,sBACE,kBAAIA,WAAW,MAAf,OAA4B,0BAAYA,WAAW,MAAvB,sBAA5B,gFAA4K,kBAAIA,WAAW,MAAf,yCAA8D,0BAAYA,WAAW,MAAvB,kBAA9D,YAAsI,0BAAYA,WAAW,MAAvB,gBAAtI,cAC5K,kBAAIA,WAAW,MAAf,OAA4B,0BAAYA,WAAW,MAAvB,UAA5B,cAA8F,kBAAIA,WAAW,MAAf,UAA+B,0BAAYA,WAAW,MAAvB,mBAA/B,WAC9F,kBAAIA,WAAW,MAAf,kCAAuD,0BAAYA,WAAW,MAAvB,kBAAvD,yBAA4I,kBAAIA,WAAW,MAAf,UAA+B,0BAAYA,WAAW,MAAvB,gBAA/B,YAE9I,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,27BAsBL,oFAAmE,0BAAYA,WAAW,KAAvB,6BAAnE,sCAA+K,0BAAYA,WAAW,KAAvB,8CAA/K,yCAA+S,0BAAYA,WAAW,KAAvB,uBAA/S,8CACA,qBAAG,sBAAQA,WAAW,KAAnB,SAAH,yHAAsK,0BAAYA,WAAW,KAAvB,WAAtK,2BAAqP,0BAAYA,WAAW,KAAvB,kBAArP,6CAA6V,0BAAYA,WAAW,KAAvB,2GAA7V,KACA,sDACA,uCAAsB,mBAAGA,WAAW,KAAQ,CACxC,KAAQ,gCADU,8BAAtB,yDAEiG,mBAAGA,WAAW,KAAQ,CACnH,KAAQ,6DACP,0BAAYA,WAAW,KAAvB,kBAF4F,oBAFjG,kDAKA,4DAA2C,0BAAYA,WAAW,KAAvB,kBAA3C,iCAAuI,0BAAYA,WAAW,KAAvB,uBAAvI,6BACA,sBACE,kBAAIA,WAAW,MAAf,OAA4B,0BAAYA,WAAW,MAAvB,sBAA5B,gFAA4K,kBAAIA,WAAW,MAAf,yCAA8D,0BAAYA,WAAW,MAAvB,kBAA9D,YAAsI,0BAAYA,WAAW,MAAvB,gBAAtI,cAC5K,kBAAIA,WAAW,MAAf,OAA4B,0BAAYA,WAAW,MAAvB,UAA5B,QAAwF,0BAAYA,WAAW,MAAvB,4BAAxF,eAA6K,kBAAIA,WAAW,MAAf,UAA+B,0BAAYA,WAAW,MAAvB,mBAA/B,WAC7K,kBAAIA,WAAW,MAAf,kCAAuD,0BAAYA,WAAW,MAAvB,kBAAvD,yBAA4I,kBAAIA,WAAW,MAAf,UAA+B,0BAAYA,WAAW,MAAvB,gBAA/B,YAE9I,qBAAG,sBAAQA,WAAW,KAAnB,eAAH,iEAAoH,mBAAGA,WAAW,KAAQ,CACtI,KAAQ,gCADwG,8BAApH,0BAEkE,mBAAGA,WAAW,KAAQ,CACpF,KAAQ,kCADsD,gCAFlE,yEAImH,sBAAQA,WAAW,KAAnB,oBAJnH,YAIoL,sBAAQA,WAAW,KAAnB,kBAJpL,oNAKA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,46BAsBL,oFAAmE,0BAAYA,WAAW,KAAvB,+BAAnE,sCAAiL,0BAAYA,WAAW,KAAvB,gDAAjL,yCAAmT,0BAAYA,WAAW,KAAvB,uBAAnT,8CACA,qBAAG,sBAAQA,WAAW,KAAnB,SAAH,iHAA8J,0BAAYA,WAAW,KAAvB,+CAA9J,+EAAqU,0BAAYA,WAAW,KAAvB,6DAArU,MACA,oCACA,sBACE,kBAAIA,WAAW,MAAf,iEAAsF,kBAAIA,WAAW,MAAf,+CAAoE,0BAAYA,WAAW,MAAvB,qBAApE,QAA2I,0BAAYA,WAAW,MAAvB,yBAA3I,mFACtF,kBAAIA,WAAW,MAAf,0FAA+G,kBAAIA,WAAW,MAAf,4DAC/G,kBAAIA,WAAW,MAAf,iHAEF,oCACA,sBACE,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,oGADQ,wEAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,8EADQ,6CAApB,mBAE0E,0BAAYA,WAAW,MAAvB,WAF1E,qBAEoJ,0BAAYA,WAAW,MAAvB,kBAFpJ,cAGA,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,iEADQ,8CAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,uGADQ,0EAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,mDADQ,sCAQ1BH,EAAWI,gBAAiB","file":"component---src-pages-scenarios-connect-s-3-index-mdx-a8ab095a1f696eff5292.js","sourcesContent":["import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/runner/work/refarch-eda/refarch-eda/docs/node_modules/gatsby-theme-carbon/src/templates/Default.js\";\nexport const _frontmatter = {};\n\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n  return <div {...props} />;\n};\n\nconst AnchorLinks = makeShortcode(\"AnchorLinks\");\nconst AnchorLink = makeShortcode(\"AnchorLink\");\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <AnchorLinks mdxType=\"AnchorLinks\">\n  <AnchorLink mdxType=\"AnchorLink\">Overview</AnchorLink>\n  <AnchorLink mdxType=\"AnchorLink\">Scenario prereqs</AnchorLink>\n  <AnchorLink mdxType=\"AnchorLink\">Kafka Connect Cluster</AnchorLink>\n  <AnchorLink mdxType=\"AnchorLink\">Kafka to S3 Sink Connector</AnchorLink>\n  <AnchorLink mdxType=\"AnchorLink\">S3 to Kafka Source Connector</AnchorLink>\n  <AnchorLink mdxType=\"AnchorLink\">Next steps</AnchorLink>\n  <AnchorLink mdxType=\"AnchorLink\">References</AnchorLink>\n    </AnchorLinks>\n    <h2>{`Overview`}</h2>\n    <p>{`This scenario walkthrough will cover the usage of `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm.github.io/event-streams/about/overview/\"\n      }}>{`IBM Event Streams`}</a>{` as a Kafka provider and `}<a parentName=\"p\" {...{\n        \"href\": \"https://aws.amazon.com/s3/\"\n      }}>{`Amazon S3`}</a>{` as an object storage service as systems to integrate with the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm-cloud-architecture.github.io/refarch-eda/kafka/connect/\"\n      }}>{`Kafka Connect framework`}</a>{`. Through the use of the `}<a parentName=\"p\" {...{\n        \"href\": \"https://camel.apache.org/\"\n      }}>{`Apache Camel opensource project`}</a>{`, we are able to use the `}<a parentName=\"p\" {...{\n        \"href\": \"https://camel.apache.org/camel-kafka-connector/latest/index.html\"\n      }}>{`Apache Camel Kafka Connector`}</a>{` in both a source and a sink capacity to provide bidirectional communication between `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm.github.io/event-streams/about/overview/\"\n      }}>{`IBM Event Streams`}</a>{` and `}<a parentName=\"p\" {...{\n        \"href\": \"https://aws.amazon.com/s3/\"\n      }}>{`AWS S3`}</a>{`.`}</p>\n    <img {...{\n      \"src\": \"https://github.com/ibm-cloud-architecture/refarch-eda/raw/master/docs-archive/kafka/images/eventstreams-to-s3-connector-flow.png\",\n      \"alt\": \"IBM Event Streams to S3 integration via Kafka Connect\"\n    }}></img>\n    <p>{`As different use cases will require different configuration details to accommodate different situational requirements, the Kafka to S3 Source and Sink capabilities described here can be used to move data between S3 buckets with a Kafka topic being the middle-man or move data between Kafka topics with an S3 Bucket being the middle-man. However, take care to ensure that you do not create an infinite processing loop by writing to the same Kafka topics and the same S3 buckets with both a Source and Sink connector deployed at the same time.`}</p>\n    <h2>{`Scenario prereqs`}</h2>\n    <p><strong parentName=\"p\">{`OpenShift Container Platform`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of `}<inlineCode parentName=\"li\">{`4.2`}</inlineCode>{`.`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Strimzi`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`This deployment scenario will make use of the `}<a parentName=\"li\" {...{\n          \"href\": \"https://strimzi.io/docs/0.17.0/\"\n        }}>{`Strimzi Operator`}</a>{` for Kafka deployments and the custom resources it manages.`}</li>\n      <li parentName=\"ul\">{`A minimum version of `}<inlineCode parentName=\"li\">{`0.17.0`}</inlineCode>{` is required for this scenario. This scenario has been explicitly validated with version `}<inlineCode parentName=\"li\">{`0.17.0`}</inlineCode>{`.`}</li>\n      <li parentName=\"ul\">{`The simplest scenario is to deploy the Strimzi Operator to `}<a parentName=\"li\" {...{\n          \"href\": \"https://strimzi.io/docs/0.17.0/#deploying-cluster-operator-to-watch-whole-cluster-deploying-co\"\n        }}>{`watch all namespaces`}</a>{` for relevant custom resource creation and management.`}</li>\n      <li parentName=\"ul\">{`This can be done in the OpenShift console via the `}<strong parentName=\"li\">{`Operators > Operator Hub`}</strong>{` page.`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Amazon Web Services account`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`As this scenario will make use of `}<a parentName=\"p\" {...{\n            \"href\": \"https://aws.amazon.com/s3/\"\n          }}>{`AWS S3`}</a>{`, an active Amazon Web Services account is required.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Using the configuration described in this walkthrough, an additional IAM user can be created for further separation of permission, roles, and responsibilities.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`This new IAM user should contain:`}</p>\n        <ul parentName=\"li\">\n          <li parentName=\"ul\">{`The `}<a parentName=\"li\" {...{\n              \"href\": \"https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn:aws:iam::aws:policy/AmazonS3FullAccess$serviceLevelSummary\"\n            }}><inlineCode parentName=\"a\">{`AmazonS3FullAccess`}</inlineCode>{` policy`}</a>{` attached to it `}<em parentName=\"li\">{`(as it will need both read and write access to S3)`}</em>{`,`}</li>\n          <li parentName=\"ul\">{`An `}<a parentName=\"li\" {...{\n              \"href\": \"https://docs.aws.amazon.com/AmazonS3/latest/dev/walkthrough1.html\"\n            }}>{`S3 Bucket Policy`}</a>{` set on the Bucket to allow the IAM user to perform CRUD actions on the bucket and its objects.`}</li>\n        </ul>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Create a file named `}<inlineCode parentName=\"p\">{`aws-credentials.properties`}</inlineCode>{` with the following format:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{}}>{`aws_access_key_id=AKIA123456EXAMPLE\naws_secret_access_key=strWrz/bb8%c3po/r2d2EXAMPLEKEY\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Create a Kubernetes Secret from this file to inject into the Kafka Connect cluster at runtime:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`kubectl create secret generic aws-credentials --from-file=aws-credentials.properties\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><em parentName=\"p\">{`Additional work is underway to enable configuration of the components to make use of IAM Roles instead.`}</em></p>\n      </li>\n    </ul>\n    <p><strong parentName=\"p\">{`IBM Event Streams API Key`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`This scenario is written with `}<a parentName=\"p\" {...{\n            \"href\": \"https://ibm.github.io/event-streams/about/overview/\"\n          }}>{`IBM Event Streams`}</a>{` as the provider of the Kafka endpoints.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`API Keys are required for connectivity to the Kafka brokers and interaction with Kafka topics.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`An API key should be created with (at minimum) read and write access to the source and target Kafka topics the connectors will interact with.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`A Kubernetes Secret must be created with the Event Streams API to inject into the Kafka Connect cluster at runtime:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`kubectl create secret generic eventstreams-apikey --from-literal=password=<eventstreams_api_key>\n`}</code></pre>\n      </li>\n    </ul>\n    <p><strong parentName=\"p\">{`IBM Event Streams Certificates (IBM Cloud Pak for Integration deployments only)`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`If you are using an IBM Event Streams instance deployed via the IBM Cloud Pak for Integration, you must also download the generated truststore file to provide TLS communication between the connectors and the Kafka brokers.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`This file, along with its password, can be found on the `}<strong parentName=\"p\">{`Connect to this cluster`}</strong>{` dialog in the Event Streams UI.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Once downloaded, it must be configured to work with the Kafka Connect certificate deployment pattern:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`keytool -importkeystore -srckeystore es-cert.jks -destkeystore es-cert.p12 -deststoretype PKCS12\nopenssl pkcs12 -in es-cert.p12 -nokeys -out es-cert.crt\nkubectl create secret generic eventstreams-truststore-cert --from-file=es-cert.crt\n`}</code></pre>\n      </li>\n    </ul>\n    <h2>{`Kafka Connect Cluster`}</h2>\n    <p>{`We will take advantage of some of the developer experience improvements that OpenShift and the Strimi Operator brings to the Kafka Connect framework. The Strimzi Operator provides a `}<inlineCode parentName=\"p\">{`KafkaConnect`}</inlineCode>{` custom resource which will manage a Kafka Connect cluster for us with minimal system interaction. The only work we need to do is to update the container image that the Kafka Connect deployment will use with the necessary Camel Kafka Connector binaries, which OpenShift can help us with through the use of its Build capabilities.`}</p>\n    <h4>{`(Optional) Create ConfigMap for log4j configuration`}</h4>\n    <p>{`Due to the robust nature of Apache Camel, the default logging settings for the Apache Kafka Connect classes will send potentially sensitive information to the logs during Apache Camel context configuration. To avoid this, we can provide an updated logging configuration to the `}<inlineCode parentName=\"p\">{`log4j`}</inlineCode>{` configuration that is used by our deployments.`}</p>\n    <p>{`Save the properties file below and name it `}<inlineCode parentName=\"p\">{`log4j.properties`}</inlineCode>{`. Then create a ConfigMap via `}<inlineCode parentName=\"p\">{`kubectl create configmap custom-connect-log4j --from-file=log4j.properties`}</inlineCode>{`. This ConfigMap will then be used in our KafkaConnect cluster creation to filter out any logging output containing `}<inlineCode parentName=\"p\">{`accesskey`}</inlineCode>{` or `}<inlineCode parentName=\"p\">{`secretkey`}</inlineCode>{` permutations.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-properties\"\n      }}>{`# Do not change this generated file. Logging can be configured in the corresponding kubernetes/openshift resource.\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\nconnect.root.logger.level=INFO\nlog4j.rootLogger=\\${connect.root.logger.level}, CONSOLE\nlog4j.logger.org.apache.zookeeper=ERROR\nlog4j.logger.org.I0Itec.zkclient=ERROR\nlog4j.logger.org.reflections=ERROR\n\n# Due to back-leveled version of log4j that is included in Kafka Connect,\n# we can use multiple StringMatchFilters to remove all the permutations\n# of the AWS accessKey and secretKey values that may get dumped to stdout\n# and thus into any connected logging system.\nlog4j.appender.CONSOLE.filter.a=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.a.StringToMatch=accesskey\nlog4j.appender.CONSOLE.filter.a.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.b=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.b.StringToMatch=accessKey\nlog4j.appender.CONSOLE.filter.b.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.c=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.c.StringToMatch=AccessKey\nlog4j.appender.CONSOLE.filter.c.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.d=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.d.StringToMatch=ACCESSKEY\nlog4j.appender.CONSOLE.filter.d.AcceptOnMatch=false\n\nlog4j.appender.CONSOLE.filter.e=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.e.StringToMatch=secretkey\nlog4j.appender.CONSOLE.filter.e.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.f=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.f.StringToMatch=secretKey\nlog4j.appender.CONSOLE.filter.f.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.g=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.g.StringToMatch=SecretKey\nlog4j.appender.CONSOLE.filter.g.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.h=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.h.StringToMatch=SECRETKEY\nlog4j.appender.CONSOLE.filter.h.AcceptOnMatch=false\n`}</code></pre>\n    <h4>{`Deploy the baseline Kafka Connect Cluster`}</h4>\n    <p>{`Review the YAML description for our `}<inlineCode parentName=\"p\">{`KafkaConnectS2I`}</inlineCode>{` custom resource below, named `}<inlineCode parentName=\"p\">{`connect-cluster-101`}</inlineCode>{`. Pay close attention to `}<em parentName=\"p\">{`(using YAML notation)`}</em>{`:`}</p>\n    <ul>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`spec.logging.name`}</inlineCode>{` should point to the name of the ConfigMap created in the previous step to configure custom `}<inlineCode parentName=\"li\">{`log4j`}</inlineCode>{` logging filters `}<em parentName=\"li\">{`(optional)`}</em></li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`spec.bootstrapServers`}</inlineCode>{` should be updated with your local Event Streams endpoints`}</li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`spec.tls.trustedCertificates[0].secretName`}</inlineCode>{` should match the Kubernetes Secret containing the IBM Event Streams certificate`}</li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`spec.authentication.passwordSecret.secretName`}</inlineCode>{` should match the Kubernetes Secret containing the IBM Event Streams API key`}</li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`spec.externalConfiguration.volumes[0].secret.secretName`}</inlineCode>{` should match the Kubernetes Secret containing your AWS credentials`}</li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`spec.config['group.id']`}</inlineCode>{` should be a unique name for this Kafka Connect cluster across all Kafka Connect instances that will be communicating with the same set of Kafka brokers.`}</li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`spec.config['*.storage.topic']`}</inlineCode>{` should be updated to provide unique topics for this Kafka Connect cluster inside your Kafka deployment. Distinct Kafka Connect clusters should not share metadata topics.`}</li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-yaml\"\n      }}>{`apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaConnectS2I\nmetadata:\n  name: connect-cluster-101\n  annotations:\n    strimzi.io/use-connector-resources: \"true\"\nspec:\n  #logging:\n  #  type: external\n  #  name: custom-connect-log4j\n  replicas: 1\n  bootstrapServers: es-1-ibm-es-proxy-route-bootstrap-eventstreams.apps.cluster.local:443\n  tls:\n    trustedCertificates:\n      - certificate: es-cert.crt\n        secretName: eventstreams-truststore-cert\n  authentication:\n    passwordSecret:\n      secretName: eventstreams-apikey\n      password: password\n    username: token\n    type: plain\n  externalConfiguration:\n    volumes:\n      - name: aws-credentials\n        secret:\n          secretName: aws-credentials\n  config:\n    group.id: connect-cluster-101\n    config.providers: file\n    config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider\n    key.converter: org.apache.kafka.connect.json.JsonConverter\n    value.converter: org.apache.kafka.connect.json.JsonConverter\n    key.converter.schemas.enable: false\n    value.converter.schemas.enable: false\n    offset.storage.topic: connect-cluster-101-offsets\n    config.storage.topic: connect-cluster-101-configs\n    status.storage.topic: connect-cluster-101-status\n`}</code></pre>\n    <p>{`Save the YAML above into a file named `}<inlineCode parentName=\"p\">{`kafka-connect.yaml`}</inlineCode>{`. If you created the ConfigMap in the previous step to filter out `}<inlineCode parentName=\"p\">{`accesskey`}</inlineCode>{` and `}<inlineCode parentName=\"p\">{`secretkey`}</inlineCode>{` values from the logs, uncomment the `}<inlineCode parentName=\"p\">{`spec.logging`}</inlineCode>{` lines to allow for the custom logging filters to be enabled during Kafka Connect cluster creation. Then this resource can be created via `}<inlineCode parentName=\"p\">{`kubectl apply -f kafka-connect.yaml`}</inlineCode>{`. You can then tail the output of the `}<inlineCode parentName=\"p\">{`connect-cluster-101`}</inlineCode>{` pods for updates on the connector status.`}</p>\n    <h4>{`Build the Camel Kafka Connector`}</h4>\n    <p>{`The next step is to build the Camel Kafka Connector binaries so that they can be loaded into the just-deployed Kafka Connect cluster’s container images.`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Clone the repository `}<a parentName=\"li\" {...{\n          \"href\": \"https://github.com/osowski/camel-kafka-connector\"\n        }}>{`https://github.com/osowski/camel-kafka-connector`}</a>{` to your local machine.`}</li>\n      <li parentName=\"ol\">{`Check out the `}<inlineCode parentName=\"li\">{`camel-kafka-connector-0.1.0-branch`}</inlineCode>{` via `}<inlineCode parentName=\"li\">{`git checkout camel-kafka-connector-0.1.0-branch`}</inlineCode>{`.`}</li>\n      <li parentName=\"ol\">{`From the root directory of the repository, build the components using the `}<inlineCode parentName=\"li\">{`mvn clean package`}</inlineCode>{` command.`}</li>\n      <li parentName=\"ol\">{`Go get a coffee and take a walk… as this build will take around 30 minutes on a normal developer workstation.`}</li>\n    </ol>\n    <p>{`Some items to note:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`The repository used here (`}<a parentName=\"li\" {...{\n          \"href\": \"https://github.com/osowski/camel-kafka-connector\"\n        }}>{`https://github.com/osowski/camel-kafka-connector`}</a>{`) is a fork of the official repository (`}<a parentName=\"li\" {...{\n          \"href\": \"https://github.com/apache/camel-kafka-connector\"\n        }}>{`https://github.com/apache/camel-kafka-connector`}</a>{`) with a minor update applied to allow for dynamic endpoints to be specified via configuration, which is critical for our `}<a parentName=\"li\" {...{\n          \"href\": \"#kafka-to-s3-sink-connector\"\n        }}>{`Kafka to S3 Sink Connector`}</a>{` scenario.`}</li>\n      <li parentName=\"ul\">{`This step (and the next step) will eventually be eliminated by providing an existing container image with the necessary Camel Kafka Connector binaries as part of a build system.`}</li>\n    </ul>\n    <h4>{`Deploy the Camel Kafka Connector binaries`}</h4>\n    <p>{`Now that the Camel Kafka Connector binaries have been built, we need to include them on the classpath inside of the container image which our Kafka Connect clusters are using.`}</p>\n    <ol>\n      <li parentName=\"ol\">{`From the root directory of the repository, start a new OpenShift Build, using the generated build artifacts:`}<pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`oc start-build connect-cluster-101-connect --from-dir=./core/target/camel-kafka-connector-0.0.1-SNAPSHOT-package/share/java --follow\n`}</code></pre></li>\n      <li parentName=\"ol\">{`Watch the Kubernetes pods as they are updated with the new build and rollout of the Kafka Connect Cluster using the updated container image `}<em parentName=\"li\">{`(which now includes the Camel Kafka Connector binaries)`}</em>{`:`}<pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-shell\"\n          }}>{`kubectl get pods -w\n`}</code></pre></li>\n      <li parentName=\"ol\">{`Once the `}<inlineCode parentName=\"li\">{`connect-cluster-101-connect-2-[random-suffix]`}</inlineCode>{` pod is in a `}<inlineCode parentName=\"li\">{`Running`}</inlineCode>{` state, you can proceed.`}</li>\n    </ol>\n    <h2>{`Kafka to S3 Sink Connector`}</h2>\n    <p>{`Now that you have a Kafka Connect cluster up and running, you will need to configure a connector to actually begin the transmission of data from one system to the other. This will be done by taking advantage of Strimzi and using the `}<a parentName=\"p\" {...{\n        \"href\": \"https://strimzi.io/docs/0.17.0/#kafkaconnector_resources\"\n      }}><inlineCode parentName=\"a\">{`KafkaConnector`}</inlineCode>{` custom resource`}</a>{` the Strimzi Operator manages for us.`}</p>\n    <p>{`Review the YAML description for our `}<inlineCode parentName=\"p\">{`KafkaConnector`}</inlineCode>{` custom resource below, named `}<inlineCode parentName=\"p\">{`s3-sink-connector`}</inlineCode>{`. Pay close attention to:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`The `}<inlineCode parentName=\"li\">{`strimzi.io/cluster`}</inlineCode>{` label must match the deployed Kafka Connect cluster you previously deployed `}<em parentName=\"li\">{`(or else Strimzi will not connect the `}<inlineCode parentName=\"em\">{`KafkaConnector`}</inlineCode>{` to your `}<inlineCode parentName=\"em\">{`KafkaConnect`}</inlineCode>{` cluster)`}</em></li>\n      <li parentName=\"ul\">{`The `}<inlineCode parentName=\"li\">{`topics`}</inlineCode>{` parameter `}<em parentName=\"li\">{`(named `}<inlineCode parentName=\"em\">{`my-source-topic`}</inlineCode>{` here)`}</em></li>\n      <li parentName=\"ul\">{`The S3 Bucket parameter of the `}<inlineCode parentName=\"li\">{`camel.sink.url`}</inlineCode>{` configuration option `}<em parentName=\"li\">{`(named `}<inlineCode parentName=\"em\">{`my-s3-bucket`}</inlineCode>{` here)`}</em></li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-yaml\"\n      }}>{`apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaConnector\nmetadata:\n  name: s3-sink-connector\n  labels:\n    strimzi.io/cluster: connect-cluster-101\nspec:\n  class: org.apache.camel.kafkaconnector.CamelSinkConnector\n  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.storage.StringConverter\n    topics: my-source-topic\n    camel.sink.url: aws-s3://my-s3-bucket?keyName=\\${date:now:yyyyMMdd-HHmmssSSS}-\\${exchangeId}\n    camel.sink.maxPollDuration: 10000\n    camel.component.aws-s3.configuration.autocloseBody: false\n    camel.component.aws-s3.accessKey: \\${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id}\n    camel.component.aws-s3.secretKey: \\${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key}\n    camel.component.aws-s3.region: US_EAST_1\n`}</code></pre>\n    <p>{`Once you have updated the YAML and saved it in a file named `}<inlineCode parentName=\"p\">{`kafka-sink-connector.yaml`}</inlineCode>{`, this resource can be created via `}<inlineCode parentName=\"p\">{`kubectl apply -f kafka-sink-connector.yaml`}</inlineCode>{`. You can then tail the output of the `}<inlineCode parentName=\"p\">{`connect-cluster-101`}</inlineCode>{` pods for updates on the connector status.`}</p>\n    <p><strong parentName=\"p\">{`NOTE:`}</strong>{` If you require objects in S3 to reside in a sub-folder of the bucket root, you can place a folder name prefix in the `}<inlineCode parentName=\"p\">{`keyName`}</inlineCode>{` query parameter of the `}<inlineCode parentName=\"p\">{`camel.sink.url`}</inlineCode>{` configuration option above. For example, `}<inlineCode parentName=\"p\">{`camel.sink.url: aws-s3://my-s3-bucket?keyName=myfoldername/\\${date:now:yyyyMMdd-HHmmssSSS}-\\${exchangeId}`}</inlineCode>{`.`}</p>\n    <h2>{`S3 to Kafka Source Connector`}</h2>\n    <p>{`Similar to the `}<a parentName=\"p\" {...{\n        \"href\": \"#kafka-to-s3-sink-connector\"\n      }}>{`Kafka to S3 Sink Connector`}</a>{` scenario, this scenario will make use of the Strimzi `}<a parentName=\"p\" {...{\n        \"href\": \"https://strimzi.io/docs/0.17.0/#kafkaconnector_resources\"\n      }}><inlineCode parentName=\"a\">{`KafkaConnector`}</inlineCode>{` custom resource`}</a>{` to configure the specific connector instance.`}</p>\n    <p>{`Review the YAML description for our `}<inlineCode parentName=\"p\">{`KafkaConnector`}</inlineCode>{` custom resource below, named `}<inlineCode parentName=\"p\">{`s3-source-connector`}</inlineCode>{`. Pay close attention to:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`The `}<inlineCode parentName=\"li\">{`strimzi.io/cluster`}</inlineCode>{` label must match the deployed Kafka Connect cluster you previously deployed `}<em parentName=\"li\">{`(or else Strimzi will not connect the `}<inlineCode parentName=\"em\">{`KafkaConnector`}</inlineCode>{` to your `}<inlineCode parentName=\"em\">{`KafkaConnect`}</inlineCode>{` cluster)`}</em></li>\n      <li parentName=\"ul\">{`The `}<inlineCode parentName=\"li\">{`topics`}</inlineCode>{` and `}<inlineCode parentName=\"li\">{`camel.source.kafka.topic`}</inlineCode>{` parameters `}<em parentName=\"li\">{`(named `}<inlineCode parentName=\"em\">{`my-target-topic`}</inlineCode>{` here)`}</em></li>\n      <li parentName=\"ul\">{`The S3 Bucket parameter of the `}<inlineCode parentName=\"li\">{`camel.sink.url`}</inlineCode>{` configuration option `}<em parentName=\"li\">{`(named `}<inlineCode parentName=\"em\">{`my-s3-bucket`}</inlineCode>{` here)`}</em></li>\n    </ul>\n    <p><strong parentName=\"p\">{`Please note`}</strong>{` that it is an explicit intention that the topics used in the `}<a parentName=\"p\" {...{\n        \"href\": \"#kafka-to-s3-sink-connector\"\n      }}>{`Kafka to S3 Sink Connector`}</a>{` configuration and the `}<a parentName=\"p\" {...{\n        \"href\": \"#s3-to-kafka-source-connector\"\n      }}>{`S3 to Kafka Source Connector`}</a>{` configuration are different. If these configurations were to use the `}<strong parentName=\"p\">{`same Kafka topic`}</strong>{` and the `}<strong parentName=\"p\">{`same S3 Bucket`}</strong>{`, we would create an infinite processing loop of the same information being endlessly recycled through the system. In our example deployments here, we are deploying to different topics but the same S3 Bucket.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-yaml\"\n      }}>{`apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaConnector\nmetadata:\n  name: s3-source-connector\n  labels:\n    strimzi.io/cluster: connect-cluster-101\nspec:\n  class: org.apache.camel.kafkaconnector.CamelSourceConnector\n  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.camel.kafkaconnector.converters.S3ObjectConverter\n    topics: my-target-topic\n    camel.source.kafka.topic: my-target-topic\n    camel.source.url: aws-s3://my-s3-bucket?autocloseBody=false\n    camel.source.maxPollDuration: 10000\n    camel.component.aws-s3.accessKey: \\${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials-secret.properties:aws_access_key_id}\n    camel.component.aws-s3.secretKey: \\${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials-secret.properties:aws_secret_access_key}\n    camel.component.aws-s3.region: US_EAST_1\n`}</code></pre>\n    <p>{`Once you have updated the YAML and saved it in a file named `}<inlineCode parentName=\"p\">{`kafka-source-connector.yaml`}</inlineCode>{`, this resource can be created via `}<inlineCode parentName=\"p\">{`kubectl apply -f kafka-source-connector.yaml`}</inlineCode>{`. You can then tail the output of the `}<inlineCode parentName=\"p\">{`connect-cluster-101`}</inlineCode>{` pods for updates on the connector status.`}</p>\n    <p><strong parentName=\"p\">{`NOTE:`}</strong>{` If you require the connector to only read objects from a subdirecotry of the S3 bucket root, you can set the `}<inlineCode parentName=\"p\">{`camel.component.aws-s3.configuration.prefix`}</inlineCode>{` configuration option with the value of the subdirectory name. For example, `}<inlineCode parentName=\"p\">{`camel.component.aws-s3.configuration.prefix: myfoldername`}</inlineCode>{` .`}</p>\n    <h2>{`Next steps`}</h2>\n    <ul>\n      <li parentName=\"ul\">{`Enable use  of IAM Credentials in the Connector configuration `}<em parentName=\"li\">{`(as the default Java code currently outputs `}<inlineCode parentName=\"em\">{`aws_access_key_id`}</inlineCode>{` and `}<inlineCode parentName=\"em\">{`aws_secret_access_key`}</inlineCode>{` to container runtime logs due to their existence as configuration properties)`}</em></li>\n      <li parentName=\"ul\">{`Optionally configure individual connector instances to startup with offset value of -1 `}<em parentName=\"li\">{`(to enable to run from beginning of available messages)`}</em></li>\n      <li parentName=\"ul\">{`Implement a build system to produce container images with the Camel Kafka Connector binaries already present`}</li>\n    </ul>\n    <h2>{`References`}</h2>\n    <ul>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://camel.apache.org/camel-kafka-connector/latest/try-it-out-on-openshift-with-strimzi.html\"\n        }}>{`Apache Camel Kafka Connector - Try it out on OpenShift with Strimzi`}</a></li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://camel.apache.org/components/latest/languages/simple-language.html\"\n        }}>{`Apache Camel - Available pattern elements`}</a>{` for use in the `}<inlineCode parentName=\"li\">{`keyName`}</inlineCode>{` parameter of the `}<inlineCode parentName=\"li\">{`camel.sink.url`}</inlineCode>{` property.`}</li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://camel.apache.org/components/latest/eips/toD-eip.html\"\n        }}>{`Apache Camel - Dynamic Endpoint reference`}</a></li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"https://developers.redhat.com/blog/2020/02/14/using-secrets-in-apache-kafka-connect-configuration/\"\n        }}>{`Red Hat Developer Blog - Using secrets in Kafka Connect configuration`}</a></li>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"http://kafka.apache.org/documentation/#connect\"\n        }}>{`Apache Kafka - Connect Overview`}</a></li>\n    </ul>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}